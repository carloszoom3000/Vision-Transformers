{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9758084,"sourceType":"datasetVersion","datasetId":5975141}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Trabajo practico 2 de Vision Trasnformers\n\nPor Carlos Villalobos\n\n1. **Implementar la arquitectura de un Vision Transformer** \n\n2. **Ingresar y ajustar los parametros del modelo***\n\n3. **Probar diferentes técnicas de data augmentation** ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport numpy as np\nimport math\nimport os\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch\nfrom torchvision import datasets, transforms\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n#from trainer import Trainer\n\n\n\ndevice =  'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\nprint('Dispositivo: ',device)\nprint('Torch version: ',torch.__version__)\n\n# La configuración, carga y preprocesamiento\nclass ConfigPreprocess:\n    def __init__(self, device: str, img_path: str, img_size: int, patch_size: int):\n        self.device = device\n        self.img_path = img_path\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.test_img = self.load_image()\n\n    def load_image(self):\n        return TF.to_tensor(Image.open(self.img_path).resize((self.img_size, self.img_size))).unsqueeze(0).to(self.device)\n\n    def extract_patches(self, image: Tensor) -> Tensor:\n        patches = image.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size)\n        patches = patches.contiguous().view(image.shape[0], -1, self.patch_size, self.patch_size)\n        return patches\n    \n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size: int, patch_size: int, in_channels: int = 3, embed_dim: int = 8):\n        super(PatchEmbedding, self).__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n        x = x.flatten(2)  # (B, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n        return x\n\nclass PositionalEncodingLearned(nn.Module):\n    import torch\nfrom torch import nn\nimport matplotlib.pyplot as plt\nimport math\n\n# Configuraciones a probar\nconfigs = [\n    {\"patch_size\": 32, \"embed_dim\": 16},\n    {\"patch_size\": 64, \"embed_dim\": 8},\n    {\"patch_size\": 128, \"embed_dim\": 4}\n]\n\ndef analyze_config(img_size, patch_size, embed_dim):\n    num_patches = (img_size // patch_size) ** 2\n    total_params = num_patches * embed_dim\n    return {\n        \"num_patches\": num_patches,\n        \"total_params\": total_params,\n        \"spatial_resolution\": patch_size\n    }\n\n# Análisis de diferentes configuraciones\nimg_size = 900\nfor config in configs:\n    analysis = analyze_config(img_size, config[\"patch_size\"], config[\"embed_dim\"])\n    print(f\"\\nConfiguración: Patch Size = {config['patch_size']}, Embed Dim = {config['embed_dim']}\")\n    print(f\"Número de parches: {analysis['num_patches']}\")\n    print(f\"Resolución espacial: {analysis['spatial_resolution']}x{analysis['spatial_resolution']} pixels\")\n    print(f\"Parámetros totales: {analysis['total_params']}\")\n\n# Clase PositionalEncoding original\nclass PositionalEncoding(nn.Module):\n    def __init__(self, num_patches, embed_dim):\n        super(PositionalEncoding, self).__init__()\n        self.register_buffer('pos_embedding', self.create_positional_encoding(num_patches, embed_dim))\n\n    def create_positional_encoding(self, num_patches, embed_dim):\n        position = torch.arange(num_patches, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n        pos_encoding = torch.zeros(num_patches, embed_dim)\n        pos_encoding[:, 0::2] = torch.sin(position * div_term)  # Aplicar seno en dimensiones pares\n        pos_encoding[:, 1::2] = torch.cos(position * div_term)  # Aplicar coseno en dimensiones impares\n        return pos_encoding.unsqueeze(0)  # Añadir dimensión de batch\n\n    def forward(self, x):\n        return x + self.pos_embedding\n\n# Implementación de la clase PositionalEncodingLearned\nclass PositionalEncodingLearned(nn.Module):\n    def __init__(self, num_patches, embed_dim):\n        super().__init__()\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim) * 0.02)\n\n    def forward(self, x):\n        return x + self.pos_embedding\n\n# Función para visualizar y comparar ambos encodings\ndef compare_positional_encodings(num_patches, embed_dim, device):\n    # Inicializar ambos tipos de encoding\n    learned_pe = PositionalEncodingLearned(num_patches, embed_dim).to(device)\n    sinusoidal_pe = PositionalEncoding(num_patches, embed_dim).to(device)\n\n    # Generar datos de ejemplo\n    x = torch.zeros(1, num_patches, embed_dim).to(device)\n\n    # Obtener embeddings\n    learned_embeddings = learned_pe(x)\n    sinusoidal_embeddings = sinusoidal_pe(x)\n\n    # Visualizar\n    plt.figure(figsize=(15, 5))\n\n    # Sinusoidal\n    plt.subplot(1, 2, 1)\n    for i in range(embed_dim):\n        plt.plot(sinusoidal_embeddings[0, :, i].detach().cpu().numpy(),\n                label=f'Dim {i+1}')\n    plt.title('Codificación Posicional Sinusoidal')\n    plt.xlabel('Posición')\n    plt.ylabel('Valor')\n    plt.legend()\n\n    # Learned\n    plt.subplot(1, 2, 2)\n    for i in range(embed_dim):\n        plt.plot(learned_embeddings[0, :, i].detach().cpu().numpy(),\n                label=f'Dim {i+1}')\n    plt.title('Codificación Posicional Aprendida')\n    plt.xlabel('Posición')\n    plt.ylabel('Valor')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n# Probar con los parámetros originales\nimg_size = 900\npatch_size = 64\nembed_dim = 8\nnum_patches = (img_size // patch_size) ** 2\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n\ncompare_positional_encodings(num_patches, embed_dim, device)\n\n\n\n# Parámetros\nimg_path = \"/kaggle/input/raccoon/raccoon.jpg\"\nimg_size = 900\npatch_size = 64\nembed_dim = 8\npatch_idx = 0  # El índice del parche para el cual queres visualiizar la codificación posicional\n\n# Preprocesamiento\nconfig = ConfigPreprocess(device,img_path, img_size, patch_size)\n\n# Extracción de parches y visualización\npatches = config.extract_patches(config.test_img.squeeze(0))\n\n# Generación de embeddings\nembedded_patches = PatchEmbedding(img_size, patch_size, 3, embed_dim).to(config.device)\npatches = embedded_patches(config.test_img)\n\n# Codificación posicional\n\nnum_patches = (img_size // patch_size) ** 2\ntry:\n    positional_encoding = PositionalEncodingLearned(num_patches, embed_dim).to(config.device)\n    pos_embeddings = positional_encoding(patches)\n\nexcept Exception as e:\n    print(f\"Revise la existencia de la función PositionalEncodingLearned. Se produjo error durante la compilación: \\n {e}.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-29T23:04:07.587829Z","iopub.execute_input":"2024-10-29T23:04:07.588178Z","iopub.status.idle":"2024-10-29T23:04:14.654240Z","shell.execute_reply.started":"2024-10-29T23:04:07.588141Z","shell.execute_reply":"2024-10-29T23:04:14.653381Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vision Trasnformer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, \n                 img_size: int = 900,\n                 patch_size: int = 64,\n                 in_channels: int = 3,\n                 num_classes: int = 1000,\n                 embed_dim: int = 8,\n                 num_heads: int = 2,\n                 ff_dim: int = 32,\n                 num_layers: int = 4,\n                 dropout: float = 0.1):\n        super().__init__()\n        \n        # Patch Embedding\n        self.patch_embed = PatchEmbedding(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_channels=in_channels,\n            embed_dim=embed_dim\n        )\n        \n        # Número de parches\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # [CLS] token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        nn.init.normal_(self.cls_token, std=0.02)\n        \n        # Codificación posicional\n        self.pos_embed = PositionalEncodingLearned(self.num_patches + 1, embed_dim)\n        \n        # Dropout\n        self.pos_drop = nn.Dropout(p=dropout)\n        \n        # Transformer Encoder\n        self.transformer = TransformerEncoder(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            ff_dim=ff_dim,\n            num_layers=num_layers,\n            dropout=dropout\n        )\n        \n        # Layer Norm\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Clasificador\n        self.head = nn.Linear(embed_dim, num_classes)\n        \n    def forward(self, x):\n        # Crear embeddings de los parches\n        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n        \n        # Añadir token CLS\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_token, x), dim=1)\n        \n        # Añadir codificación posicional\n        x = self.pos_embed(x)\n        \n        # Aplicar dropout\n        x = self.pos_drop(x)\n        \n        # Pasar por el transformer\n        x = self.transformer(x)\n        \n        # Normalización\n        x = self.norm(x)\n        \n        # Clasificación usando solo el token CLS\n        x = x[:, 0]\n        x = self.head(x)\n        \n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"embed_dim debe ser divisible por num_heads\"\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Proyecciones lineales para Q, K, V\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.scale = self.head_dim ** -0.5\n        \n    def forward(self, x):\n        batch_size, seq_len, _ = x.shape\n        \n        # Proyectar y reshape para multi-head\n        q = self.q_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        k = self.k_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        v = self.v_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n        \n        # Calcular scores de atención\n        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n        \n        # Aplicar softmax\n        attn = F.softmax(scores, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Aplicar atención a los valores\n        x = torch.matmul(attn, v)\n        \n        # Reshape y proyección final\n        x = x.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.embed_dim)\n        x = self.out_proj(x)\n        \n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout: float = 0.1):\n        super().__init__()\n        \n        # Multi-Head Attention\n        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n        \n        # Feed Forward Network\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, ff_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n        \n        # Layer Normalization\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        # Atención con residual connection\n        x = x + self.attention(self.norm1(x))\n        \n        # FFN con residual connection\n        x = x + self.ffn(self.norm2(x))\n        \n        return x\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, num_layers: int, dropout: float = 0.1):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)\n            for _ in range(num_layers)\n        ])\n        \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:05:09.006371Z","iopub.execute_input":"2024-10-29T23:05:09.006877Z","iopub.status.idle":"2024-10-29T23:05:09.032815Z","shell.execute_reply.started":"2024-10-29T23:05:09.006825Z","shell.execute_reply":"2024-10-29T23:05:09.031726Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Define transformations for the input data\ntransform = transforms.Compose([\n    #transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n    #transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load your dataset\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=True)\n\ndef get_folder_size(folder_path :os.PathLike) -> str:\n    total_size = 0\n    for dirpath, _, filenames in os.walk(folder_path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n\n            if not os.path.islink(file_path):\n                total_size += os.path.getsize(file_path)\n\n    if total_size == 0:\n        return \"0B\"\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\")\n    i = int(math.floor(math.log(total_size, 1024)))\n    p = math.pow(1024, i)\n    s = round(total_size / p, 2)\n    return f\"{s} {size_name[i]}\"\n\n\nfolder_path = './data'\nsize_in_bytes = get_folder_size(folder_path)\nprint(f\"Tamaño de carpeta: {size_in_bytes}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:05:49.250441Z","iopub.execute_input":"2024-10-29T23:05:49.251378Z","iopub.status.idle":"2024-10-29T23:05:54.243907Z","shell.execute_reply.started":"2024-10-29T23:05:49.251334Z","shell.execute_reply":"2024-10-29T23:05:54.242741Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:01<00:00, 105511015.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nTamaño de carpeta: 340.19 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2. ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, \n                 img_size: int = 32,          # CIFAR-10 image size\n                 patch_size: int = 4,          # Smaller patches for small images\n                 in_channels: int = 3,\n                 num_classes: int = 10,        # CIFAR-10 has 10 classes\n                 embed_dim: int = 192,         # Increased embedding dimension\n                 num_heads: int = 8,           # More heads for better attention\n                 ff_dim: int = 768,           # Larger feed-forward dimension\n                 num_layers: int = 7,          # More layers for better feature extraction\n                 dropout: float = 0.1):\n        super().__init__()\n        \n        assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n        \n        # Patch Embedding\n        self.patch_embed = PatchEmbedding(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_channels=in_channels,\n            embed_dim=embed_dim\n        )\n        \n        # Número de parches (64 parches para 32x32 con patch_size=4)\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # [CLS] token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        \n        # Codificación posicional\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        \n        # Dropout\n        self.pos_drop = nn.Dropout(p=dropout)\n        \n        # Transformer Encoder\n        self.transformer = TransformerEncoder(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            ff_dim=ff_dim,\n            num_layers=num_layers,\n            dropout=dropout\n        )\n        \n        # Layer Norm\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Clasificador con dropout\n        self.pre_head_dropout = nn.Dropout(0.2)\n        self.head = nn.Linear(embed_dim, num_classes)\n        \n        # Inicialización de pesos\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Inicialización de la capa de clasificación\n        nn.init.zeros_(self.head.bias)\n        nn.init.xavier_uniform_(self.head.weight)\n        \n    def forward(self, x):\n        # Crear embeddings de los parches\n        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n        \n        # Añadir token CLS\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_token, x), dim=1)\n        \n        # Añadir codificación posicional\n        x = x + self.pos_embed\n        \n        # Aplicar dropout\n        x = self.pos_drop(x)\n        \n        # Pasar por el transformer\n        x = self.transformer(x)\n        \n        # Normalización\n        x = self.norm(x)\n        \n        # Clasificación usando solo el token CLS con dropout adicional\n        x = x[:, 0]\n        x = self.pre_head_dropout(x)\n        x = self.head(x)\n        \n        return x\n\n# Función de entrenamiento y evaluación\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Batch: {batch_idx}, Loss: {loss.item():.4f}, '\n                  f'Acc: {100.*correct/total:.2f}%')\n    \n    return total_loss / len(train_loader), 100. * correct / total\n\ndef evaluate(model, test_loader, criterion, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n    \n    return test_loss / len(test_loader), 100. * correct / total\n\n# Configuración del modelo y entrenamiento\ndef setup_training():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    num_epochs = 10\n    # Inicializar modelo\n    model = VisionTransformer(\n        img_size=32,          # CIFAR-10 image size\n        patch_size=4,         # 4x4 patches\n        in_channels=3,\n        num_classes=10,\n        embed_dim=192,        # Embedding dimension\n        num_heads=8,          # Number of attention heads\n        ff_dim=768,          # Feed-forward dimension\n        num_layers=7,         # Number of transformer layers\n        dropout=0.1\n    ).to(device)\n    \n    # Criterio y optimizador\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), \n                           lr=1e-3, \n                           weight_decay=0.05,\n                           betas=(0.9, 0.999))\n    \n    # Learning rate scheduler\n    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    return model, criterion, optimizer, scheduler, device\n\n# Uso del modelo\nmodel, criterion, optimizer, scheduler, device = setup_training()\nprint(f\"Modelo creado en dispositivo: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:06:20.389394Z","iopub.execute_input":"2024-10-29T23:06:20.390097Z","iopub.status.idle":"2024-10-29T23:06:20.471183Z","shell.execute_reply.started":"2024-10-29T23:06:20.390053Z","shell.execute_reply":"2024-10-29T23:06:20.470135Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Modelo creado en dispositivo: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Parametros del modelo:\", list(model.parameters()))\n\n# Definimos funciones de loss y optimizador\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=3e-4)\nscheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n\n\nmodel = model.to(device)\n\n\ntry:\n    model = torch.compile(model)\nexcept Exception as e:\n    print(\"Se produjo error durante la compilación:\", e)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:06:39.377988Z","iopub.execute_input":"2024-10-29T23:06:39.378950Z","iopub.status.idle":"2024-10-29T23:06:41.621062Z","shell.execute_reply.started":"2024-10-29T23:06:39.378904Z","shell.execute_reply":"2024-10-29T23:06:41.620079Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Parametros del modelo: [Parameter containing:\ntensor([[[ 0.0271, -0.0157,  0.0161,  0.0147, -0.0008, -0.0447, -0.0281,\n          -0.0166, -0.0087, -0.0048,  0.0098,  0.0107, -0.0171, -0.0110,\n          -0.0006, -0.0242,  0.0112,  0.0422, -0.0211, -0.0024, -0.0458,\n           0.0407,  0.0158,  0.0018,  0.0013,  0.0235, -0.0013,  0.0263,\n           0.0114,  0.0292, -0.0283,  0.0285,  0.0134, -0.0318,  0.0200,\n           0.0288,  0.0027,  0.0145,  0.0101, -0.0089,  0.0270, -0.0019,\n          -0.0186, -0.0150, -0.0210,  0.0089, -0.0309, -0.0055, -0.0195,\n          -0.0100,  0.0033,  0.0124,  0.0196, -0.0051, -0.0105, -0.0198,\n           0.0065, -0.0180, -0.0073,  0.0125, -0.0269,  0.0417, -0.0133,\n          -0.0060, -0.0232, -0.0029, -0.0056,  0.0201,  0.0038,  0.0019,\n          -0.0235, -0.0233, -0.0509, -0.0064, -0.0071,  0.0302, -0.0107,\n          -0.0070, -0.0017, -0.0122,  0.0045,  0.0020, -0.0021,  0.0112,\n           0.0024,  0.0164,  0.0054,  0.0357, -0.0178, -0.0133, -0.0013,\n           0.0119,  0.0181,  0.0285, -0.0296, -0.0012,  0.0072, -0.0189,\n          -0.0006,  0.0011, -0.0080, -0.0170, -0.0419, -0.0298, -0.0228,\n          -0.0289,  0.0006, -0.0094,  0.0044, -0.0077,  0.0009, -0.0066,\n           0.0077, -0.0123, -0.0006, -0.0373, -0.0109,  0.0251, -0.0578,\n           0.0082, -0.0171, -0.0258, -0.0020,  0.0072,  0.0184, -0.0108,\n           0.0069,  0.0114, -0.0035,  0.0261, -0.0212, -0.0071,  0.0183,\n          -0.0022,  0.0280,  0.0221,  0.0058, -0.0029,  0.0040,  0.0167,\n           0.0044,  0.0370,  0.0246,  0.0413, -0.0059, -0.0089,  0.0200,\n           0.0123, -0.0124, -0.0380, -0.0244, -0.0278,  0.0137,  0.0105,\n           0.0413, -0.0093,  0.0193, -0.0581, -0.0444, -0.0250,  0.0119,\n           0.0393,  0.0154, -0.0157,  0.0052, -0.0085, -0.0046, -0.0029,\n          -0.0044, -0.0135,  0.0015, -0.0029,  0.0015,  0.0069,  0.0194,\n           0.0237,  0.0034, -0.0072,  0.0148,  0.0162, -0.0089, -0.0293,\n          -0.0234, -0.0156,  0.0124,  0.0206,  0.0003, -0.0092, -0.0034,\n          -0.0207,  0.0020,  0.0378]]], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[[ 8.2726e-03, -2.5606e-03,  4.9510e-05,  ..., -3.8814e-02,\n           1.3774e-02, -1.1091e-02],\n         [ 8.2462e-03, -3.1617e-02,  1.9018e-02,  ..., -1.8395e-02,\n           7.3852e-03, -2.3375e-02],\n         [-4.8178e-03, -5.8461e-03,  1.9961e-02,  ...,  6.5695e-03,\n           4.2990e-03,  2.6832e-03],\n         ...,\n         [ 3.1788e-02, -3.7083e-02,  1.0206e-02,  ...,  3.0478e-02,\n           1.5826e-02,  2.3686e-02],\n         [-1.0794e-02, -3.6303e-02, -1.3459e-02,  ...,  6.5547e-03,\n           3.1556e-02,  8.1172e-03],\n         [-2.1028e-02,  2.8990e-02,  1.2279e-02,  ...,  1.1217e-02,\n           1.5425e-02, -3.7086e-02]]], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[[[-0.0645,  0.1440,  0.0558,  0.0545],\n          [-0.1042, -0.0974, -0.0364, -0.0821],\n          [ 0.0547,  0.0731, -0.1372,  0.0113],\n          [ 0.0442,  0.0620,  0.1068,  0.0047]],\n\n         [[-0.0692,  0.0675, -0.0131, -0.0984],\n          [-0.0821, -0.1088, -0.1113,  0.0563],\n          [ 0.0620,  0.0374, -0.0756, -0.0825],\n          [ 0.0374, -0.1036, -0.0895, -0.0828]],\n\n         [[ 0.0066, -0.0884, -0.0541, -0.0751],\n          [ 0.0797,  0.1428, -0.0419,  0.0800],\n          [ 0.1304, -0.0742,  0.0941,  0.0853],\n          [ 0.0468,  0.1374, -0.1372, -0.1288]]],\n\n\n        [[[-0.1154,  0.1130, -0.1287, -0.0002],\n          [ 0.1253,  0.0165,  0.0445,  0.1440],\n          [ 0.0441,  0.0888,  0.1164,  0.0578],\n          [-0.0178, -0.1131, -0.0130, -0.0128]],\n\n         [[ 0.0249, -0.0619,  0.0829,  0.0658],\n          [-0.1148,  0.0892,  0.1414,  0.0751],\n          [ 0.0822, -0.0994, -0.0280,  0.1063],\n          [-0.1208, -0.0714, -0.1331,  0.0547]],\n\n         [[-0.0733, -0.0193,  0.0873,  0.0105],\n          [ 0.1000, -0.1073,  0.0827,  0.0304],\n          [-0.0456, -0.0569,  0.1108, -0.1324],\n          [ 0.0922,  0.1231, -0.0330, -0.1247]]],\n\n\n        [[[-0.0849, -0.0612,  0.0388,  0.1221],\n          [-0.0057, -0.0099,  0.1443,  0.0400],\n          [-0.1354, -0.0969, -0.0654, -0.0136],\n          [-0.1422,  0.0838, -0.0286, -0.1260]],\n\n         [[ 0.0427,  0.0016,  0.1241,  0.1031],\n          [ 0.0488, -0.0530, -0.0061, -0.0973],\n          [-0.0225, -0.1170,  0.1164,  0.0287],\n          [-0.1318, -0.0330, -0.0601,  0.0480]],\n\n         [[ 0.0344,  0.1118,  0.0179,  0.0296],\n          [-0.1330,  0.0420,  0.0419,  0.0742],\n          [-0.0469, -0.0064, -0.1365, -0.0426],\n          [-0.1002,  0.1217,  0.0470,  0.0012]]],\n\n\n        ...,\n\n\n        [[[-0.1392, -0.0220, -0.0780,  0.1079],\n          [ 0.0146, -0.0242, -0.1067,  0.0568],\n          [-0.0252,  0.1285, -0.0419,  0.1186],\n          [-0.1012,  0.0641, -0.0149,  0.1417]],\n\n         [[-0.1340,  0.0334, -0.0742, -0.0215],\n          [ 0.1012, -0.0829,  0.0288, -0.0917],\n          [-0.1009, -0.1128,  0.1000, -0.1174],\n          [-0.1000,  0.0505,  0.0702, -0.1333]],\n\n         [[-0.0448, -0.1314, -0.0051, -0.1121],\n          [-0.0174, -0.1409, -0.0157,  0.0398],\n          [-0.0667, -0.1294, -0.1013,  0.0415],\n          [-0.0554, -0.1158, -0.1148,  0.1105]]],\n\n\n        [[[ 0.0890, -0.0621,  0.1332,  0.0867],\n          [-0.0329, -0.1231,  0.1319,  0.1222],\n          [ 0.0003,  0.0753,  0.0309,  0.0537],\n          [-0.0167,  0.1080,  0.0228,  0.0546]],\n\n         [[-0.1010, -0.1150,  0.0761,  0.0469],\n          [ 0.0750,  0.1184,  0.0898,  0.0528],\n          [-0.1311, -0.0863, -0.0863, -0.0811],\n          [ 0.1100, -0.0209,  0.1419,  0.0959]],\n\n         [[-0.0850,  0.0912,  0.0592,  0.1273],\n          [ 0.0096, -0.0050,  0.0493,  0.0771],\n          [ 0.1439,  0.0333, -0.0992, -0.0112],\n          [ 0.0403, -0.1007, -0.0714,  0.1120]]],\n\n\n        [[[-0.1029,  0.0166, -0.1112,  0.0217],\n          [ 0.0307, -0.0563, -0.0453,  0.0573],\n          [-0.0138,  0.0783, -0.0374, -0.0613],\n          [ 0.0070, -0.0492,  0.0090,  0.1158]],\n\n         [[ 0.0375, -0.1335,  0.0075,  0.0263],\n          [-0.0205,  0.1343, -0.1354,  0.0662],\n          [ 0.0015, -0.0487, -0.0064, -0.0287],\n          [-0.1331, -0.0648,  0.0327,  0.1433]],\n\n         [[-0.1118,  0.1312, -0.0504, -0.1288],\n          [ 0.0755,  0.1241, -0.0924, -0.0963],\n          [-0.0877,  0.0960, -0.0579,  0.0696],\n          [-0.0230,  0.0351,  0.1244, -0.1325]]]], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([ 1.4052e-01,  9.5598e-02, -4.7631e-02, -7.5965e-02,  1.1106e-01,\n         1.4147e-01, -1.0238e-01,  1.3827e-01, -2.3480e-02, -1.2952e-01,\n        -6.6644e-02,  3.0921e-03,  5.6076e-02, -9.6071e-03,  1.4242e-01,\n        -8.2320e-02,  9.9822e-02,  6.1114e-03,  1.3836e-02,  7.2326e-02,\n        -4.2852e-02, -9.3320e-02, -2.9857e-02, -1.2886e-01, -7.2725e-02,\n         6.7239e-03,  4.9795e-02, -2.7360e-02,  8.1984e-02,  1.1071e-01,\n         1.1070e-01,  1.2757e-01, -1.3421e-01,  6.1501e-02,  7.0712e-02,\n         3.2108e-02,  1.3538e-01,  6.0155e-02,  3.3485e-02, -4.2533e-02,\n        -1.3602e-01,  5.3220e-02, -6.9439e-02,  1.3997e-01,  6.1058e-02,\n         2.9048e-02, -3.1892e-02, -1.2927e-01, -3.7026e-02, -9.6719e-02,\n         6.0061e-02,  2.4549e-02, -9.2856e-02, -1.7791e-02,  1.2675e-01,\n         1.1941e-01, -1.1085e-01, -4.1268e-02,  6.9581e-02,  8.8118e-03,\n        -3.5001e-03, -5.7792e-02,  9.6689e-02, -1.1025e-01,  1.0198e-01,\n         1.4389e-01,  4.3323e-02,  1.3185e-01, -8.4428e-02,  2.3601e-02,\n         7.1540e-02,  3.6165e-02,  1.3459e-01, -5.6224e-02,  3.4150e-02,\n        -2.9311e-03,  6.3686e-03, -9.0752e-03,  9.3718e-02, -3.1677e-02,\n        -1.0485e-02, -3.4216e-02, -7.8209e-02,  1.1558e-01, -1.3917e-01,\n        -1.2022e-04, -1.0690e-01, -4.8659e-02,  6.6079e-02,  7.0472e-02,\n         1.4196e-01,  4.6384e-02,  1.0518e-01,  1.1910e-01, -1.3643e-01,\n         5.3076e-02, -6.6825e-02, -1.3696e-01, -1.1363e-01,  1.0323e-01,\n         4.8260e-02, -8.9608e-02,  2.0478e-02,  2.0930e-04, -1.1831e-01,\n        -1.1469e-02,  6.2279e-02,  2.9269e-03, -3.4459e-02,  1.5976e-03,\n        -1.1078e-01,  4.6291e-02, -8.7565e-02, -9.6713e-02, -3.8226e-02,\n        -7.3705e-02, -1.0646e-01,  1.4192e-01,  5.4749e-03, -9.5092e-02,\n         1.4170e-01, -1.2920e-01,  3.0509e-02,  1.3974e-01,  6.2721e-02,\n        -2.3082e-02, -1.4014e-02, -1.3645e-01, -7.3244e-02, -3.3574e-02,\n         1.2140e-01,  2.8643e-02, -1.1511e-01, -6.1584e-02, -5.1832e-02,\n         6.2691e-02, -7.0319e-02,  2.1058e-02,  8.0634e-02, -9.8760e-02,\n         3.5749e-02, -3.7411e-02, -1.2797e-01,  5.8155e-02,  8.9862e-02,\n         4.7099e-02, -2.8181e-03,  4.1457e-02,  8.6172e-02,  1.1524e-01,\n        -1.2412e-01, -1.9853e-03,  4.7468e-02, -7.0313e-02, -1.2364e-01,\n        -4.7066e-02, -1.0337e-01, -1.2781e-01, -1.0887e-01,  1.1767e-01,\n        -1.3528e-03,  1.0944e-01,  1.0132e-02, -2.7272e-02,  1.3784e-01,\n         4.2679e-02, -8.7788e-02,  1.4432e-02, -5.6632e-02, -4.4870e-02,\n         6.9039e-02, -6.3216e-02, -4.0114e-02,  2.3591e-02, -7.3970e-02,\n         1.8995e-03, -1.0111e-01, -6.3557e-02,  1.3948e-01,  1.6611e-02,\n         1.9662e-02, -1.1139e-01,  9.8012e-03,  5.9012e-02, -4.2047e-02,\n         1.3765e-01, -1.1386e-02, -1.1115e-02,  1.2618e-01,  1.2015e-01,\n        -1.0863e-01,  2.6036e-03], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0435, -0.0276,  0.0055,  ..., -0.0373, -0.0487, -0.0159],\n        [-0.0370,  0.0415,  0.0004,  ..., -0.0329,  0.0530,  0.0649],\n        [-0.0168,  0.0373,  0.0131,  ..., -0.0621,  0.0569, -0.0081],\n        ...,\n        [-0.0102,  0.0289, -0.0374,  ...,  0.0663, -0.0352,  0.0463],\n        [ 0.0279,  0.0675, -0.0252,  ...,  0.0441,  0.0094, -0.0076],\n        [ 0.0603,  0.0178, -0.0459,  ...,  0.0682, -0.0308,  0.0245]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0512, -0.0076, -0.0241,  ...,  0.0496, -0.0272, -0.0250],\n        [ 0.0329, -0.0558, -0.0601,  ..., -0.0480, -0.0450,  0.0403],\n        [ 0.0556,  0.0703,  0.0573,  ...,  0.0274,  0.0500, -0.0650],\n        ...,\n        [-0.0685,  0.0536, -0.0530,  ..., -0.0377, -0.0508,  0.0426],\n        [ 0.0209, -0.0462, -0.0101,  ..., -0.0682, -0.0284, -0.0516],\n        [-0.0044, -0.0112, -0.0176,  ..., -0.0608,  0.0362,  0.0018]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0221, -0.0238, -0.0605,  ..., -0.0573, -0.0633,  0.0194],\n        [ 0.0303, -0.0238, -0.0257,  ..., -0.0649,  0.0429,  0.0181],\n        [ 0.0414,  0.0679,  0.0062,  ...,  0.0078,  0.0690, -0.0464],\n        ...,\n        [ 0.0680,  0.0396,  0.0700,  ..., -0.0667, -0.0148,  0.0327],\n        [-0.0019, -0.0210, -0.0542,  ...,  0.0386, -0.0625,  0.0445],\n        [-0.0709, -0.0274, -0.0486,  ..., -0.0666, -0.0082, -0.0389]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0328,  0.0171, -0.0393,  ..., -0.0061,  0.0454, -0.0363],\n        [ 0.0046, -0.0514,  0.0216,  ..., -0.0285,  0.0301,  0.0252],\n        [-0.0710, -0.0350,  0.0355,  ..., -0.0158, -0.0642, -0.0243],\n        ...,\n        [ 0.0570, -0.0685, -0.0672,  ...,  0.0566,  0.0087, -0.0235],\n        [-0.0452, -0.0192,  0.0565,  ...,  0.0475,  0.0009, -0.0562],\n        [ 0.0453, -0.0580,  0.0604,  ...,  0.0353,  0.0693, -0.0268]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.0215, -0.0102, -0.0699,  0.0633, -0.0017,  0.0716,  0.0693, -0.0629,\n        -0.0246, -0.0056,  0.0444, -0.0456,  0.0377,  0.0115,  0.0181, -0.0514,\n        -0.0247, -0.0712, -0.0047, -0.0484,  0.0082, -0.0065, -0.0348, -0.0477,\n        -0.0231,  0.0525, -0.0116, -0.0720,  0.0015, -0.0708, -0.0071, -0.0474,\n        -0.0233,  0.0437,  0.0104, -0.0461, -0.0565, -0.0035,  0.0226,  0.0147,\n         0.0053,  0.0352, -0.0032,  0.0220,  0.0452, -0.0212,  0.0287,  0.0404,\n         0.0146, -0.0405, -0.0232,  0.0585, -0.0463, -0.0508, -0.0437, -0.0427,\n         0.0306,  0.0124, -0.0145,  0.0497, -0.0132, -0.0659, -0.0175, -0.0491,\n        -0.0016, -0.0231,  0.0512, -0.0525, -0.0657,  0.0376,  0.0551, -0.0003,\n         0.0595,  0.0524,  0.0529, -0.0421,  0.0704, -0.0281,  0.0438, -0.0219,\n         0.0507,  0.0525,  0.0005, -0.0256, -0.0004, -0.0630, -0.0469, -0.0118,\n         0.0442,  0.0054,  0.0075,  0.0467, -0.0567,  0.0369,  0.0194, -0.0482,\n         0.0390, -0.0571, -0.0525, -0.0083, -0.0283,  0.0494,  0.0427,  0.0360,\n         0.0164, -0.0100,  0.0055,  0.0558, -0.0271,  0.0017,  0.0109, -0.0659,\n         0.0246,  0.0196, -0.0500,  0.0573, -0.0217,  0.0611, -0.0543,  0.0029,\n         0.0132,  0.0709,  0.0375,  0.0066, -0.0132,  0.0307, -0.0550,  0.0522,\n         0.0364,  0.0624, -0.0011,  0.0150,  0.0299,  0.0113, -0.0041, -0.0460,\n        -0.0178,  0.0590,  0.0643, -0.0532, -0.0506,  0.0194, -0.0619,  0.0629,\n         0.0403, -0.0620,  0.0457, -0.0039,  0.0454,  0.0099, -0.0649, -0.0617,\n        -0.0616, -0.0103,  0.0623,  0.0335, -0.0345, -0.0345, -0.0177, -0.0654,\n         0.0248, -0.0691, -0.0714, -0.0575, -0.0563,  0.0435,  0.0299,  0.0609,\n         0.0041, -0.0044, -0.0529, -0.0452,  0.0191,  0.0030, -0.0361,  0.0310,\n        -0.0575, -0.0252,  0.0268,  0.0661, -0.0030, -0.0042,  0.0369, -0.0640,\n        -0.0232, -0.0231, -0.0252, -0.0705,  0.0312,  0.0117,  0.0464,  0.0531],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0464, -0.0652, -0.0573,  ...,  0.0671,  0.0388, -0.0721],\n        [ 0.0358, -0.0324, -0.0354,  ...,  0.0191, -0.0220,  0.0459],\n        [ 0.0378, -0.0206,  0.0536,  ...,  0.0467,  0.0154,  0.0074],\n        ...,\n        [-0.0385,  0.0514, -0.0266,  ..., -0.0604, -0.0497, -0.0470],\n        [ 0.0241,  0.0170,  0.0516,  ..., -0.0488, -0.0190,  0.0332],\n        [-0.0373,  0.0718, -0.0183,  ...,  0.0089, -0.0093, -0.0661]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-3.5796e-02, -4.4605e-02,  7.1670e-02,  1.9554e-02, -3.0128e-02,\n        -5.6930e-02, -6.4268e-02, -5.7522e-03, -2.0661e-02,  6.5601e-02,\n        -6.8510e-02, -5.4430e-04,  5.2227e-02,  2.6941e-02,  7.4631e-04,\n        -6.3315e-02,  4.1489e-02, -5.6276e-02, -5.1099e-02,  6.4426e-02,\n         4.9009e-03, -1.6876e-02,  6.0998e-02,  6.1558e-03,  6.2970e-02,\n        -3.7822e-02, -6.3259e-02, -3.5508e-02,  4.3210e-02, -1.2020e-02,\n         2.1987e-02, -5.9750e-02,  2.8496e-03, -2.7308e-02, -1.7285e-02,\n        -5.8161e-02, -4.2497e-02,  6.4332e-02,  3.5380e-02,  5.6488e-03,\n        -3.0821e-02,  1.6442e-02, -1.6337e-02, -2.2002e-02, -3.4936e-02,\n         2.7832e-02, -2.4247e-02,  5.8974e-02,  4.8680e-02, -5.5734e-02,\n         6.5074e-03,  3.7330e-02,  2.7828e-02,  5.1527e-02, -6.4852e-02,\n         6.0556e-02, -3.0646e-02,  1.9677e-02,  6.6401e-02, -3.1203e-02,\n         3.5948e-02, -3.2445e-02, -3.6401e-03, -4.0382e-03, -6.7964e-02,\n        -5.5930e-02,  5.6673e-02, -1.4698e-02,  2.9856e-02,  4.6397e-02,\n        -6.1444e-02,  1.3958e-02,  2.0173e-02,  4.0052e-02,  1.2433e-02,\n         3.1213e-02, -1.0562e-02,  5.4547e-03, -1.1849e-02, -4.8257e-02,\n         2.7490e-03,  3.6924e-02,  3.2508e-02,  5.5106e-02,  1.0389e-02,\n        -5.1515e-02, -5.5990e-02,  4.9302e-02, -2.9226e-02, -4.4596e-02,\n        -3.9417e-02,  1.5804e-02, -5.6042e-02, -7.7598e-03, -3.6520e-04,\n         1.0053e-02,  5.7437e-02, -7.6562e-03, -6.7942e-02, -1.5758e-02,\n        -6.6605e-02,  2.6739e-02, -3.6813e-02,  4.7182e-02,  3.3544e-02,\n         1.9272e-02, -5.6709e-02, -4.6204e-02,  2.5063e-03,  7.1045e-03,\n         4.9920e-02,  6.2767e-02, -3.2038e-02, -4.0139e-02, -1.4752e-02,\n        -3.0698e-02,  9.7867e-03,  5.9680e-02,  1.4295e-02,  6.0456e-03,\n         6.5627e-02, -6.7837e-02,  5.2953e-02,  1.4095e-02,  2.5734e-02,\n        -4.0403e-02,  5.0999e-02,  4.9186e-02, -2.7849e-02, -3.5536e-02,\n         1.4860e-02,  4.8433e-02,  9.9009e-04, -2.3380e-02,  8.8287e-03,\n        -3.2619e-02,  7.1673e-02,  1.5888e-02, -4.8848e-02, -5.2295e-02,\n         2.5103e-02,  3.0364e-02,  1.7545e-02, -1.6195e-02, -4.6206e-02,\n         4.4677e-02,  5.7329e-02,  2.0325e-02, -6.6003e-02,  3.9270e-02,\n        -5.9256e-02,  1.9587e-02,  3.2637e-02, -1.2239e-02,  7.1568e-02,\n         1.9009e-02, -4.6799e-02, -7.1897e-02,  8.3654e-03, -2.3882e-02,\n        -6.7194e-02, -6.8812e-03,  2.1477e-02, -1.3630e-03,  6.9233e-02,\n        -4.2444e-02,  2.0903e-02,  5.6510e-02,  1.4475e-02, -2.8194e-02,\n         7.0508e-02,  5.6848e-02, -2.3716e-02,  8.3755e-03, -5.9395e-02,\n        -3.8257e-02, -6.5523e-02, -4.3054e-02,  3.0074e-02,  1.3728e-02,\n        -2.1178e-02,  4.7650e-02,  1.1410e-02,  1.1953e-02, -5.6082e-03,\n        -5.3322e-02, -1.9204e-02, -6.4203e-02, -3.7807e-02,  5.8128e-02,\n         7.1278e-02, -4.5440e-02, -7.9206e-03,  2.2346e-02,  5.1244e-03,\n         2.1285e-02, -5.2341e-02,  2.3021e-03, -2.1634e-02, -4.2182e-02,\n        -1.8789e-03, -4.9117e-02, -4.1026e-02,  1.0426e-03,  6.8188e-02,\n         2.6185e-02,  2.3825e-02,  9.9235e-03,  2.3329e-02,  6.5630e-02,\n        -4.1868e-02, -1.6514e-02, -3.9267e-02, -3.8244e-02, -4.7018e-02,\n         9.1042e-04,  3.1620e-03, -3.0899e-02, -2.7113e-02,  3.1086e-02,\n         4.4507e-02,  3.6098e-02,  5.3432e-02, -6.1199e-02, -6.6789e-03,\n        -5.3240e-02,  3.9602e-02, -1.9900e-02,  2.3527e-02,  2.7570e-02,\n         7.0413e-02,  1.0510e-02, -4.8599e-02, -3.5912e-02,  2.9092e-02,\n         2.0581e-02, -3.8359e-03, -4.8252e-02, -3.3187e-02, -4.0184e-02,\n        -4.4336e-02,  5.5546e-02,  2.7597e-02, -4.2612e-02, -2.5138e-02,\n        -1.5570e-02, -1.3605e-02,  1.7353e-02,  1.7742e-02,  6.1476e-02,\n         3.2060e-02, -3.1323e-02, -4.1764e-02, -4.9923e-02,  2.9474e-02,\n        -3.3634e-02,  1.0876e-02,  2.5400e-02, -3.5561e-02,  1.8487e-03,\n         6.6554e-02, -2.4140e-02,  3.2095e-02,  3.0780e-02, -6.1611e-02,\n        -5.0896e-02, -1.7268e-02,  2.6359e-02,  3.0023e-02,  1.7131e-02,\n        -2.2804e-02,  3.9125e-03,  4.8835e-02, -2.4876e-02, -5.0397e-02,\n         7.0173e-02,  2.9936e-02,  6.1522e-02, -4.8128e-02,  6.6592e-02,\n         5.9583e-02, -1.4625e-02,  2.9762e-02,  3.6382e-02, -3.5665e-02,\n        -9.1086e-04, -5.3643e-02,  4.3499e-02,  5.8681e-04, -5.5067e-02,\n         2.2856e-02,  3.4180e-02,  3.5235e-03, -3.0405e-02,  6.5487e-02,\n         5.3207e-02,  5.4258e-02, -5.5565e-02,  1.8791e-02,  1.0302e-02,\n         6.1560e-02, -5.3673e-02, -1.2222e-03,  4.4365e-02,  2.9491e-02,\n        -7.0976e-02,  5.1715e-02, -4.6343e-02,  3.2107e-02,  7.8805e-03,\n         7.0786e-02,  9.9479e-05,  3.6192e-02,  5.2365e-02,  6.1118e-02,\n        -6.7533e-02, -5.7091e-02,  6.7683e-02,  6.5108e-02,  7.1477e-03,\n         8.2755e-03, -7.2054e-02,  5.6128e-02,  6.9971e-02, -5.9705e-02,\n        -3.8466e-02,  2.0617e-02,  4.7532e-02, -2.6743e-02, -3.7417e-02,\n        -5.5066e-02,  4.6864e-02, -6.1642e-02,  6.4861e-02,  4.6564e-02,\n         1.9841e-02, -6.2143e-02, -2.7605e-02, -2.0212e-02,  5.1124e-02,\n         4.1201e-02,  6.9949e-02, -2.1442e-03,  5.5577e-06, -1.5174e-02,\n        -6.6880e-02, -1.7146e-02,  4.0886e-02, -5.5111e-02,  6.3486e-02,\n        -2.2151e-02,  6.7057e-03, -5.7279e-02, -2.8537e-02,  1.5459e-02,\n         3.5719e-02, -6.9692e-02, -2.5031e-02,  2.5763e-02, -5.3359e-02,\n         5.0825e-02,  1.2684e-02, -7.1732e-02, -1.3924e-04, -1.1699e-02,\n         6.8115e-02, -1.4196e-02, -2.5884e-02,  6.1259e-02,  1.3358e-02,\n         4.1524e-02,  6.3927e-02, -6.7082e-02,  2.2053e-02,  3.5287e-03,\n        -1.0271e-02,  3.1082e-02,  4.3564e-02,  3.8054e-02,  5.1272e-02,\n        -7.2223e-03,  3.3389e-02,  1.8999e-02,  5.4860e-03, -4.6891e-02,\n         3.2197e-02, -6.5627e-02,  5.0755e-02, -4.5935e-02, -4.4522e-02,\n        -5.6276e-02,  1.6882e-02, -8.0682e-03, -3.0338e-02,  2.2993e-02,\n        -6.8484e-02, -3.8071e-03, -1.1749e-02, -4.5821e-02, -1.2445e-02,\n        -2.8664e-02,  7.0934e-02, -3.0580e-02,  3.3333e-02, -2.1709e-02,\n         6.7946e-02, -4.8491e-02, -4.7673e-02,  2.8116e-02, -3.5393e-02,\n        -2.6082e-02,  3.1491e-02,  9.3547e-03, -3.8612e-02, -3.6539e-02,\n        -4.3120e-02,  7.0073e-02, -1.7059e-02,  1.5015e-02, -2.7084e-02,\n        -4.4167e-02,  8.2386e-03,  4.2993e-02, -5.9923e-02,  2.4183e-02,\n         5.6318e-02,  3.1358e-02, -1.6501e-02, -5.5054e-02, -4.8136e-02,\n         5.5204e-02,  6.0936e-03, -1.9354e-02, -2.2070e-02,  1.6934e-02,\n        -2.1265e-02,  7.2913e-03, -1.0685e-02,  5.3190e-02,  3.8814e-02,\n         4.0695e-02, -5.6794e-02, -4.6440e-02,  6.6679e-02, -6.8695e-02,\n         5.7941e-02,  1.7928e-02, -2.4285e-02,  2.7190e-02,  2.1194e-02,\n        -4.0771e-02, -1.1437e-02,  1.7448e-02,  2.3806e-02, -1.6414e-02,\n        -2.9077e-02, -2.8015e-02,  4.6152e-02,  5.9751e-02, -2.9564e-02,\n        -3.6967e-03,  7.0363e-02,  1.9149e-02, -4.4409e-02,  4.2428e-02,\n        -3.8346e-02,  2.6494e-02, -2.3407e-02,  6.7418e-02, -4.8457e-02,\n         1.4284e-02, -3.3531e-02, -3.4876e-02, -6.8474e-02,  6.4348e-02,\n         4.6968e-02,  4.3035e-02, -5.1014e-02,  6.2700e-02,  1.2190e-02,\n        -4.4766e-02, -4.1177e-02, -7.1819e-02,  6.1534e-02,  3.4347e-02,\n        -2.2346e-02,  2.3259e-02,  3.2463e-02, -1.3293e-02,  7.7265e-03,\n         4.8878e-02,  1.4443e-03, -6.6911e-02,  3.1773e-03,  4.4264e-02,\n         1.0208e-02, -2.1699e-02, -1.8146e-02,  2.8451e-02,  8.1887e-03,\n         1.8776e-02,  3.8091e-03, -7.0864e-02,  4.6796e-02, -5.4284e-02,\n         3.7888e-02,  7.0132e-04, -3.9012e-02, -2.8005e-02, -1.9285e-02,\n        -2.1081e-03, -5.3016e-02,  3.6896e-02, -4.7784e-02, -5.4923e-03,\n        -2.8100e-02, -5.4827e-02,  5.5024e-02,  7.0142e-02,  3.5447e-03,\n        -2.8864e-02,  7.0881e-02, -6.2471e-02, -6.1841e-02,  4.1211e-02,\n         3.4345e-02, -1.8638e-03, -6.1076e-02, -2.2770e-02, -4.9835e-02,\n         3.0591e-02,  3.1250e-02,  4.6576e-02, -6.1861e-02, -4.0802e-03,\n         5.5135e-02, -2.2707e-02, -1.2244e-02,  5.1050e-02,  4.2559e-02,\n        -3.1592e-02, -3.0532e-02,  6.7865e-02, -6.5737e-02, -4.3151e-02,\n         3.2381e-02,  1.9372e-02,  6.6355e-02,  1.2428e-03, -2.0578e-02,\n         1.4426e-02,  3.0649e-02, -3.1928e-02,  6.9430e-02,  6.5997e-02,\n        -8.4102e-03,  3.0653e-02, -5.5487e-02, -1.5014e-02,  3.7615e-02,\n        -7.1947e-02, -6.6216e-02,  5.3302e-02, -5.4768e-02,  3.6353e-02,\n        -3.9655e-02, -6.9049e-02,  3.5718e-02,  4.6162e-02, -3.1252e-02,\n         3.2132e-02, -1.0217e-03, -9.5649e-04,  1.0759e-03,  3.5636e-02,\n        -5.0904e-02, -1.1370e-02, -4.3635e-02,  1.0318e-02, -4.9858e-02,\n         4.0282e-02, -4.4720e-02, -5.2814e-02,  2.2010e-02, -6.0467e-02,\n        -1.6109e-02,  9.4293e-03, -6.9467e-02,  5.9226e-02,  3.4478e-02,\n         6.2984e-03, -7.1686e-02, -1.0396e-02,  7.2012e-02,  3.4849e-02,\n        -6.5537e-02, -3.8401e-02,  6.4010e-02,  3.9345e-02,  6.4517e-02,\n        -2.5746e-02,  3.4879e-02, -3.2040e-02, -7.1852e-02,  4.7968e-03,\n         5.9761e-02, -1.5928e-02,  6.6258e-02, -3.7864e-02,  2.2783e-02,\n        -4.8483e-02, -6.1117e-02,  1.4804e-02,  2.6639e-02,  4.3774e-02,\n        -4.0659e-02,  1.3704e-02,  1.6613e-02, -6.1227e-02,  4.8465e-02,\n        -3.7262e-02,  2.9731e-03,  6.8725e-02, -5.9923e-02, -1.8257e-02,\n         1.1651e-02, -7.0015e-02, -5.5777e-02,  4.1385e-03,  2.2621e-02,\n        -1.7171e-02, -6.1917e-02, -2.5862e-02, -6.8805e-02,  6.3968e-02,\n        -6.7398e-02,  2.5401e-02, -3.7151e-02, -6.9501e-02, -4.1059e-02,\n         6.8652e-02, -6.0032e-02,  3.5628e-02, -1.4838e-02,  1.7239e-03,\n         5.0667e-02,  5.0097e-03, -6.9704e-02, -3.8168e-02,  5.8197e-03,\n        -7.2031e-02, -6.5627e-02, -2.5131e-02,  6.5728e-02,  4.0993e-02,\n         3.0582e-02,  4.1938e-02, -2.7475e-02,  2.9112e-02, -1.0672e-02,\n        -2.0838e-02,  6.2814e-02, -6.9695e-02,  6.4868e-02, -5.7654e-02,\n        -4.8565e-02, -1.6750e-02,  6.2061e-02,  1.9872e-02, -4.9023e-02,\n        -5.4124e-02,  5.7963e-02,  1.0866e-02, -3.5367e-02,  4.4121e-02,\n         1.4258e-03,  5.9225e-02,  5.4379e-03,  4.3124e-02,  3.9255e-02,\n         5.2561e-02,  3.3983e-02,  6.7660e-02,  8.6352e-03, -7.1084e-02,\n         2.5601e-02, -3.2736e-02,  1.6785e-02,  2.0871e-02, -5.1651e-02,\n         6.2511e-02,  5.7147e-02,  6.9306e-02,  6.5571e-02, -5.7012e-02,\n        -1.0583e-02, -2.9362e-02, -4.8117e-02,  1.9218e-02, -5.7550e-02,\n         7.1362e-02, -4.5958e-02,  2.6512e-04, -7.9726e-03,  5.9365e-02,\n         3.8867e-02, -8.1955e-03,  1.7233e-02,  4.2370e-02,  3.2001e-02,\n         2.7352e-02,  2.1139e-02, -4.3663e-02,  1.7321e-02, -2.9714e-02,\n         4.7386e-02, -1.4859e-02,  4.3658e-02,  5.5553e-02,  1.9973e-02,\n        -5.0198e-02,  9.7429e-03,  6.3385e-02, -3.1854e-02, -2.8072e-02,\n         1.1708e-02,  4.5008e-03,  9.7104e-03, -2.0165e-03, -3.1987e-02,\n        -1.2289e-02,  6.3166e-02, -1.8587e-02, -4.7307e-03, -6.1824e-02,\n         3.4868e-02,  2.0018e-02, -3.3232e-03,  4.8983e-02, -3.7803e-02,\n         1.5918e-02, -7.0027e-02,  5.4207e-03,  6.0417e-02,  4.8896e-04,\n        -5.0616e-02, -1.3701e-03,  2.5566e-02,  4.4060e-03, -2.6813e-02,\n        -6.6529e-02, -2.7133e-02,  3.0506e-02, -2.7508e-02, -7.4227e-03,\n        -4.6108e-02, -6.7529e-02,  2.1769e-03,  4.2043e-02,  2.8580e-02,\n         4.4012e-02, -5.0091e-02, -4.8299e-02,  6.9809e-02, -6.7303e-02,\n        -4.0472e-02, -1.5423e-02,  3.8979e-02], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([[-0.0189, -0.0112,  0.0216,  ...,  0.0104,  0.0243,  0.0012],\n        [-0.0027, -0.0357,  0.0208,  ...,  0.0203,  0.0035, -0.0186],\n        [-0.0181,  0.0176,  0.0257,  ..., -0.0044,  0.0202,  0.0134],\n        ...,\n        [-0.0179,  0.0103,  0.0070,  ..., -0.0123, -0.0178,  0.0160],\n        [-0.0219, -0.0099, -0.0058,  ...,  0.0199,  0.0250,  0.0304],\n        [ 0.0313, -0.0298, -0.0015,  ..., -0.0164,  0.0039, -0.0253]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.0018,  0.0186, -0.0203, -0.0158,  0.0307, -0.0352, -0.0348, -0.0256,\n         0.0299,  0.0189,  0.0108,  0.0346, -0.0273, -0.0323, -0.0295,  0.0108,\n        -0.0218, -0.0317,  0.0165, -0.0114, -0.0216, -0.0058,  0.0221, -0.0331,\n        -0.0360,  0.0274,  0.0211,  0.0330, -0.0091, -0.0072, -0.0042,  0.0269,\n        -0.0154, -0.0164,  0.0296, -0.0208, -0.0361,  0.0059,  0.0098,  0.0270,\n         0.0228, -0.0250,  0.0200,  0.0343, -0.0018, -0.0220, -0.0079,  0.0312,\n         0.0212,  0.0357,  0.0225,  0.0357, -0.0074,  0.0143,  0.0146, -0.0308,\n         0.0284,  0.0187, -0.0211, -0.0338, -0.0233,  0.0052, -0.0151, -0.0189,\n        -0.0311,  0.0064,  0.0288, -0.0147, -0.0294,  0.0240, -0.0161,  0.0260,\n         0.0093,  0.0308, -0.0021,  0.0047, -0.0141,  0.0213,  0.0110,  0.0250,\n         0.0020,  0.0221, -0.0031,  0.0188,  0.0095,  0.0015,  0.0302,  0.0145,\n        -0.0054, -0.0332,  0.0268, -0.0060,  0.0180,  0.0344,  0.0003, -0.0097,\n         0.0199,  0.0349, -0.0158, -0.0030,  0.0236,  0.0204,  0.0271, -0.0166,\n         0.0002, -0.0004,  0.0218,  0.0360, -0.0222,  0.0273, -0.0288, -0.0271,\n         0.0279, -0.0175,  0.0209, -0.0221, -0.0207,  0.0034, -0.0258, -0.0281,\n         0.0197,  0.0302,  0.0160,  0.0347, -0.0300,  0.0111, -0.0326,  0.0123,\n        -0.0300, -0.0185, -0.0259, -0.0255, -0.0269, -0.0086, -0.0107, -0.0152,\n         0.0166, -0.0004,  0.0144, -0.0126,  0.0109,  0.0092, -0.0108,  0.0143,\n        -0.0236,  0.0159, -0.0085, -0.0099, -0.0122, -0.0341, -0.0025, -0.0002,\n        -0.0320, -0.0148,  0.0190, -0.0104, -0.0008, -0.0178, -0.0219,  0.0268,\n        -0.0148,  0.0354, -0.0201, -0.0335,  0.0320, -0.0264,  0.0288,  0.0173,\n        -0.0357,  0.0273, -0.0183,  0.0242, -0.0030,  0.0264,  0.0116,  0.0352,\n         0.0263,  0.0015, -0.0214,  0.0139, -0.0094, -0.0051,  0.0341,  0.0109,\n         0.0105,  0.0061,  0.0232, -0.0139,  0.0211, -0.0040, -0.0331,  0.0239],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0444,  0.0149,  0.0554,  ..., -0.0004, -0.0202, -0.0499],\n        [-0.0720,  0.0158, -0.0086,  ..., -0.0190, -0.0479,  0.0078],\n        [ 0.0527, -0.0632,  0.0608,  ...,  0.0184,  0.0107, -0.0604],\n        ...,\n        [-0.0695, -0.0009, -0.0034,  ..., -0.0436, -0.0310, -0.0476],\n        [ 0.0344, -0.0359,  0.0159,  ...,  0.0088, -0.0557,  0.0649],\n        [ 0.0234, -0.0086, -0.0270,  ..., -0.0396,  0.0128, -0.0458]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0191,  0.0154, -0.0146,  ...,  0.0046,  0.0449, -0.0720],\n        [ 0.0026,  0.0551,  0.0157,  ..., -0.0019,  0.0297, -0.0487],\n        [ 0.0496, -0.0691,  0.0007,  ..., -0.0632,  0.0444,  0.0384],\n        ...,\n        [-0.0311, -0.0622,  0.0412,  ...,  0.0401, -0.0670, -0.0226],\n        [-0.0526, -0.0503,  0.0165,  ..., -0.0251, -0.0586,  0.0569],\n        [-0.0256,  0.0617,  0.0218,  ..., -0.0113,  0.0370,  0.0604]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0701,  0.0470,  0.0225,  ...,  0.0342, -0.0069,  0.0315],\n        [ 0.0557, -0.0271,  0.0649,  ..., -0.0221,  0.0592, -0.0249],\n        [ 0.0555,  0.0356, -0.0458,  ..., -0.0496,  0.0260,  0.0006],\n        ...,\n        [ 0.0284, -0.0271,  0.0295,  ..., -0.0455,  0.0330,  0.0280],\n        [ 0.0162, -0.0262,  0.0061,  ..., -0.0257, -0.0500, -0.0100],\n        [-0.0238, -0.0511,  0.0476,  ..., -0.0596, -0.0391, -0.0476]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0398, -0.0253,  0.0318,  ..., -0.0216,  0.0717, -0.0233],\n        [-0.0491,  0.0617, -0.0051,  ...,  0.0683,  0.0717, -0.0387],\n        [ 0.0037, -0.0637, -0.0568,  ...,  0.0344, -0.0255,  0.0088],\n        ...,\n        [-0.0082,  0.0199, -0.0164,  ...,  0.0634, -0.0187, -0.0300],\n        [-0.0567,  0.0220,  0.0546,  ..., -0.0294,  0.0605, -0.0681],\n        [-0.0349, -0.0230,  0.0181,  ...,  0.0415, -0.0156,  0.0058]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.0497, -0.0626,  0.0363, -0.0451,  0.0339,  0.0051, -0.0298, -0.0581,\n        -0.0519,  0.0272, -0.0150, -0.0215,  0.0321,  0.0125,  0.0159, -0.0278,\n        -0.0273,  0.0040,  0.0572,  0.0050,  0.0667,  0.0683, -0.0052, -0.0108,\n        -0.0250, -0.0589, -0.0080,  0.0387, -0.0410, -0.0451, -0.0472, -0.0030,\n        -0.0269,  0.0350,  0.0043, -0.0019,  0.0591, -0.0142, -0.0095,  0.0540,\n        -0.0242, -0.0512,  0.0200, -0.0153, -0.0512,  0.0087,  0.0217, -0.0368,\n        -0.0291, -0.0464, -0.0553,  0.0512,  0.0253, -0.0609, -0.0417, -0.0706,\n         0.0706,  0.0670, -0.0048,  0.0157, -0.0496,  0.0144, -0.0514, -0.0162,\n         0.0533, -0.0603,  0.0477,  0.0497,  0.0517,  0.0661,  0.0127, -0.0108,\n        -0.0020,  0.0269, -0.0028, -0.0402,  0.0457, -0.0546, -0.0172, -0.0007,\n         0.0180,  0.0097,  0.0021, -0.0406, -0.0540,  0.0077, -0.0167,  0.0456,\n        -0.0406,  0.0231,  0.0175,  0.0703,  0.0720,  0.0688, -0.0511,  0.0492,\n         0.0437, -0.0069, -0.0143, -0.0584, -0.0006, -0.0377, -0.0425,  0.0297,\n        -0.0194,  0.0583,  0.0236,  0.0346, -0.0572, -0.0005,  0.0401, -0.0332,\n         0.0526, -0.0545,  0.0536, -0.0303, -0.0281, -0.0580,  0.0666, -0.0009,\n         0.0580,  0.0177,  0.0467,  0.0254,  0.0614,  0.0245, -0.0331,  0.0706,\n         0.0516, -0.0607,  0.0313,  0.0168,  0.0220,  0.0238, -0.0236, -0.0033,\n        -0.0537,  0.0441,  0.0681, -0.0466, -0.0706,  0.0105,  0.0059,  0.0120,\n         0.0244, -0.0299, -0.0632,  0.0655,  0.0323, -0.0625, -0.0595, -0.0598,\n        -0.0522,  0.0159, -0.0626, -0.0374, -0.0123, -0.0711, -0.0430,  0.0078,\n         0.0693,  0.0596, -0.0194, -0.0415,  0.0655, -0.0679, -0.0402,  0.0148,\n         0.0043,  0.0009,  0.0177,  0.0321,  0.0027,  0.0702,  0.0673, -0.0197,\n        -0.0218, -0.0545, -0.0295, -0.0679, -0.0262, -0.0714, -0.0141,  0.0443,\n         0.0033,  0.0303,  0.0095, -0.0524, -0.0409,  0.0154,  0.0148,  0.0536],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0228, -0.0581,  0.0646,  ...,  0.0646, -0.0562, -0.0600],\n        [ 0.0087, -0.0136,  0.0394,  ...,  0.0106,  0.0701,  0.0276],\n        [-0.0284,  0.0675,  0.0561,  ...,  0.0197, -0.0056,  0.0489],\n        ...,\n        [-0.0605,  0.0038, -0.0565,  ..., -0.0277,  0.0586,  0.0374],\n        [ 0.0526,  0.0506,  0.0230,  ..., -0.0701,  0.0578,  0.0223],\n        [-0.0167, -0.0401, -0.0480,  ...,  0.0659,  0.0387,  0.0592]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.0149, -0.0568,  0.0455, -0.0502, -0.0546,  0.0075,  0.0404,  0.0200,\n        -0.0063,  0.0540,  0.0280,  0.0266, -0.0298, -0.0163,  0.0422,  0.0429,\n         0.0322, -0.0359,  0.0037, -0.0483,  0.0271, -0.0264, -0.0214, -0.0530,\n        -0.0054,  0.0115,  0.0102, -0.0194, -0.0164,  0.0202,  0.0543, -0.0569,\n        -0.0578, -0.0461,  0.0041,  0.0357,  0.0703,  0.0591, -0.0090,  0.0093,\n         0.0051,  0.0028,  0.0006,  0.0633,  0.0686, -0.0091,  0.0113,  0.0038,\n        -0.0554,  0.0193,  0.0582,  0.0053,  0.0010, -0.0124, -0.0399,  0.0700,\n        -0.0460, -0.0531, -0.0314, -0.0613, -0.0272,  0.0203, -0.0360, -0.0708,\n         0.0600, -0.0587,  0.0584, -0.0278,  0.0334, -0.0525,  0.0070,  0.0104,\n        -0.0685, -0.0608,  0.0571,  0.0420,  0.0674, -0.0687,  0.0255, -0.0653,\n        -0.0698, -0.0127,  0.0193,  0.0046,  0.0711,  0.0211,  0.0624, -0.0370,\n        -0.0696,  0.0291, -0.0541, -0.0467,  0.0487, -0.0469, -0.0032, -0.0293,\n        -0.0367, -0.0235,  0.0458,  0.0636,  0.0264, -0.0416, -0.0432,  0.0093,\n         0.0659, -0.0018, -0.0599, -0.0292,  0.0143, -0.0357, -0.0293, -0.0096,\n        -0.0452, -0.0206, -0.0217, -0.0250, -0.0340,  0.0118, -0.0614, -0.0017,\n         0.0405, -0.0051, -0.0421,  0.0300,  0.0402, -0.0692,  0.0093,  0.0405,\n         0.0307, -0.0402,  0.0397, -0.0568,  0.0634,  0.0057,  0.0623,  0.0306,\n        -0.0008,  0.0458, -0.0060,  0.0357,  0.0220, -0.0596, -0.0357, -0.0228,\n        -0.0302,  0.0402,  0.0114,  0.0170,  0.0565,  0.0446,  0.0523, -0.0549,\n        -0.0206,  0.0381, -0.0066, -0.0647, -0.0557,  0.0476,  0.0620, -0.0214,\n        -0.0280, -0.0057,  0.0192, -0.0005, -0.0005, -0.0441,  0.0355, -0.0005,\n        -0.0135,  0.0425, -0.0238, -0.0613,  0.0062,  0.0387, -0.0215,  0.0144,\n        -0.0417,  0.0716, -0.0054, -0.0125,  0.0114,  0.0427,  0.0050,  0.0336,\n         0.0713,  0.0313,  0.0079, -0.0158, -0.0258, -0.0117, -0.0664, -0.0322,\n         0.0018, -0.0074, -0.0074,  0.0270, -0.0705, -0.0472,  0.0700,  0.0100,\n        -0.0012, -0.0532, -0.0343,  0.0330, -0.0165, -0.0294,  0.0231, -0.0407,\n        -0.0678, -0.0212, -0.0345, -0.0563, -0.0568, -0.0149,  0.0626,  0.0688,\n         0.0510,  0.0334, -0.0197,  0.0403,  0.0690,  0.0051, -0.0412, -0.0225,\n         0.0664,  0.0290,  0.0403, -0.0499, -0.0694, -0.0337, -0.0526, -0.0495,\n         0.0636, -0.0470, -0.0269,  0.0473, -0.0612,  0.0179, -0.0456,  0.0013,\n        -0.0173,  0.0392,  0.0439,  0.0522,  0.0588,  0.0036,  0.0710, -0.0587,\n         0.0272, -0.0129, -0.0687,  0.0628,  0.0658,  0.0099,  0.0105,  0.0439,\n         0.0472,  0.0524, -0.0450,  0.0304, -0.0013,  0.0275, -0.0586,  0.0044,\n        -0.0213, -0.0173, -0.0325,  0.0459,  0.0666, -0.0124,  0.0392, -0.0480,\n         0.0159,  0.0203,  0.0613,  0.0158, -0.0456, -0.0627, -0.0606,  0.0553,\n        -0.0574, -0.0484, -0.0334,  0.0410,  0.0350,  0.0493,  0.0476,  0.0622,\n        -0.0687,  0.0234, -0.0286,  0.0184, -0.0217,  0.0255, -0.0642,  0.0027,\n        -0.0411, -0.0112,  0.0121,  0.0558,  0.0037,  0.0507,  0.0620, -0.0626,\n        -0.0691,  0.0099,  0.0543,  0.0632, -0.0256,  0.0265, -0.0504, -0.0255,\n        -0.0085,  0.0410, -0.0015,  0.0503,  0.0464, -0.0554,  0.0136,  0.0268,\n         0.0070,  0.0198,  0.0715,  0.0600,  0.0656, -0.0520,  0.0022,  0.0403,\n         0.0502, -0.0629, -0.0294, -0.0342, -0.0179,  0.0404,  0.0206, -0.0466,\n        -0.0632,  0.0683, -0.0621, -0.0332, -0.0639,  0.0013, -0.0193,  0.0378,\n        -0.0636,  0.0230,  0.0033,  0.0027,  0.0320,  0.0660,  0.0478, -0.0586,\n         0.0049,  0.0313, -0.0328, -0.0174, -0.0006, -0.0622,  0.0631, -0.0035,\n         0.0242, -0.0178, -0.0213,  0.0700,  0.0607,  0.0224,  0.0036,  0.0187,\n         0.0339, -0.0454,  0.0188,  0.0315,  0.0696,  0.0213, -0.0606,  0.0693,\n         0.0147, -0.0506,  0.0531, -0.0513, -0.0623, -0.0610, -0.0385, -0.0237,\n        -0.0343,  0.0159, -0.0069,  0.0194, -0.0571,  0.0487, -0.0557, -0.0510,\n        -0.0693,  0.0668,  0.0681,  0.0247,  0.0072,  0.0194,  0.0060, -0.0093,\n        -0.0071,  0.0671,  0.0099,  0.0463, -0.0291,  0.0376,  0.0294, -0.0191,\n         0.0066, -0.0354, -0.0550,  0.0541, -0.0047,  0.0151, -0.0414, -0.0517,\n         0.0533, -0.0608,  0.0063,  0.0395,  0.0674,  0.0043,  0.0040, -0.0182,\n         0.0044, -0.0585,  0.0718, -0.0684,  0.0093,  0.0237, -0.0513,  0.0276,\n         0.0061, -0.0382,  0.0390,  0.0285, -0.0092, -0.0573, -0.0704,  0.0553,\n         0.0331, -0.0213,  0.0092,  0.0008,  0.0702, -0.0541,  0.0349,  0.0270,\n         0.0251,  0.0695,  0.0115, -0.0248,  0.0500, -0.0244,  0.0530,  0.0606,\n         0.0176, -0.0107, -0.0036,  0.0342,  0.0450, -0.0696,  0.0336,  0.0279,\n        -0.0200,  0.0659,  0.0355, -0.0082, -0.0042, -0.0202,  0.0334,  0.0351,\n        -0.0291,  0.0306, -0.0466,  0.0544, -0.0281, -0.0025,  0.0675, -0.0253,\n         0.0037,  0.0064,  0.0293,  0.0115,  0.0276,  0.0229, -0.0388, -0.0292,\n        -0.0372, -0.0195,  0.0164, -0.0082, -0.0479, -0.0265, -0.0079,  0.0551,\n         0.0173,  0.0508, -0.0397,  0.0409, -0.0593, -0.0552,  0.0034,  0.0098,\n         0.0243,  0.0141, -0.0094, -0.0324,  0.0572, -0.0428, -0.0397, -0.0549,\n         0.0268,  0.0546,  0.0585, -0.0471,  0.0721, -0.0295, -0.0663,  0.0697,\n         0.0556, -0.0445,  0.0670,  0.0676, -0.0403,  0.0586, -0.0198,  0.0491,\n        -0.0165, -0.0453,  0.0499, -0.0625, -0.0191,  0.0431,  0.0244, -0.0635,\n         0.0248,  0.0426,  0.0660, -0.0570, -0.0502,  0.0569, -0.0601,  0.0331,\n         0.0526, -0.0662, -0.0250, -0.0315,  0.0486,  0.0696,  0.0429, -0.0504,\n        -0.0375,  0.0006,  0.0110,  0.0118,  0.0162, -0.0306, -0.0252,  0.0137,\n         0.0126,  0.0039, -0.0067,  0.0003, -0.0649, -0.0092, -0.0492, -0.0653,\n         0.0548,  0.0605,  0.0523,  0.0382, -0.0574,  0.0676,  0.0374, -0.0094,\n         0.0008, -0.0404,  0.0442, -0.0475, -0.0542, -0.0579,  0.0420, -0.0447,\n         0.0129,  0.0584,  0.0138, -0.0611, -0.0595, -0.0712,  0.0526, -0.0631,\n        -0.0342, -0.0040, -0.0384, -0.0361, -0.0002, -0.0048,  0.0321, -0.0503,\n         0.0493,  0.0641, -0.0294, -0.0531,  0.0195,  0.0207, -0.0353, -0.0027,\n        -0.0521,  0.0033,  0.0140, -0.0337, -0.0392, -0.0368, -0.0171, -0.0491,\n        -0.0502, -0.0243, -0.0330, -0.0356,  0.0055, -0.0519, -0.0650, -0.0132,\n         0.0079,  0.0634, -0.0002, -0.0336,  0.0487, -0.0470,  0.0686,  0.0509,\n         0.0604, -0.0680,  0.0317, -0.0091, -0.0140, -0.0296, -0.0405, -0.0090,\n        -0.0124,  0.0469, -0.0115,  0.0024,  0.0058,  0.0505,  0.0687, -0.0486,\n        -0.0512,  0.0374,  0.0616, -0.0280, -0.0568,  0.0677,  0.0273,  0.0598,\n         0.0524, -0.0451,  0.0192, -0.0211, -0.0465, -0.0454,  0.0659,  0.0519,\n         0.0153,  0.0263, -0.0276, -0.0403, -0.0506, -0.0326,  0.0486, -0.0613,\n        -0.0476,  0.0004, -0.0158, -0.0176,  0.0538, -0.0041, -0.0670,  0.0595,\n        -0.0663, -0.0137, -0.0538, -0.0549, -0.0683, -0.0195, -0.0082,  0.0598,\n         0.0297,  0.0396, -0.0123,  0.0663,  0.0501, -0.0583, -0.0032, -0.0441,\n        -0.0619,  0.0412,  0.0664, -0.0406,  0.0447, -0.0149,  0.0179,  0.0052,\n        -0.0322, -0.0049,  0.0710, -0.0202,  0.0186, -0.0453, -0.0134,  0.0598,\n         0.0582, -0.0367,  0.0135,  0.0151, -0.0400, -0.0027, -0.0613, -0.0379,\n        -0.0364,  0.0632,  0.0254,  0.0153, -0.0426, -0.0651,  0.0020,  0.0316,\n        -0.0277, -0.0156,  0.0458, -0.0455,  0.0242,  0.0214, -0.0474,  0.0404,\n         0.0608,  0.0465, -0.0145, -0.0561,  0.0253,  0.0582,  0.0220, -0.0567,\n         0.0616, -0.0680,  0.0303,  0.0121,  0.0228,  0.0360, -0.0333,  0.0554,\n         0.0018, -0.0100,  0.0583,  0.0568,  0.0302, -0.0404, -0.0578,  0.0049,\n         0.0148, -0.0618,  0.0092,  0.0651, -0.0539,  0.0667, -0.0220,  0.0140],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0086, -0.0210,  0.0035,  ...,  0.0216, -0.0336,  0.0338],\n        [-0.0299, -0.0103,  0.0061,  ...,  0.0147, -0.0331,  0.0172],\n        [-0.0303, -0.0150,  0.0088,  ..., -0.0163, -0.0120, -0.0336],\n        ...,\n        [ 0.0180, -0.0197,  0.0018,  ...,  0.0037, -0.0267,  0.0009],\n        [-0.0055, -0.0063,  0.0198,  ...,  0.0108,  0.0192, -0.0181],\n        [-0.0283, -0.0173, -0.0156,  ...,  0.0341, -0.0089, -0.0024]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-2.6209e-02, -1.3136e-02,  3.0148e-02,  1.8242e-02, -3.9477e-04,\n         3.5938e-03, -2.5230e-02, -3.1558e-02,  2.0435e-02, -2.0509e-02,\n         2.1839e-02, -1.7418e-02,  3.1575e-02,  2.3750e-02,  7.4223e-03,\n        -2.2016e-02, -2.0437e-02,  1.1545e-02, -1.2683e-02, -1.3423e-02,\n         1.9813e-02, -6.7136e-03,  7.5757e-03, -6.5343e-03,  3.4119e-02,\n        -9.8701e-03,  2.1028e-02, -6.5566e-03, -1.0548e-02,  2.3140e-02,\n         1.3346e-02,  1.8314e-02,  3.0567e-02, -7.2021e-03,  2.6884e-02,\n        -5.6890e-03,  1.0742e-02, -1.8482e-02,  9.5987e-03, -3.7315e-03,\n        -7.4138e-03, -2.3452e-02,  2.1031e-02, -7.7432e-04,  9.8844e-03,\n        -1.6682e-02,  1.1879e-02,  3.2236e-02,  5.9274e-03,  2.5328e-02,\n        -9.4121e-03,  2.7904e-02,  3.4517e-02, -2.9685e-02,  3.2091e-02,\n         3.4058e-02, -5.4220e-03,  1.4308e-02, -1.9569e-02,  4.9613e-03,\n         2.3058e-02, -7.0660e-04,  3.2691e-02,  9.8980e-03, -1.1645e-02,\n        -6.5414e-03,  2.8428e-02,  2.8954e-03, -3.7987e-03, -2.2155e-02,\n         3.3621e-02, -2.6078e-02,  3.4040e-02, -3.5968e-02,  4.6871e-03,\n         2.6140e-03, -3.5405e-02, -9.8613e-03, -1.9381e-02, -2.9360e-02,\n         3.9532e-03,  2.3706e-03, -1.6544e-02,  8.7348e-03,  3.4518e-02,\n         2.6034e-02, -1.0460e-02, -2.4085e-02, -2.8240e-02,  2.7841e-02,\n         2.4508e-02, -3.3909e-02,  3.1961e-02,  2.7040e-02,  1.0420e-02,\n        -2.8918e-02, -2.9904e-02, -2.1315e-02,  2.9277e-02, -3.5266e-02,\n         1.8925e-02, -2.5495e-02,  3.1557e-02,  2.9218e-02, -1.8452e-02,\n         1.6897e-02,  3.0132e-02, -3.2242e-03,  9.9001e-04,  3.4369e-02,\n        -2.5805e-02, -1.2381e-02, -2.1727e-02, -8.8711e-03, -3.4004e-02,\n        -1.3047e-02,  4.5355e-03, -8.2767e-03, -1.7858e-02, -3.5432e-02,\n        -2.2320e-02,  1.9274e-02, -6.2793e-03,  7.3748e-03,  1.4397e-02,\n         2.0837e-02,  3.3389e-02,  1.6422e-02, -2.4904e-02, -6.4202e-03,\n        -4.8686e-03,  1.1627e-02, -2.2956e-02, -1.9892e-02, -1.7199e-02,\n         1.0156e-02,  7.2774e-05,  3.1858e-02,  2.7334e-02,  1.6992e-03,\n         3.5002e-02, -4.6091e-03,  1.3761e-02,  1.2367e-03, -2.8234e-02,\n         2.5151e-03,  1.5268e-02, -1.8446e-02, -2.0397e-02,  2.2540e-03,\n         2.3800e-02, -1.4575e-02,  5.5210e-03,  1.1248e-02,  6.3729e-03,\n        -4.4881e-03,  2.5066e-02,  1.2429e-02,  1.8763e-02,  1.9274e-02,\n        -2.9453e-02, -3.3864e-02, -1.7102e-02,  2.9583e-02,  1.8221e-02,\n         2.0150e-02,  2.7717e-02,  4.4614e-03,  1.1270e-02,  2.5148e-02,\n         3.6452e-03, -1.9161e-02,  2.7333e-02,  1.3521e-02, -2.2264e-02,\n         2.2278e-02,  5.5639e-03,  8.1456e-03, -2.2028e-03,  3.7060e-03,\n         1.5565e-02,  2.6928e-02,  1.4913e-02, -7.3170e-04,  3.4676e-02,\n        -2.3538e-02, -2.2506e-02, -2.6783e-02, -1.4949e-02,  3.1141e-02,\n        -3.5803e-02, -3.4460e-02], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0276, -0.0675, -0.0270,  ..., -0.0709,  0.0005, -0.0132],\n        [ 0.0201,  0.0633, -0.0247,  ..., -0.0350,  0.0172, -0.0512],\n        [ 0.0658,  0.0275,  0.0209,  ..., -0.0126,  0.0683,  0.0150],\n        ...,\n        [-0.0425, -0.0612,  0.0362,  ...,  0.0458,  0.0116,  0.0226],\n        [-0.0272, -0.0187,  0.0250,  ..., -0.0404, -0.0286, -0.0308],\n        [ 0.0599, -0.0393, -0.0227,  ..., -0.0037,  0.0173, -0.0083]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0395,  0.0123,  0.0566,  ..., -0.0450,  0.0549, -0.0700],\n        [ 0.0194,  0.0606,  0.0686,  ..., -0.0022, -0.0418,  0.0602],\n        [ 0.0220, -0.0631,  0.0202,  ..., -0.0522, -0.0689, -0.0535],\n        ...,\n        [ 0.0048,  0.0540, -0.0188,  ...,  0.0610,  0.0029,  0.0035],\n        [-0.0093, -0.0162,  0.0350,  ..., -0.0376,  0.0172,  0.0325],\n        [-0.0541, -0.0607,  0.0328,  ..., -0.0247, -0.0171,  0.0127]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0705,  0.0555,  0.0179,  ...,  0.0126, -0.0684,  0.0687],\n        [ 0.0506,  0.0693, -0.0384,  ..., -0.0052,  0.0458, -0.0021],\n        [-0.0293, -0.0678, -0.0505,  ..., -0.0580, -0.0426,  0.0266],\n        ...,\n        [-0.0484, -0.0666, -0.0307,  ...,  0.0667,  0.0247, -0.0456],\n        [-0.0409, -0.0214,  0.0672,  ..., -0.0192,  0.0142, -0.0317],\n        [-0.0580,  0.0231,  0.0431,  ..., -0.0291,  0.0068, -0.0162]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0620,  0.0087, -0.0164,  ...,  0.0454, -0.0558,  0.0058],\n        [-0.0567,  0.0095,  0.0039,  ...,  0.0246, -0.0416,  0.0648],\n        [-0.0626, -0.0378,  0.0417,  ..., -0.0548,  0.0232,  0.0117],\n        ...,\n        [ 0.0157,  0.0453,  0.0061,  ..., -0.0659,  0.0271,  0.0470],\n        [ 0.0577,  0.0287, -0.0061,  ...,  0.0336, -0.0508,  0.0613],\n        [ 0.0185, -0.0523,  0.0128,  ...,  0.0175, -0.0175, -0.0198]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 0.0451, -0.0450, -0.0163, -0.0258, -0.0193,  0.0168, -0.0071, -0.0434,\n         0.0095,  0.0694, -0.0327, -0.0435, -0.0629,  0.0068, -0.0103, -0.0274,\n        -0.0470,  0.0592,  0.0374,  0.0304,  0.0206,  0.0565, -0.0677, -0.0558,\n         0.0119,  0.0089,  0.0493,  0.0447,  0.0653,  0.0567,  0.0586,  0.0317,\n         0.0527,  0.0671, -0.0419,  0.0279,  0.0615, -0.0479, -0.0024, -0.0511,\n         0.0098, -0.0177,  0.0073, -0.0620,  0.0537,  0.0467,  0.0658,  0.0119,\n         0.0260, -0.0011, -0.0308, -0.0234,  0.0118,  0.0183,  0.0175,  0.0294,\n        -0.0484, -0.0572,  0.0105,  0.0716,  0.0596, -0.0053,  0.0661,  0.0668,\n         0.0477,  0.0160, -0.0453,  0.0391, -0.0242, -0.0240, -0.0254,  0.0253,\n         0.0473,  0.0346,  0.0495,  0.0084, -0.0421,  0.0475, -0.0003, -0.0703,\n        -0.0231,  0.0710, -0.0296,  0.0011, -0.0602, -0.0657,  0.0290, -0.0656,\n        -0.0385, -0.0660, -0.0104, -0.0023, -0.0686,  0.0075, -0.0382, -0.0617,\n         0.0097,  0.0545,  0.0133,  0.0136, -0.0652,  0.0263, -0.0177, -0.0115,\n         0.0633,  0.0406,  0.0289, -0.0632, -0.0394, -0.0615,  0.0104, -0.0516,\n         0.0059,  0.0476, -0.0469,  0.0535, -0.0650,  0.0298, -0.0614,  0.0664,\n        -0.0627, -0.0683,  0.0693,  0.0463, -0.0285, -0.0592, -0.0179, -0.0709,\n        -0.0636,  0.0072, -0.0703,  0.0606, -0.0384,  0.0192, -0.0613,  0.0623,\n         0.0403,  0.0612, -0.0380, -0.0201,  0.0078, -0.0375, -0.0088, -0.0379,\n        -0.0490,  0.0137, -0.0248,  0.0361, -0.0318,  0.0250, -0.0073, -0.0503,\n         0.0465,  0.0283,  0.0069, -0.0049, -0.0219,  0.0408,  0.0019,  0.0186,\n        -0.0233,  0.0549,  0.0228,  0.0164, -0.0226, -0.0556,  0.0440,  0.0369,\n        -0.0262,  0.0220, -0.0653,  0.0046,  0.0676, -0.0373, -0.0297,  0.0563,\n        -0.0135,  0.0016, -0.0470,  0.0682, -0.0681, -0.0051, -0.0638, -0.0212,\n        -0.0665,  0.0689,  0.0527, -0.0211,  0.0165,  0.0553,  0.0388,  0.0128],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0162, -0.0478, -0.0071,  ...,  0.0226, -0.0517,  0.0519],\n        [ 0.0673,  0.0152, -0.0047,  ..., -0.0260,  0.0036, -0.0213],\n        [ 0.0519, -0.0385, -0.0721,  ...,  0.0067, -0.0378, -0.0430],\n        ...,\n        [ 0.0062,  0.0682,  0.0470,  ...,  0.0463,  0.0476, -0.0127],\n        [-0.0521,  0.0176, -0.0574,  ..., -0.0443,  0.0174,  0.0485],\n        [ 0.0322, -0.0155,  0.0025,  ..., -0.0217, -0.0203,  0.0161]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 0.0308,  0.0520, -0.0688, -0.0639,  0.0705,  0.0636,  0.0199, -0.0458,\n         0.0246, -0.0607, -0.0509, -0.0631,  0.0228, -0.0684,  0.0456,  0.0045,\n         0.0436,  0.0690,  0.0492, -0.0285,  0.0171, -0.0282, -0.0456,  0.0040,\n         0.0250,  0.0291,  0.0329, -0.0402, -0.0598, -0.0100, -0.0266,  0.0302,\n         0.0309, -0.0674, -0.0064, -0.0252, -0.0529, -0.0382, -0.0315, -0.0449,\n        -0.0676, -0.0690,  0.0161,  0.0343,  0.0156, -0.0354,  0.0624, -0.0714,\n        -0.0672,  0.0325, -0.0236,  0.0342, -0.0691,  0.0053, -0.0697, -0.0324,\n         0.0164, -0.0291, -0.0018, -0.0479, -0.0429, -0.0520,  0.0204,  0.0411,\n        -0.0235,  0.0039,  0.0704,  0.0652, -0.0098,  0.0247,  0.0079,  0.0409,\n        -0.0606,  0.0110, -0.0544,  0.0550, -0.0179, -0.0595,  0.0112,  0.0414,\n        -0.0546,  0.0396, -0.0273, -0.0592,  0.0143, -0.0066,  0.0672,  0.0482,\n         0.0364, -0.0645, -0.0039,  0.0562,  0.0596, -0.0706, -0.0222, -0.0608,\n         0.0573,  0.0583,  0.0578, -0.0678,  0.0422,  0.0416, -0.0628, -0.0172,\n        -0.0501,  0.0430, -0.0031,  0.0669,  0.0368, -0.0461,  0.0515, -0.0024,\n         0.0575,  0.0241, -0.0469,  0.0324,  0.0503,  0.0008, -0.0241,  0.0195,\n        -0.0118,  0.0306,  0.0494,  0.0612,  0.0519, -0.0389,  0.0381, -0.0356,\n        -0.0610,  0.0441, -0.0355,  0.0666,  0.0182,  0.0551,  0.0717,  0.0126,\n        -0.0361, -0.0122,  0.0367,  0.0237, -0.0608, -0.0233,  0.0494, -0.0504,\n         0.0367,  0.0704, -0.0557, -0.0199, -0.0194, -0.0041, -0.0645, -0.0629,\n        -0.0225, -0.0360, -0.0640,  0.0251,  0.0389,  0.0371, -0.0453,  0.0100,\n        -0.0563, -0.0562,  0.0405, -0.0530, -0.0236,  0.0527, -0.0318, -0.0350,\n        -0.0227, -0.0501, -0.0487,  0.0230,  0.0658, -0.0365,  0.0655, -0.0067,\n        -0.0309,  0.0698, -0.0153,  0.0710, -0.0618,  0.0528, -0.0140,  0.0293,\n        -0.0288,  0.0028,  0.0166, -0.0608,  0.0391,  0.0206, -0.0312,  0.0711,\n        -0.0481,  0.0620, -0.0135, -0.0059,  0.0067, -0.0424,  0.0469, -0.0506,\n        -0.0266, -0.0397,  0.0175, -0.0507, -0.0212, -0.0267,  0.0697,  0.0536,\n         0.0210,  0.0085, -0.0372, -0.0227, -0.0059,  0.0565, -0.0710, -0.0188,\n         0.0169,  0.0549, -0.0186, -0.0293,  0.0181,  0.0669, -0.0602, -0.0082,\n         0.0092,  0.0450,  0.0289, -0.0287,  0.0035, -0.0022, -0.0005,  0.0561,\n         0.0318,  0.0089,  0.0583,  0.0487,  0.0422,  0.0095, -0.0136,  0.0687,\n         0.0575,  0.0312,  0.0581,  0.0557,  0.0586,  0.0114,  0.0225, -0.0511,\n         0.0645, -0.0462,  0.0645, -0.0554,  0.0393,  0.0395, -0.0312,  0.0321,\n        -0.0618, -0.0270, -0.0114, -0.0280, -0.0451, -0.0491,  0.0112, -0.0490,\n         0.0373, -0.0068,  0.0696,  0.0066, -0.0066,  0.0043, -0.0386,  0.0366,\n        -0.0057, -0.0323, -0.0283,  0.0367, -0.0178,  0.0071,  0.0106,  0.0340,\n         0.0035,  0.0266,  0.0136,  0.0534, -0.0021, -0.0060,  0.0509,  0.0082,\n        -0.0104,  0.0274,  0.0627,  0.0523, -0.0367,  0.0632, -0.0138,  0.0304,\n        -0.0410, -0.0716, -0.0065,  0.0176,  0.0195, -0.0651,  0.0492, -0.0206,\n         0.0196, -0.0585,  0.0287,  0.0543, -0.0653,  0.0517,  0.0449,  0.0477,\n        -0.0465, -0.0525,  0.0586, -0.0310,  0.0328, -0.0240, -0.0131, -0.0311,\n         0.0186,  0.0311, -0.0561,  0.0607,  0.0395,  0.0559, -0.0068,  0.0002,\n         0.0136,  0.0196,  0.0692,  0.0165,  0.0145, -0.0234, -0.0178,  0.0553,\n         0.0683, -0.0706, -0.0177,  0.0651, -0.0564, -0.0449,  0.0143, -0.0329,\n        -0.0697,  0.0341, -0.0520,  0.0273,  0.0511,  0.0680, -0.0450,  0.0566,\n        -0.0333,  0.0252, -0.0705, -0.0588,  0.0019,  0.0524,  0.0691, -0.0028,\n         0.0278,  0.0046, -0.0717, -0.0097, -0.0004,  0.0510,  0.0251,  0.0216,\n        -0.0054, -0.0227,  0.0575, -0.0293, -0.0289, -0.0211,  0.0232, -0.0498,\n         0.0438, -0.0443, -0.0671, -0.0512, -0.0466, -0.0179, -0.0116,  0.0386,\n         0.0412, -0.0318, -0.0122, -0.0316, -0.0507, -0.0356, -0.0683,  0.0539,\n        -0.0210, -0.0274,  0.0279, -0.0390,  0.0169,  0.0351, -0.0122, -0.0577,\n        -0.0449, -0.0160,  0.0018, -0.0584, -0.0353,  0.0225, -0.0622,  0.0719,\n         0.0579,  0.0601,  0.0420, -0.0337, -0.0654, -0.0353,  0.0469,  0.0489,\n        -0.0608,  0.0081,  0.0072, -0.0604, -0.0533,  0.0094, -0.0596,  0.0417,\n         0.0361,  0.0094,  0.0595, -0.0109,  0.0526,  0.0586,  0.0259, -0.0393,\n         0.0171,  0.0036,  0.0289, -0.0453,  0.0531, -0.0187,  0.0149,  0.0612,\n         0.0636, -0.0390,  0.0426,  0.0126, -0.0546, -0.0068, -0.0115,  0.0186,\n         0.0587, -0.0598,  0.0461, -0.0317, -0.0117,  0.0438,  0.0220, -0.0096,\n         0.0038,  0.0443,  0.0056, -0.0231,  0.0261, -0.0718,  0.0166, -0.0236,\n        -0.0025, -0.0075, -0.0603, -0.0190,  0.0566,  0.0005, -0.0161,  0.0144,\n         0.0265, -0.0134,  0.0213,  0.0620,  0.0152,  0.0481, -0.0217,  0.0422,\n        -0.0261, -0.0319,  0.0082,  0.0203, -0.0039, -0.0255,  0.0386, -0.0192,\n        -0.0385, -0.0113, -0.0254, -0.0337, -0.0107,  0.0016, -0.0567, -0.0147,\n        -0.0296,  0.0598, -0.0665, -0.0105, -0.0440, -0.0250,  0.0024, -0.0145,\n        -0.0286, -0.0077, -0.0009, -0.0388,  0.0646, -0.0026, -0.0225, -0.0459,\n        -0.0550,  0.0199,  0.0258,  0.0427,  0.0493, -0.0376,  0.0452, -0.0083,\n         0.0702,  0.0152,  0.0122, -0.0312,  0.0611, -0.0598, -0.0217,  0.0432,\n         0.0264,  0.0363, -0.0212, -0.0570,  0.0326,  0.0149,  0.0315,  0.0404,\n        -0.0406,  0.0152, -0.0394,  0.0375,  0.0686, -0.0556,  0.0532, -0.0331,\n        -0.0138,  0.0152, -0.0484, -0.0191, -0.0101, -0.0112, -0.0061,  0.0124,\n         0.0319, -0.0065, -0.0237, -0.0159,  0.0661, -0.0411, -0.0332, -0.0536,\n        -0.0389,  0.0241, -0.0275, -0.0550, -0.0514,  0.0422, -0.0207,  0.0448,\n         0.0118,  0.0624,  0.0624,  0.0006,  0.0520, -0.0369, -0.0637, -0.0105,\n         0.0589, -0.0273,  0.0554, -0.0434, -0.0018, -0.0556, -0.0230, -0.0375,\n        -0.0621, -0.0621,  0.0669, -0.0578, -0.0704,  0.0689,  0.0360,  0.0058,\n        -0.0259, -0.0703, -0.0371, -0.0216,  0.0249,  0.0133,  0.0676, -0.0562,\n        -0.0533, -0.0573, -0.0215,  0.0122,  0.0604, -0.0037,  0.0585,  0.0373,\n        -0.0631, -0.0110,  0.0594,  0.0657,  0.0232, -0.0504, -0.0344, -0.0394,\n        -0.0266, -0.0647,  0.0269, -0.0231, -0.0589,  0.0107, -0.0494,  0.0270,\n        -0.0018,  0.0247, -0.0205, -0.0492,  0.0410,  0.0260,  0.0704, -0.0551,\n         0.0407,  0.0662,  0.0560,  0.0154,  0.0158, -0.0642, -0.0010,  0.0240,\n        -0.0451, -0.0401, -0.0234, -0.0167,  0.0010, -0.0380, -0.0268, -0.0065,\n         0.0169,  0.0147,  0.0709, -0.0066, -0.0108,  0.0074,  0.0031, -0.0649,\n        -0.0427,  0.0668,  0.0142, -0.0103,  0.0678,  0.0632, -0.0080, -0.0366,\n        -0.0334, -0.0421,  0.0236,  0.0591, -0.0606,  0.0295, -0.0411,  0.0418,\n        -0.0209,  0.0312, -0.0407,  0.0459, -0.0245, -0.0597, -0.0699, -0.0043,\n        -0.0714,  0.0071,  0.0057,  0.0105, -0.0184, -0.0223, -0.0249,  0.0610,\n        -0.0241, -0.0378,  0.0077,  0.0074,  0.0668,  0.0246,  0.0392, -0.0573,\n        -0.0651, -0.0178, -0.0134, -0.0664, -0.0674,  0.0440, -0.0409, -0.0706,\n         0.0550,  0.0228, -0.0416, -0.0112,  0.0326, -0.0658, -0.0115,  0.0478,\n        -0.0502, -0.0380, -0.0004,  0.0242, -0.0367, -0.0196, -0.0578, -0.0178,\n         0.0186,  0.0585,  0.0451,  0.0358,  0.0412,  0.0189,  0.0139, -0.0423,\n        -0.0669,  0.0656, -0.0415,  0.0275,  0.0498,  0.0186, -0.0239, -0.0345,\n         0.0593,  0.0136,  0.0085,  0.0455,  0.0243, -0.0609, -0.0694,  0.0572,\n         0.0477,  0.0406, -0.0136, -0.0431,  0.0173,  0.0303, -0.0154,  0.0225,\n         0.0483,  0.0089, -0.0370, -0.0538,  0.0621, -0.0692, -0.0666,  0.0149,\n         0.0549,  0.0603,  0.0495,  0.0070,  0.0425, -0.0306,  0.0692, -0.0577],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0141,  0.0277, -0.0191,  ..., -0.0075, -0.0115,  0.0252],\n        [-0.0270, -0.0194,  0.0281,  ...,  0.0121,  0.0222, -0.0009],\n        [-0.0067,  0.0268,  0.0246,  ..., -0.0299,  0.0296, -0.0188],\n        ...,\n        [-0.0062, -0.0258, -0.0259,  ..., -0.0249,  0.0072, -0.0020],\n        [ 0.0216,  0.0080,  0.0184,  ..., -0.0212, -0.0088,  0.0271],\n        [-0.0224,  0.0292,  0.0067,  ..., -0.0268,  0.0095,  0.0169]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-2.6919e-03,  3.3172e-02, -3.2157e-02, -8.4823e-03, -3.3292e-02,\n        -1.0155e-02,  7.1378e-03, -3.4303e-03,  1.6300e-02, -2.3503e-02,\n        -2.3473e-02, -1.0079e-02,  2.5819e-03,  2.6181e-02, -1.9373e-02,\n        -1.3470e-02,  2.0110e-02,  1.5102e-02,  3.0977e-02,  2.4555e-02,\n        -2.3758e-02, -2.0089e-02,  5.0040e-03, -1.7817e-02, -1.2674e-03,\n         2.0766e-02, -1.3540e-02,  1.5411e-02,  1.3300e-02,  5.1689e-03,\n         2.9908e-02,  1.5646e-02, -1.3757e-02,  1.1227e-02,  1.8508e-02,\n        -1.5554e-02,  4.0577e-03, -1.7877e-02, -6.2249e-03,  1.9287e-02,\n         6.2257e-03, -3.5822e-02,  1.5774e-02,  1.9126e-02, -9.1001e-03,\n        -2.3785e-02,  2.0111e-04, -1.6818e-02,  2.8955e-03,  2.3075e-02,\n        -1.4320e-02,  2.8960e-02, -1.3173e-02, -2.3716e-02,  8.7392e-03,\n         1.4449e-02,  1.2122e-02, -2.9311e-02, -6.6146e-03,  3.0394e-02,\n         9.6105e-03, -2.5176e-02, -1.7840e-02, -2.5223e-02, -1.3183e-02,\n         1.9078e-03,  3.4958e-02, -5.3788e-04, -2.6139e-02,  3.0334e-03,\n        -1.2386e-02,  3.2252e-02, -1.6564e-02,  2.0335e-03,  3.1008e-02,\n        -2.9257e-03,  3.0090e-02, -1.4621e-02,  3.4380e-02, -9.5859e-03,\n        -1.3854e-02, -1.8705e-02, -1.5735e-02, -3.3527e-02,  3.0349e-02,\n        -2.3715e-02,  1.3156e-02,  3.7679e-03, -2.7012e-02, -2.5282e-02,\n        -5.7459e-04,  1.9381e-02, -1.8913e-02, -1.1453e-02, -2.7948e-02,\n        -9.7503e-03, -1.6191e-03, -1.8871e-02,  2.1387e-02,  3.4070e-02,\n         1.9785e-02,  1.4567e-02, -2.1206e-02, -3.5405e-02,  2.8568e-02,\n        -1.7946e-02,  1.7209e-02,  1.4607e-02, -7.1398e-03,  2.6121e-02,\n        -2.5371e-02, -8.4889e-03, -1.2833e-02,  2.4916e-02,  2.3555e-02,\n         6.7543e-03, -8.0085e-04,  4.4643e-03, -1.4076e-02, -6.9053e-03,\n        -1.8058e-02, -2.9329e-02, -3.0839e-02, -1.7702e-02,  5.8765e-03,\n         1.2231e-02,  1.8364e-02,  3.5035e-02, -3.4973e-02,  1.0463e-02,\n         2.2515e-05,  1.8639e-02, -2.5658e-02, -2.9146e-02, -1.7736e-02,\n        -1.4458e-02,  2.0908e-02,  7.8986e-03, -3.4800e-02, -2.2602e-02,\n        -1.5533e-02, -2.7919e-02, -1.1643e-02,  2.2164e-02,  1.1939e-02,\n        -1.9797e-02,  3.2032e-02,  3.5414e-02,  1.0242e-02,  2.1762e-02,\n         1.8587e-02,  8.6985e-03,  1.4254e-02, -2.6762e-03, -3.1185e-02,\n         1.4550e-02,  9.3576e-03,  1.0419e-03,  1.7439e-02,  1.8830e-02,\n         2.7373e-03, -2.3792e-02, -3.4141e-02,  5.5474e-03,  3.2727e-02,\n        -1.9861e-02,  2.5969e-02,  1.1657e-03,  3.2977e-02, -2.7516e-02,\n         8.2400e-03, -4.2481e-03,  3.3916e-02,  2.8441e-02, -3.2412e-03,\n        -3.5887e-02, -2.7592e-02,  9.7263e-03, -9.7326e-03, -2.5560e-02,\n        -2.4574e-02, -1.1107e-02, -3.1793e-02, -6.5675e-03, -8.1956e-03,\n         2.7236e-02,  1.8533e-02,  8.4863e-03, -1.0699e-02, -1.8122e-02,\n        -2.8946e-03, -7.0648e-03], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0670,  0.0180,  0.0569,  ...,  0.0015, -0.0108,  0.0057],\n        [-0.0269,  0.0706,  0.0502,  ...,  0.0179,  0.0127,  0.0081],\n        [-0.0149, -0.0485,  0.0541,  ..., -0.0648, -0.0026,  0.0352],\n        ...,\n        [ 0.0290, -0.0141, -0.0033,  ..., -0.0550, -0.0263,  0.0188],\n        [ 0.0646, -0.0115, -0.0645,  ..., -0.0333,  0.0528, -0.0324],\n        [ 0.0387,  0.0652,  0.0401,  ...,  0.0050, -0.0260,  0.0223]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0545, -0.0569,  0.0002,  ...,  0.0063,  0.0303, -0.0099],\n        [ 0.0097, -0.0226,  0.0397,  ...,  0.0032,  0.0094,  0.0497],\n        [-0.0431, -0.0055,  0.0590,  ...,  0.0012,  0.0159,  0.0009],\n        ...,\n        [ 0.0594, -0.0188, -0.0241,  ...,  0.0532, -0.0720,  0.0115],\n        [-0.0660, -0.0373,  0.0134,  ...,  0.0150,  0.0328, -0.0710],\n        [-0.0030,  0.0291, -0.0652,  ..., -0.0383, -0.0544,  0.0526]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-6.2967e-02, -1.9930e-02,  2.0887e-02,  ...,  8.4476e-03,\n          5.7377e-02, -5.4412e-02],\n        [ 6.8851e-02, -6.0081e-02, -2.3523e-02,  ...,  5.7571e-02,\n         -6.8266e-02, -3.0821e-02],\n        [ 5.4455e-02, -1.0985e-02,  1.7396e-02,  ..., -6.9002e-02,\n         -4.4258e-02, -2.2156e-02],\n        ...,\n        [ 8.0296e-03, -3.5389e-02, -4.4332e-05,  ...,  2.0157e-02,\n         -1.2001e-02,  1.6454e-02],\n        [ 5.5773e-02, -7.0973e-02,  2.6329e-03,  ...,  1.7102e-02,\n          4.4380e-02, -7.8892e-03],\n        [-7.1419e-02, -5.4343e-02,  3.5826e-02,  ..., -4.3098e-03,\n          4.6167e-02, -1.4081e-02]], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0406, -0.0098,  0.0554,  ...,  0.0225,  0.0002,  0.0386],\n        [ 0.0365, -0.0006,  0.0471,  ..., -0.0532, -0.0698,  0.0067],\n        [-0.0142,  0.0414, -0.0218,  ...,  0.0168,  0.0239, -0.0447],\n        ...,\n        [-0.0529,  0.0066,  0.0005,  ..., -0.0243,  0.0277,  0.0492],\n        [-0.0028,  0.0683, -0.0188,  ..., -0.0678,  0.0127, -0.0468],\n        [-0.0590, -0.0555,  0.0666,  ...,  0.0432, -0.0371, -0.0230]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 0.0220,  0.0683,  0.0273, -0.0425, -0.0174,  0.0715,  0.0349,  0.0534,\n        -0.0162, -0.0328,  0.0607, -0.0347,  0.0141,  0.0562,  0.0328, -0.0690,\n         0.0046,  0.0655, -0.0403,  0.0586, -0.0374, -0.0389, -0.0573, -0.0442,\n         0.0024,  0.0455, -0.0493,  0.0706,  0.0118,  0.0477,  0.0541, -0.0138,\n         0.0665,  0.0128,  0.0139, -0.0412, -0.0195,  0.0155, -0.0006, -0.0140,\n        -0.0066, -0.0102, -0.0441, -0.0708, -0.0346,  0.0498,  0.0672, -0.0188,\n        -0.0433, -0.0388,  0.0626, -0.0044,  0.0480, -0.0473, -0.0213, -0.0441,\n        -0.0556, -0.0462, -0.0136,  0.0484,  0.0361,  0.0510,  0.0440,  0.0422,\n        -0.0191,  0.0567,  0.0227,  0.0254,  0.0600,  0.0152, -0.0108,  0.0511,\n        -0.0382,  0.0255, -0.0178,  0.0048,  0.0194,  0.0080, -0.0450,  0.0079,\n         0.0389,  0.0363,  0.0188,  0.0658,  0.0549, -0.0063, -0.0302, -0.0626,\n         0.0597,  0.0150, -0.0288, -0.0330,  0.0272,  0.0489,  0.0445, -0.0479,\n         0.0308, -0.0714,  0.0384,  0.0153, -0.0065, -0.0298,  0.0226,  0.0365,\n         0.0310, -0.0490, -0.0245,  0.0568, -0.0148,  0.0129,  0.0675,  0.0328,\n        -0.0205,  0.0375,  0.0359,  0.0113, -0.0169, -0.0425, -0.0472,  0.0200,\n        -0.0357, -0.0507,  0.0594, -0.0197, -0.0282, -0.0506,  0.0667,  0.0593,\n        -0.0329, -0.0283,  0.0080, -0.0710,  0.0664,  0.0045,  0.0343, -0.0369,\n         0.0137,  0.0128, -0.0449,  0.0667, -0.0203,  0.0140, -0.0690,  0.0079,\n        -0.0057, -0.0147,  0.0119,  0.0508,  0.0284,  0.0243, -0.0307,  0.0351,\n        -0.0177,  0.0198, -0.0465,  0.0640, -0.0259, -0.0283,  0.0264, -0.0367,\n        -0.0639, -0.0324, -0.0060,  0.0153, -0.0534,  0.0533,  0.0604,  0.0305,\n         0.0594, -0.0101, -0.0306, -0.0325, -0.0209, -0.0030, -0.0085,  0.0380,\n        -0.0483, -0.0040,  0.0581, -0.0668,  0.0664,  0.0562,  0.0426,  0.0597,\n        -0.0365, -0.0126, -0.0424,  0.0577,  0.0521, -0.0031,  0.0674,  0.0423],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0065,  0.0539, -0.0266,  ...,  0.0636, -0.0592, -0.0432],\n        [ 0.0152, -0.0359, -0.0302,  ..., -0.0433,  0.0573,  0.0174],\n        [-0.0124,  0.0283, -0.0118,  ...,  0.0325,  0.0091,  0.0315],\n        ...,\n        [ 0.0538, -0.0195, -0.0155,  ...,  0.0540,  0.0695, -0.0617],\n        [-0.0286,  0.0122, -0.0220,  ..., -0.0461, -0.0619, -0.0649],\n        [-0.0329,  0.0326, -0.0040,  ...,  0.0517, -0.0083,  0.0063]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 1.8860e-02, -5.5513e-02, -2.4903e-02, -2.1314e-02, -4.7758e-02,\n        -4.3991e-02, -5.6773e-02, -1.5389e-02,  6.7308e-02, -3.0367e-02,\n        -3.2748e-02,  1.2592e-03, -4.2838e-02, -7.1084e-02,  3.8468e-02,\n         3.6530e-02, -1.1165e-02,  3.4944e-02,  5.9974e-02, -1.0133e-02,\n         4.0365e-02,  2.9802e-02,  4.2186e-02,  4.8890e-02, -7.2603e-03,\n        -4.1736e-02,  4.3566e-02,  4.1047e-02,  6.6669e-02, -5.4470e-02,\n        -4.5942e-02, -5.2075e-03,  5.0761e-02,  2.5186e-02, -7.2318e-03,\n         1.8052e-02, -2.1818e-02,  2.6267e-02, -2.0497e-02,  1.0106e-02,\n         7.0605e-02,  3.2298e-03,  4.6605e-02, -6.3375e-02, -3.4863e-02,\n        -3.8618e-02,  8.1478e-03, -4.4871e-02,  4.2160e-02,  3.0416e-02,\n        -1.3576e-02, -8.2827e-03, -4.9064e-02,  7.6152e-03,  1.8478e-02,\n        -4.3515e-02,  3.0101e-02,  6.5553e-02, -3.5160e-02, -6.1438e-03,\n        -1.0629e-02, -4.6351e-02,  5.2637e-03,  8.5142e-03,  3.5152e-02,\n        -5.0687e-03, -6.2639e-02, -1.2830e-02, -5.9046e-02,  1.5936e-02,\n         5.4724e-02,  5.1635e-02,  5.2657e-02,  4.1894e-02,  6.8747e-02,\n         5.8953e-02,  6.4461e-02,  3.2389e-02,  2.3062e-02, -3.2647e-02,\n        -6.8874e-04, -9.2444e-03,  2.3872e-02, -2.8220e-02, -8.0312e-03,\n         1.0407e-02,  3.3507e-02,  1.7175e-02, -5.7804e-03,  4.5830e-02,\n         4.4524e-02,  3.9679e-02, -3.4668e-02, -3.7069e-02,  4.4034e-02,\n        -3.0226e-02,  5.8657e-03, -5.7300e-02, -4.8947e-02,  1.8487e-02,\n         3.7344e-02,  6.2400e-02,  1.0021e-02, -6.4247e-02,  2.5745e-02,\n         1.7809e-03,  4.1600e-02,  6.7755e-02, -5.7872e-02, -4.6204e-02,\n         5.4018e-03,  2.5289e-02, -3.3089e-02, -5.4233e-02,  7.2113e-02,\n         5.2946e-03, -5.3576e-02,  3.1422e-02,  2.1901e-02, -5.9332e-02,\n         6.1570e-02, -8.4465e-03,  4.9741e-02,  1.7396e-02,  1.0375e-02,\n         4.7152e-02,  4.3666e-02,  5.4860e-02, -4.1545e-02,  2.2992e-02,\n        -2.2643e-02,  6.7019e-03,  7.7024e-03, -6.9979e-02, -1.5330e-02,\n         6.7104e-02, -3.8038e-02,  6.2055e-02,  7.1536e-02, -1.9225e-02,\n        -5.9951e-03, -3.0955e-02,  4.5929e-02,  6.6584e-02, -1.9146e-02,\n        -3.7200e-02, -2.0939e-02,  1.8083e-02,  5.0832e-02,  4.9905e-03,\n         1.5261e-02, -1.2884e-03,  3.5101e-02, -6.1938e-02, -3.3095e-02,\n         6.5230e-02, -5.4811e-02,  6.7533e-02,  3.1132e-02,  5.1808e-02,\n         6.2642e-02, -4.1566e-02,  6.6748e-02, -6.6605e-03,  5.2468e-02,\n         9.5652e-03,  3.7183e-02,  5.1722e-02,  2.8006e-04, -4.4985e-02,\n        -2.3722e-02,  5.5746e-02,  3.2392e-02, -2.0221e-03,  6.2601e-03,\n         2.0694e-03,  6.0658e-02,  2.9468e-02,  5.2657e-02,  4.0713e-02,\n         6.4556e-02, -6.9540e-02,  5.7500e-02, -4.6631e-02, -2.9432e-02,\n         1.3173e-02,  2.4460e-02,  3.2202e-02, -2.5264e-02,  5.3218e-02,\n         1.2768e-02,  5.0469e-02, -3.3869e-02, -1.7189e-02,  3.1841e-02,\n        -3.0731e-02, -6.8889e-02,  4.0717e-02, -2.3855e-02, -2.0212e-02,\n         2.2407e-02,  3.6582e-02, -3.3778e-02, -4.9675e-02, -3.3188e-02,\n         7.2083e-02,  5.7265e-03, -4.0321e-02, -9.2359e-03, -7.9234e-03,\n        -2.6828e-02,  5.3731e-02,  5.3443e-02,  8.1773e-03, -2.8482e-02,\n         2.8706e-02, -2.6927e-02, -6.3530e-02, -1.6200e-04,  2.7969e-02,\n        -3.6095e-02, -5.2153e-02,  6.8261e-02, -5.9937e-02, -1.7646e-02,\n         1.7141e-02,  1.7041e-02,  2.7251e-02, -3.1187e-02,  4.6298e-02,\n         5.3903e-03,  6.2989e-02, -1.1059e-02,  5.3962e-02, -4.8847e-02,\n        -4.6124e-02, -5.9289e-02, -4.6990e-02, -5.5621e-02,  4.6577e-02,\n         3.3078e-02,  3.4822e-02, -2.8717e-02, -4.4332e-02, -5.3848e-02,\n        -1.2895e-02, -3.9270e-02,  1.1943e-02,  3.3646e-02, -5.0358e-02,\n        -4.2525e-02,  1.0199e-02,  5.8165e-02,  1.9507e-02,  6.3571e-02,\n         1.5595e-02, -3.2578e-02, -3.5927e-02,  3.8759e-02,  5.5736e-02,\n        -1.1786e-02, -6.6487e-02,  5.2516e-02, -3.4011e-02, -5.7068e-02,\n         9.3064e-03,  4.0429e-03, -6.1077e-02, -3.9256e-02,  3.6645e-02,\n        -5.2131e-02, -3.4111e-02,  4.7741e-02, -1.2561e-02,  3.0895e-02,\n        -4.4850e-02,  3.1867e-02,  5.0432e-02,  7.0802e-02, -6.0275e-02,\n         6.6822e-02, -5.3350e-03, -2.8402e-02, -3.2663e-02,  5.9401e-02,\n        -3.3281e-02,  1.8582e-02,  3.3035e-03,  5.4198e-02,  1.8530e-02,\n        -1.2567e-02,  1.7990e-02,  5.8632e-02, -5.0405e-02,  4.5277e-02,\n         2.7888e-02,  4.6024e-02, -2.7056e-02,  3.6880e-02,  4.6450e-02,\n         6.7651e-02, -6.4716e-02,  1.8866e-02, -4.8613e-02,  5.1673e-02,\n         4.1683e-02,  4.8667e-03,  4.7249e-03, -3.5841e-02,  6.9960e-02,\n         3.3495e-02, -2.1432e-02,  1.8770e-02,  4.7374e-03, -5.7515e-02,\n        -3.9233e-02,  5.9196e-02, -4.9700e-02,  6.0581e-03, -2.0069e-02,\n         1.5962e-02, -1.0108e-02, -5.3395e-02,  4.3646e-03, -3.6686e-02,\n        -5.0447e-02,  4.0781e-02,  2.3079e-02,  5.1231e-02, -2.3153e-02,\n         3.6940e-02, -4.3871e-02,  3.4836e-04, -3.1146e-02,  3.9894e-02,\n        -6.0794e-02,  6.5842e-02,  9.6441e-03,  9.5392e-03, -3.2559e-02,\n        -3.4276e-02,  6.3467e-02, -1.3912e-02,  1.5077e-02,  9.5453e-03,\n         4.9001e-02, -5.2169e-02,  4.3935e-02, -2.6877e-02, -3.8433e-02,\n        -5.7885e-02, -2.4416e-02, -1.9286e-02,  3.1759e-02, -1.8909e-02,\n         3.9569e-02, -4.4347e-02, -4.2769e-02, -2.2459e-02, -3.4221e-02,\n         5.0966e-02, -1.6382e-02, -2.1303e-02,  3.3156e-02, -6.9193e-02,\n         5.9611e-02, -3.5868e-02,  1.6534e-02, -4.1688e-02,  7.0274e-02,\n        -6.5030e-03,  5.1221e-02, -5.5259e-04, -3.7974e-02, -3.8897e-02,\n        -1.2797e-02,  5.0016e-02,  3.4664e-02, -9.0330e-03, -3.0346e-02,\n        -4.4307e-02, -5.6061e-02, -4.1590e-02,  9.0684e-03,  3.5369e-02,\n        -7.6586e-03,  3.1142e-03, -2.2407e-02, -6.5624e-02, -6.7994e-03,\n         6.9384e-02, -8.0191e-03,  1.3827e-02, -7.0411e-02, -4.1169e-02,\n        -4.4853e-02, -3.8045e-02, -2.0133e-02, -2.0165e-02, -7.6730e-03,\n        -2.2316e-02, -4.5034e-02, -4.8310e-02, -3.6197e-02,  5.9285e-04,\n        -3.9150e-02,  3.0624e-02, -4.7958e-03,  3.0668e-02,  3.1460e-02,\n         1.7470e-02, -2.1955e-02, -1.6472e-02,  9.4518e-03,  1.7114e-02,\n        -5.8454e-02, -5.7998e-02, -1.8406e-02, -4.3245e-03,  3.5295e-02,\n         1.1013e-02, -6.2570e-02,  5.3120e-03, -4.7702e-02,  2.7746e-02,\n        -6.0626e-02, -2.7995e-02, -4.1634e-02, -3.1813e-02,  4.4357e-02,\n        -6.7672e-02, -2.8817e-03, -2.5054e-02, -3.1745e-02, -1.6632e-02,\n         6.4104e-02,  6.8649e-02,  1.9082e-02, -5.5977e-02,  6.6083e-02,\n         8.4278e-04,  4.2924e-02,  2.0917e-02,  6.2679e-04,  1.1236e-02,\n         3.4434e-02,  6.7209e-02, -3.0835e-02, -6.0124e-02, -7.6665e-03,\n        -5.5812e-03,  1.7550e-02,  4.2445e-02, -1.9397e-02,  4.3700e-02,\n        -9.2765e-03, -5.5290e-02,  3.7833e-02,  3.3099e-02, -6.8781e-02,\n        -2.7964e-02,  4.5431e-02,  2.6781e-02,  5.6712e-02,  1.1849e-02,\n        -5.9802e-02,  1.5371e-03, -1.0052e-02, -3.0088e-02,  5.0288e-02,\n         6.1639e-02, -4.4747e-02, -5.7687e-02,  4.9515e-02,  3.1314e-02,\n        -8.1493e-03, -1.4071e-02,  4.6104e-02, -1.0685e-02,  6.2103e-02,\n         5.9528e-03,  3.7041e-02, -3.1064e-02, -3.2119e-02,  4.1323e-02,\n        -1.8513e-02,  6.8997e-03, -7.2992e-03,  5.7227e-02,  5.6806e-02,\n        -1.3135e-02, -4.1439e-02,  3.1819e-02,  4.6517e-02,  6.8854e-02,\n        -3.0109e-02,  6.6405e-02, -4.1929e-03, -7.0156e-02,  2.2984e-02,\n         6.8492e-02, -6.7130e-02,  6.3534e-02, -6.4957e-02, -6.6704e-02,\n         2.3166e-02, -1.5459e-02, -6.0216e-02,  1.3042e-02,  5.4619e-02,\n        -5.0318e-04,  7.1990e-03,  1.8381e-02, -1.1242e-02, -7.0387e-02,\n         1.2828e-02,  2.2156e-02, -1.4812e-02, -6.2416e-02, -3.8213e-02,\n        -5.7172e-02,  3.3813e-03, -7.0314e-02, -2.2733e-02, -1.6454e-02,\n        -3.5759e-02,  6.4736e-02, -5.5196e-02, -6.8745e-02, -4.8894e-02,\n         2.1484e-02, -9.6141e-03,  6.7925e-03, -6.9273e-02,  1.6247e-02,\n         7.5670e-03,  4.1805e-04,  1.2872e-02,  6.7713e-04,  2.4779e-02,\n         4.9731e-02, -5.1497e-02, -2.3258e-02, -1.6791e-02, -2.0102e-02,\n         4.7997e-02,  2.2278e-02,  2.0101e-02, -6.4563e-02,  7.5499e-03,\n         1.9579e-02, -1.1614e-02, -5.9233e-02,  2.4109e-02, -3.8049e-02,\n         6.6389e-02, -4.2713e-02,  6.6952e-02, -1.0947e-02,  2.3534e-02,\n         4.7811e-02, -6.5979e-02,  6.0059e-02, -3.9021e-02,  6.8197e-03,\n         2.6138e-02, -4.9132e-02,  5.6414e-03,  4.1198e-02, -3.7530e-02,\n        -2.4847e-02, -5.9932e-02,  1.3025e-02, -2.8522e-02, -3.2384e-02,\n        -4.8542e-02, -6.7321e-02,  4.3654e-02,  4.7361e-02,  3.4357e-02,\n         2.1890e-02,  5.1937e-03,  2.1205e-02, -2.3020e-02, -2.3379e-02,\n        -3.7535e-02, -4.7017e-02,  6.7093e-02, -4.2203e-02,  3.8395e-02,\n         4.4409e-02, -1.2114e-02, -3.1420e-02, -2.6704e-02, -5.8799e-02,\n         5.2020e-03, -6.3143e-02,  4.3673e-02, -5.9250e-03,  3.3110e-02,\n         3.0033e-02,  4.2384e-02, -4.0143e-02,  2.6257e-05, -6.6760e-02,\n        -4.6757e-02,  6.5523e-02, -4.5789e-02,  7.0436e-02, -2.7569e-02,\n         5.4970e-02, -1.5514e-03,  3.7886e-02,  7.1956e-02, -2.7273e-02,\n         7.1975e-02,  1.2421e-02, -5.0120e-02,  4.4084e-02, -7.0178e-02,\n        -6.4278e-02, -3.4865e-02, -3.6584e-02, -1.4588e-02, -3.1889e-02,\n        -3.0093e-02, -4.4982e-02,  4.2829e-02,  6.9005e-02, -5.8871e-02,\n        -6.7045e-02, -6.5110e-02,  3.6267e-02, -8.0469e-03,  5.0924e-02,\n         5.1025e-02, -3.0558e-02, -5.0922e-02, -9.0082e-03, -3.0565e-02,\n         5.7822e-02, -5.3577e-02, -3.8015e-02, -1.1043e-02,  6.7844e-02,\n         5.1591e-02, -4.2581e-02, -2.3931e-02,  1.2305e-03, -3.9701e-02,\n         1.3409e-02,  5.6433e-02,  6.9483e-02, -9.9268e-03,  5.9286e-02,\n        -3.9926e-02,  7.0877e-02,  3.3113e-02, -2.2239e-02, -3.7421e-02,\n        -1.3431e-02, -5.5350e-02, -1.9240e-02,  1.6138e-02, -5.2639e-03,\n        -3.5108e-02,  2.5878e-03, -6.6297e-02,  3.1003e-03,  4.3432e-02,\n        -6.5367e-02, -6.9441e-02,  5.3332e-02,  2.6108e-02, -4.3772e-02,\n         6.4716e-02, -1.2894e-02,  6.5115e-03,  2.2597e-03,  6.5548e-02,\n         1.0796e-02, -5.6295e-02, -2.5313e-02, -5.8843e-03,  3.2519e-03,\n        -6.2562e-02,  6.8430e-02,  5.0638e-02, -4.6633e-02, -6.2776e-02,\n         9.5342e-03, -3.7331e-02, -4.7084e-02, -9.3253e-03, -1.4705e-02,\n        -3.6084e-02, -2.1886e-02,  1.7039e-02,  4.1278e-02, -2.6782e-02,\n         2.4201e-02,  5.2656e-02,  5.6306e-02, -5.3677e-02,  2.8306e-02,\n         7.0372e-02, -3.8362e-02,  1.0845e-02,  6.6420e-02,  2.4137e-03,\n         2.5095e-02,  4.9919e-02, -1.8129e-02,  4.1519e-02,  1.9344e-02,\n         1.8344e-02,  5.7869e-02, -6.5685e-02, -5.1512e-02, -1.1902e-02,\n        -9.2870e-03, -4.1881e-02, -3.5959e-02, -4.8690e-02, -3.7688e-02,\n         4.4314e-02, -1.1850e-02, -1.4271e-02, -2.9497e-02, -3.6863e-02,\n         4.1035e-02,  5.5925e-02, -5.5307e-02, -4.3407e-02,  1.4758e-02,\n         2.1871e-02,  5.5679e-02,  5.6891e-02,  3.7251e-02,  5.9928e-02,\n        -7.0987e-02,  4.5988e-02,  6.0551e-02, -4.4079e-03, -6.0413e-02,\n        -4.2402e-02, -2.8344e-02, -5.4575e-03, -5.4888e-02,  6.3206e-03,\n         5.8179e-02, -1.2465e-02, -6.6313e-02, -5.2427e-02,  7.0956e-02,\n        -4.2928e-02, -1.8020e-02, -6.0991e-02, -5.9678e-02, -2.3609e-02,\n        -4.2062e-02,  3.9796e-02, -6.9947e-02, -5.5313e-02,  3.3027e-02,\n         4.0132e-02,  4.6308e-04, -3.7190e-03], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([[-0.0278, -0.0112, -0.0183,  ...,  0.0256,  0.0172,  0.0358],\n        [ 0.0249,  0.0304, -0.0137,  ...,  0.0067,  0.0270,  0.0195],\n        [ 0.0235, -0.0324,  0.0087,  ..., -0.0261,  0.0274,  0.0110],\n        ...,\n        [-0.0027,  0.0176, -0.0306,  ...,  0.0137,  0.0222,  0.0135],\n        [ 0.0293, -0.0282, -0.0264,  ..., -0.0060, -0.0006,  0.0290],\n        [ 0.0354, -0.0024,  0.0165,  ...,  0.0328, -0.0267,  0.0097]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.0338, -0.0195, -0.0217, -0.0186,  0.0053, -0.0195, -0.0297, -0.0269,\n        -0.0292,  0.0328, -0.0012, -0.0216, -0.0115,  0.0005,  0.0233,  0.0108,\n         0.0083, -0.0262,  0.0294,  0.0169,  0.0086,  0.0041, -0.0061,  0.0147,\n         0.0011,  0.0150, -0.0191,  0.0355, -0.0329,  0.0058,  0.0089, -0.0128,\n         0.0217,  0.0035,  0.0276,  0.0349, -0.0245,  0.0217,  0.0306,  0.0159,\n         0.0280, -0.0246, -0.0285,  0.0022, -0.0042, -0.0146, -0.0053, -0.0017,\n        -0.0097, -0.0016, -0.0113, -0.0358,  0.0026,  0.0236, -0.0251,  0.0218,\n         0.0293,  0.0125, -0.0199,  0.0208,  0.0033, -0.0308,  0.0274, -0.0253,\n        -0.0043, -0.0187,  0.0086,  0.0262, -0.0143,  0.0012,  0.0136,  0.0134,\n        -0.0317, -0.0268, -0.0258,  0.0290, -0.0260,  0.0042, -0.0191, -0.0338,\n        -0.0324, -0.0147,  0.0304, -0.0039,  0.0309,  0.0311, -0.0355, -0.0251,\n         0.0181,  0.0055, -0.0169, -0.0137,  0.0051,  0.0355, -0.0307, -0.0333,\n         0.0301, -0.0125, -0.0014,  0.0226, -0.0341,  0.0315, -0.0046,  0.0245,\n         0.0009, -0.0037, -0.0002, -0.0021,  0.0255,  0.0150, -0.0321, -0.0243,\n        -0.0004, -0.0133, -0.0343, -0.0060, -0.0148, -0.0259, -0.0133,  0.0257,\n        -0.0241,  0.0082,  0.0200,  0.0065,  0.0099, -0.0162, -0.0324,  0.0327,\n         0.0282,  0.0004, -0.0247, -0.0280,  0.0059,  0.0093,  0.0286, -0.0185,\n        -0.0034, -0.0183,  0.0100, -0.0042, -0.0136,  0.0007, -0.0199, -0.0123,\n         0.0345, -0.0249,  0.0094,  0.0130, -0.0158,  0.0038,  0.0355, -0.0275,\n         0.0225, -0.0262, -0.0194,  0.0210,  0.0161, -0.0022,  0.0235,  0.0107,\n         0.0266,  0.0149, -0.0325,  0.0145,  0.0179,  0.0019, -0.0255,  0.0217,\n        -0.0092,  0.0156,  0.0252, -0.0284, -0.0276, -0.0008, -0.0039,  0.0267,\n        -0.0234,  0.0135,  0.0310, -0.0300, -0.0233, -0.0046,  0.0262,  0.0025,\n         0.0090, -0.0114,  0.0242,  0.0113, -0.0172,  0.0047,  0.0263, -0.0300],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0095,  0.0568, -0.0201,  ...,  0.0388,  0.0175, -0.0689],\n        [-0.0255, -0.0149, -0.0602,  ...,  0.0434,  0.0026,  0.0576],\n        [-0.0015, -0.0226, -0.0291,  ..., -0.0537,  0.0442,  0.0467],\n        ...,\n        [-0.0312,  0.0549,  0.0515,  ...,  0.0703, -0.0612, -0.0721],\n        [-0.0168,  0.0607, -0.0541,  ..., -0.0291, -0.0633, -0.0090],\n        [-0.0125, -0.0431,  0.0082,  ..., -0.0264, -0.0711,  0.0456]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0120, -0.0311, -0.0149,  ..., -0.0510, -0.0543,  0.0600],\n        [ 0.0614, -0.0187,  0.0345,  ...,  0.0193,  0.0016,  0.0439],\n        [ 0.0538,  0.0261, -0.0279,  ...,  0.0683,  0.0517,  0.0340],\n        ...,\n        [ 0.0401,  0.0537, -0.0060,  ..., -0.0507, -0.0009,  0.0113],\n        [ 0.0213,  0.0693,  0.0349,  ...,  0.0704, -0.0187, -0.0599],\n        [-0.0106,  0.0683,  0.0610,  ...,  0.0329,  0.0517,  0.0408]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0328, -0.0440, -0.0238,  ...,  0.0429,  0.0528,  0.0079],\n        [-0.0076,  0.0629,  0.0720,  ...,  0.0618, -0.0081, -0.0194],\n        [ 0.0331, -0.0637,  0.0125,  ...,  0.0281, -0.0276,  0.0531],\n        ...,\n        [ 0.0563, -0.0619,  0.0197,  ..., -0.0447, -0.0412, -0.0433],\n        [-0.0261, -0.0103,  0.0083,  ...,  0.0482,  0.0523,  0.0277],\n        [-0.0290,  0.0058,  0.0673,  ..., -0.0281, -0.0517,  0.0554]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0137, -0.0609, -0.0317,  ...,  0.0593,  0.0523,  0.0304],\n        [ 0.0031,  0.0022, -0.0200,  ...,  0.0673,  0.0556, -0.0667],\n        [-0.0433,  0.0458, -0.0417,  ..., -0.0240,  0.0489,  0.0148],\n        ...,\n        [-0.0037,  0.0027, -0.0169,  ...,  0.0275,  0.0327,  0.0235],\n        [-0.0662, -0.0155, -0.0673,  ..., -0.0244, -0.0395,  0.0212],\n        [ 0.0434,  0.0414,  0.0644,  ...,  0.0437,  0.0204,  0.0437]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 0.0027,  0.0163,  0.0372,  0.0006, -0.0208,  0.0224, -0.0721,  0.0013,\n         0.0359,  0.0304,  0.0337,  0.0487,  0.0330,  0.0031, -0.0520,  0.0378,\n        -0.0283,  0.0721,  0.0213,  0.0191, -0.0627,  0.0316,  0.0558,  0.0246,\n        -0.0401,  0.0110, -0.0018, -0.0232, -0.0153, -0.0641,  0.0265,  0.0646,\n        -0.0350, -0.0110,  0.0022,  0.0509,  0.0223,  0.0190,  0.0191,  0.0658,\n        -0.0679,  0.0541, -0.0040,  0.0047,  0.0676, -0.0009, -0.0486,  0.0678,\n        -0.0032, -0.0633, -0.0385, -0.0380,  0.0520, -0.0057,  0.0014, -0.0681,\n         0.0705, -0.0632, -0.0273, -0.0300, -0.0535, -0.0345, -0.0367, -0.0662,\n         0.0136, -0.0486, -0.0012, -0.0670,  0.0499,  0.0700, -0.0045,  0.0298,\n        -0.0598,  0.0500, -0.0249,  0.0396, -0.0087, -0.0371, -0.0637,  0.0688,\n         0.0610, -0.0039, -0.0647,  0.0421, -0.0023, -0.0150,  0.0356, -0.0093,\n        -0.0002, -0.0369, -0.0015, -0.0655,  0.0442, -0.0297, -0.0415, -0.0587,\n        -0.0055, -0.0720, -0.0640,  0.0024, -0.0708, -0.0665,  0.0562, -0.0372,\n        -0.0229,  0.0513,  0.0030, -0.0226,  0.0203,  0.0436, -0.0358, -0.0339,\n        -0.0102,  0.0657, -0.0542, -0.0639,  0.0193,  0.0476,  0.0262, -0.0407,\n        -0.0243,  0.0061, -0.0680, -0.0528,  0.0612,  0.0167,  0.0359, -0.0651,\n        -0.0421, -0.0598, -0.0571,  0.0203, -0.0203,  0.0259, -0.0675,  0.0074,\n        -0.0077,  0.0160, -0.0250,  0.0608,  0.0028,  0.0070, -0.0070, -0.0377,\n         0.0559, -0.0233, -0.0384,  0.0666, -0.0682, -0.0188,  0.0523, -0.0088,\n         0.0371, -0.0288, -0.0226,  0.0295,  0.0060,  0.0499,  0.0013, -0.0373,\n        -0.0135, -0.0509,  0.0044, -0.0048, -0.0123, -0.0509, -0.0166,  0.0115,\n        -0.0048,  0.0458, -0.0189, -0.0658, -0.0006,  0.0676, -0.0290,  0.0398,\n        -0.0703, -0.0431, -0.0594,  0.0612, -0.0212,  0.0495, -0.0671, -0.0187,\n         0.0328, -0.0503, -0.0682,  0.0289,  0.0584,  0.0080,  0.0320, -0.0099],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0132, -0.0282, -0.0446,  ..., -0.0554,  0.0331, -0.0365],\n        [-0.0371, -0.0242,  0.0307,  ..., -0.0710,  0.0544,  0.0649],\n        [ 0.0403, -0.0320, -0.0686,  ...,  0.0475, -0.0285, -0.0533],\n        ...,\n        [-0.0009,  0.0658,  0.0484,  ..., -0.0186,  0.0132, -0.0231],\n        [-0.0639, -0.0667, -0.0131,  ...,  0.0716, -0.0152, -0.0228],\n        [-0.0557, -0.0147, -0.0541,  ...,  0.0596,  0.0619,  0.0485]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.0078,  0.0349, -0.0414,  0.0382, -0.0392, -0.0148, -0.0140,  0.0556,\n         0.0691, -0.0033,  0.0260, -0.0555,  0.0013,  0.0329, -0.0499, -0.0603,\n        -0.0217, -0.0002,  0.0401,  0.0114,  0.0469, -0.0400, -0.0133,  0.0013,\n         0.0165, -0.0395, -0.0614, -0.0247,  0.0472, -0.0467, -0.0334,  0.0497,\n        -0.0083, -0.0678,  0.0498,  0.0224, -0.0408, -0.0062,  0.0257, -0.0305,\n         0.0219, -0.0056, -0.0660,  0.0614,  0.0634,  0.0576, -0.0058, -0.0406,\n         0.0199,  0.0008,  0.0005, -0.0209,  0.0345, -0.0633,  0.0609,  0.0568,\n         0.0236,  0.0358,  0.0232,  0.0065,  0.0380, -0.0582,  0.0347, -0.0288,\n         0.0586,  0.0095,  0.0305, -0.0538,  0.0041,  0.0511,  0.0606, -0.0542,\n        -0.0451,  0.0503, -0.0100, -0.0044, -0.0493, -0.0197, -0.0534, -0.0646,\n        -0.0388, -0.0358,  0.0522, -0.0304, -0.0225, -0.0141,  0.0521,  0.0522,\n         0.0020, -0.0187, -0.0350,  0.0258, -0.0003, -0.0143, -0.0074, -0.0263,\n        -0.0382, -0.0530, -0.0167, -0.0457,  0.0434,  0.0061,  0.0460, -0.0239,\n        -0.0158,  0.0083, -0.0217, -0.0501, -0.0316, -0.0031, -0.0206,  0.0222,\n         0.0639,  0.0300,  0.0542, -0.0644,  0.0406,  0.0477, -0.0141, -0.0109,\n        -0.0103,  0.0416, -0.0045,  0.0321, -0.0465,  0.0291,  0.0031,  0.0019,\n        -0.0143,  0.0168,  0.0710, -0.0102, -0.0101, -0.0392,  0.0411,  0.0604,\n        -0.0256, -0.0598,  0.0349,  0.0335, -0.0326, -0.0470, -0.0051,  0.0575,\n         0.0023,  0.0206,  0.0695, -0.0096, -0.0172,  0.0070, -0.0658,  0.0655,\n        -0.0019, -0.0446, -0.0235, -0.0561, -0.0680, -0.0379,  0.0138, -0.0534,\n        -0.0253,  0.0309, -0.0570, -0.0536, -0.0613, -0.0215, -0.0436,  0.0668,\n        -0.0545,  0.0655, -0.0462,  0.0641, -0.0451, -0.0081, -0.0650, -0.0419,\n         0.0427,  0.0290, -0.0700, -0.0409,  0.0384, -0.0389,  0.0663,  0.0381,\n         0.0550,  0.0151, -0.0123, -0.0429,  0.0538, -0.0538, -0.0404,  0.0556,\n         0.0238, -0.0594,  0.0418, -0.0580, -0.0392, -0.0183, -0.0079, -0.0081,\n         0.0408,  0.0310, -0.0711,  0.0111, -0.0153, -0.0174, -0.0421, -0.0630,\n        -0.0146,  0.0124, -0.0350,  0.0467, -0.0517, -0.0512,  0.0373,  0.0143,\n         0.0461,  0.0167,  0.0060, -0.0595, -0.0596, -0.0229, -0.0477,  0.0350,\n        -0.0653, -0.0222, -0.0229,  0.0691, -0.0672, -0.0118,  0.0221,  0.0366,\n        -0.0304,  0.0372,  0.0318, -0.0427,  0.0309, -0.0578, -0.0146, -0.0369,\n         0.0700,  0.0511,  0.0113,  0.0115,  0.0210,  0.0547, -0.0260, -0.0661,\n        -0.0248,  0.0255, -0.0356, -0.0195,  0.0345,  0.0147,  0.0287, -0.0046,\n        -0.0388, -0.0593, -0.0470,  0.0363,  0.0345,  0.0561, -0.0611, -0.0135,\n        -0.0645, -0.0169,  0.0633,  0.0115,  0.0146,  0.0268, -0.0651, -0.0516,\n        -0.0200, -0.0190, -0.0101,  0.0289,  0.0208, -0.0707,  0.0272,  0.0494,\n        -0.0551,  0.0195,  0.0219, -0.0584,  0.0549,  0.0209, -0.0620,  0.0345,\n         0.0578, -0.0563,  0.0628,  0.0300, -0.0681,  0.0210, -0.0155,  0.0644,\n         0.0358, -0.0686,  0.0324, -0.0579,  0.0635, -0.0640, -0.0400, -0.0071,\n        -0.0257,  0.0393,  0.0360, -0.0269, -0.0190, -0.0039,  0.0218, -0.0636,\n         0.0623,  0.0539,  0.0037, -0.0194, -0.0449,  0.0603,  0.0141, -0.0413,\n         0.0583, -0.0442,  0.0310, -0.0286,  0.0218,  0.0697, -0.0185, -0.0489,\n        -0.0252, -0.0559, -0.0382, -0.0212, -0.0502,  0.0549, -0.0133, -0.0675,\n        -0.0180,  0.0145, -0.0130, -0.0309, -0.0568, -0.0470,  0.0291,  0.0403,\n        -0.0467,  0.0288, -0.0219, -0.0486, -0.0490,  0.0316, -0.0037, -0.0067,\n        -0.0567,  0.0182,  0.0618,  0.0352,  0.0263,  0.0527,  0.0457,  0.0235,\n         0.0658, -0.0379,  0.0517, -0.0705,  0.0354, -0.0518, -0.0028, -0.0236,\n        -0.0323,  0.0204, -0.0396, -0.0196, -0.0704, -0.0070,  0.0336, -0.0061,\n         0.0140,  0.0120, -0.0172, -0.0623, -0.0164,  0.0692, -0.0407, -0.0391,\n        -0.0557, -0.0498, -0.0562, -0.0683,  0.0622,  0.0690,  0.0239, -0.0258,\n        -0.0651, -0.0294,  0.0677, -0.0405,  0.0145,  0.0219,  0.0684,  0.0068,\n        -0.0585,  0.0076, -0.0145,  0.0093,  0.0100,  0.0173,  0.0635,  0.0272,\n         0.0066, -0.0025, -0.0353,  0.0650, -0.0428, -0.0400,  0.0295,  0.0351,\n        -0.0090, -0.0198, -0.0401,  0.0594,  0.0524, -0.0158,  0.0200,  0.0623,\n         0.0637,  0.0056, -0.0444,  0.0397,  0.0582, -0.0282,  0.0148, -0.0378,\n         0.0355, -0.0322, -0.0709, -0.0292, -0.0284, -0.0588,  0.0106,  0.0221,\n        -0.0644, -0.0549, -0.0104,  0.0642, -0.0438,  0.0479, -0.0370, -0.0364,\n         0.0473,  0.0697, -0.0539, -0.0251,  0.0687, -0.0674, -0.0247,  0.0545,\n         0.0612, -0.0417, -0.0407,  0.0171, -0.0286, -0.0145, -0.0037,  0.0396,\n        -0.0479,  0.0171,  0.0146, -0.0405,  0.0712, -0.0384,  0.0303, -0.0096,\n        -0.0213,  0.0086,  0.0084, -0.0280, -0.0523,  0.0429, -0.0105,  0.0205,\n        -0.0124, -0.0442,  0.0372,  0.0166, -0.0705, -0.0258, -0.0678,  0.0255,\n        -0.0399, -0.0211, -0.0253,  0.0178, -0.0137, -0.0178,  0.0096, -0.0025,\n        -0.0692,  0.0144, -0.0290, -0.0305,  0.0721,  0.0168, -0.0100, -0.0160,\n        -0.0709, -0.0283,  0.0706,  0.0684, -0.0112, -0.0667,  0.0026,  0.0653,\n        -0.0104, -0.0470, -0.0576,  0.0675, -0.0047,  0.0126,  0.0122, -0.0245,\n         0.0041, -0.0437, -0.0156, -0.0496,  0.0417,  0.0443, -0.0156,  0.0048,\n        -0.0103, -0.0315,  0.0162, -0.0462, -0.0501,  0.0442,  0.0095, -0.0633,\n         0.0344,  0.0108, -0.0201,  0.0168, -0.0503, -0.0329,  0.0403,  0.0369,\n        -0.0071,  0.0702,  0.0650, -0.0705, -0.0678, -0.0224,  0.0192, -0.0395,\n        -0.0646,  0.0518,  0.0080,  0.0668, -0.0243,  0.0002,  0.0243, -0.0210,\n         0.0437, -0.0131,  0.0294,  0.0010, -0.0242, -0.0120, -0.0401,  0.0538,\n         0.0609,  0.0445,  0.0411, -0.0035,  0.0372, -0.0334, -0.0277,  0.0258,\n         0.0635,  0.0166, -0.0033, -0.0532,  0.0140,  0.0176, -0.0081,  0.0011,\n         0.0176,  0.0136,  0.0279,  0.0334, -0.0341,  0.0465, -0.0485, -0.0177,\n        -0.0642, -0.0185, -0.0224, -0.0221,  0.0271,  0.0127,  0.0596,  0.0617,\n         0.0097, -0.0647, -0.0594,  0.0404, -0.0319, -0.0637,  0.0313, -0.0619,\n         0.0130, -0.0617,  0.0071,  0.0714, -0.0069,  0.0012, -0.0707,  0.0240,\n        -0.0701, -0.0434, -0.0194, -0.0548,  0.0238, -0.0675, -0.0176,  0.0390,\n         0.0461, -0.0658, -0.0455,  0.0641, -0.0342, -0.0536,  0.0570,  0.0436,\n         0.0050, -0.0502,  0.0647, -0.0113,  0.0243,  0.0436, -0.0221, -0.0680,\n         0.0172,  0.0068, -0.0660,  0.0360, -0.0416, -0.0528,  0.0561, -0.0216,\n         0.0651,  0.0529,  0.0422,  0.0236, -0.0222, -0.0207,  0.0153,  0.0101,\n        -0.0592,  0.0195, -0.0277,  0.0327,  0.0169,  0.0709,  0.0388,  0.0479,\n         0.0351,  0.0181, -0.0030, -0.0262,  0.0109, -0.0503,  0.0060, -0.0427,\n        -0.0026,  0.0540,  0.0635,  0.0249, -0.0578, -0.0484,  0.0168,  0.0381,\n         0.0364,  0.0272,  0.0690,  0.0665,  0.0384,  0.0347,  0.0627, -0.0297,\n         0.0123,  0.0408, -0.0481, -0.0192,  0.0719,  0.0677, -0.0019, -0.0578,\n        -0.0657, -0.0285,  0.0516, -0.0078,  0.0125,  0.0613,  0.0121, -0.0385,\n         0.0213,  0.0195, -0.0126, -0.0552,  0.0211, -0.0341,  0.0218, -0.0411,\n        -0.0358,  0.0474, -0.0230,  0.0379, -0.0654,  0.0214, -0.0472,  0.0356,\n         0.0402,  0.0140,  0.0197, -0.0205, -0.0325, -0.0521,  0.0344,  0.0380,\n         0.0385,  0.0198,  0.0535,  0.0106, -0.0435, -0.0633, -0.0431, -0.0128,\n        -0.0441, -0.0125,  0.0430,  0.0047,  0.0675,  0.0155, -0.0200,  0.0436,\n        -0.0659,  0.0142, -0.0341,  0.0325, -0.0635,  0.0391, -0.0613,  0.0203,\n        -0.0303, -0.0311,  0.0418, -0.0300, -0.0332,  0.0310, -0.0235, -0.0027,\n         0.0181,  0.0240,  0.0221,  0.0260, -0.0079,  0.0243, -0.0592, -0.0242],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0120, -0.0354,  0.0356,  ...,  0.0163,  0.0090,  0.0055],\n        [ 0.0111,  0.0314, -0.0136,  ..., -0.0179,  0.0100, -0.0274],\n        [-0.0344,  0.0136,  0.0223,  ...,  0.0294, -0.0224,  0.0066],\n        ...,\n        [-0.0029, -0.0309,  0.0328,  ...,  0.0037, -0.0062,  0.0298],\n        [ 0.0294,  0.0008, -0.0284,  ..., -0.0158, -0.0086,  0.0306],\n        [ 0.0313, -0.0130,  0.0359,  ..., -0.0117,  0.0325,  0.0036]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 9.4702e-03,  2.8545e-02,  3.2946e-02, -1.6272e-02,  2.4362e-02,\n        -3.1955e-02, -1.6599e-02,  4.0612e-03,  2.0183e-03,  6.8679e-03,\n         3.6385e-03,  3.0655e-02, -3.5918e-02, -1.1177e-02,  9.7017e-04,\n         2.4396e-02,  1.2588e-02,  3.1580e-02, -7.8570e-03,  2.8515e-02,\n         2.4111e-02,  2.0706e-02, -2.7928e-02, -3.3760e-02,  1.1648e-02,\n        -2.9363e-02, -1.9265e-02, -5.0845e-03,  1.8009e-02, -2.1730e-02,\n         3.3412e-02, -2.2295e-04,  2.8329e-02,  9.8740e-03,  4.3436e-03,\n         3.3358e-02, -1.0637e-02, -1.3641e-02,  1.0737e-02, -1.8845e-02,\n        -3.3748e-02, -2.6190e-03,  2.5665e-02,  1.7591e-02, -1.1842e-02,\n        -2.2152e-02,  1.5267e-02,  6.4878e-04,  2.8993e-02,  3.5576e-02,\n        -2.3730e-02,  2.2935e-02,  1.3568e-02,  2.8047e-02, -3.2061e-02,\n         2.7749e-02, -2.7298e-02,  2.0021e-02,  2.5849e-03,  2.7812e-02,\n        -1.7273e-02,  4.6560e-03, -2.9584e-02, -3.1300e-02, -5.0932e-03,\n         1.3430e-02, -7.9017e-03, -1.5533e-02,  6.9959e-03,  2.1699e-02,\n        -5.5030e-04,  6.1230e-03,  4.5287e-03, -2.6741e-03,  3.5844e-02,\n         6.0244e-03,  2.5038e-02, -2.6611e-02, -1.7536e-02, -4.1841e-04,\n         1.8498e-02, -3.3099e-02, -2.9917e-02,  1.7304e-04,  2.8675e-02,\n         1.7537e-02,  1.1048e-02,  1.4623e-02, -8.7180e-03,  1.7495e-02,\n         2.9671e-02, -7.0880e-04, -3.4410e-02,  3.5726e-02, -1.5180e-02,\n        -1.7522e-02, -3.2772e-02,  2.4658e-02, -1.5119e-02, -3.6737e-03,\n        -2.7392e-02,  2.4412e-03,  1.7566e-02,  1.0191e-03, -1.8179e-02,\n         2.9544e-02, -7.1097e-03,  2.5694e-02,  3.5961e-02, -1.2813e-02,\n         1.4501e-02, -2.1338e-02,  3.3558e-02,  2.9821e-02,  3.4511e-02,\n         2.9461e-02,  4.0406e-03, -4.0383e-05, -1.7531e-02,  3.5301e-02,\n         1.1642e-02, -2.2975e-02,  1.4845e-02,  2.2391e-02, -2.4316e-03,\n         3.5406e-02, -1.3705e-03, -2.2266e-02,  5.2025e-03,  1.1067e-02,\n         3.3216e-02, -2.6836e-02, -3.5116e-02,  3.2046e-02, -1.0296e-02,\n         2.7273e-02, -3.2202e-02,  1.0931e-02,  2.1954e-02, -1.5362e-02,\n        -7.8333e-03, -1.5160e-02,  1.2871e-03, -1.6698e-02, -1.7739e-02,\n        -1.4497e-02, -2.3485e-02,  2.2757e-02, -3.5447e-02, -3.2099e-02,\n         2.8443e-03,  9.7270e-03, -9.3678e-04,  2.8101e-03,  2.7311e-02,\n         2.3644e-02,  2.9957e-02, -7.8163e-03, -6.9617e-03, -1.6432e-02,\n         2.7869e-02, -5.4568e-03, -1.5995e-02, -1.3461e-02, -2.5989e-02,\n        -2.2903e-02, -9.6847e-03, -1.7007e-02, -7.1804e-03, -1.7082e-05,\n         4.0894e-03, -1.7446e-02,  2.7527e-02, -3.9477e-03,  3.5859e-02,\n        -1.3231e-02, -3.5601e-03, -4.9215e-03, -3.3687e-02,  1.9992e-02,\n         1.3203e-02,  2.4962e-02,  3.5051e-02,  3.0054e-02,  1.1049e-02,\n        -2.5165e-02, -1.6603e-02, -6.9634e-05, -1.3713e-02, -3.3811e-02,\n         1.9107e-02,  1.3676e-03], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0060,  0.0589, -0.0517,  ..., -0.0201, -0.0156,  0.0215],\n        [ 0.0268, -0.0487, -0.0022,  ...,  0.0296, -0.0657, -0.0277],\n        [-0.0200, -0.0422, -0.0624,  ..., -0.0410,  0.0398, -0.0721],\n        ...,\n        [ 0.0026,  0.0115,  0.0025,  ..., -0.0635, -0.0627, -0.0140],\n        [ 0.0494,  0.0387, -0.0542,  ...,  0.0412,  0.0104,  0.0014],\n        [ 0.0504,  0.0676,  0.0010,  ..., -0.0319, -0.0389, -0.0411]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0242,  0.0076, -0.0702,  ..., -0.0635,  0.0381,  0.0261],\n        [-0.0206, -0.0092,  0.0526,  ..., -0.0453, -0.0703, -0.0069],\n        [ 0.0146, -0.0654,  0.0262,  ...,  0.0549, -0.0563, -0.0361],\n        ...,\n        [ 0.0469,  0.0436, -0.0298,  ..., -0.0135, -0.0321, -0.0184],\n        [-0.0328,  0.0278,  0.0473,  ..., -0.0091, -0.0270,  0.0511],\n        [-0.0715, -0.0671, -0.0718,  ...,  0.0237,  0.0167,  0.0713]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0135, -0.0402, -0.0466,  ...,  0.0506,  0.0028,  0.0531],\n        [-0.0508, -0.0309,  0.0448,  ..., -0.0447, -0.0142, -0.0556],\n        [ 0.0157,  0.0563, -0.0036,  ...,  0.0404,  0.0567,  0.0331],\n        ...,\n        [ 0.0267, -0.0256,  0.0209,  ..., -0.0496,  0.0701, -0.0709],\n        [ 0.0339,  0.0614, -0.0697,  ...,  0.0303,  0.0271, -0.0367],\n        [ 0.0605, -0.0393, -0.0094,  ...,  0.0028,  0.0539,  0.0309]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 1.2907e-02, -3.0152e-02,  1.0635e-02,  ..., -6.1340e-03,\n         -4.3723e-02, -1.5561e-02],\n        [ 2.1218e-02,  4.4584e-02, -6.3851e-02,  ...,  4.4394e-02,\n          4.0311e-02,  4.4379e-02],\n        [-3.9285e-02,  6.8653e-02,  2.4750e-02,  ..., -4.4923e-02,\n         -3.6985e-02, -4.3227e-02],\n        ...,\n        [ 4.2834e-02, -6.1361e-02, -5.6695e-02,  ...,  2.4958e-02,\n         -4.8099e-02, -7.0996e-02],\n        [-7.8908e-03,  8.2160e-06,  6.5735e-02,  ..., -5.8294e-02,\n          4.3361e-03, -5.8730e-02],\n        [ 2.3704e-02,  7.0960e-02, -1.7645e-02,  ..., -4.2392e-02,\n         -1.5419e-02,  3.2040e-02]], device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 0.0529,  0.0213, -0.0574, -0.0227,  0.0500, -0.0422, -0.0217,  0.0069,\n         0.0640, -0.0201, -0.0198, -0.0277, -0.0253, -0.0565, -0.0174, -0.0521,\n         0.0018,  0.0356, -0.0523, -0.0415,  0.0347,  0.0543, -0.0668,  0.0620,\n         0.0058, -0.0628, -0.0110, -0.0563, -0.0222, -0.0118, -0.0678, -0.0272,\n        -0.0152, -0.0363, -0.0552, -0.0604, -0.0456, -0.0258, -0.0430, -0.0417,\n         0.0476, -0.0690,  0.0551,  0.0078, -0.0618, -0.0355, -0.0392, -0.0657,\n         0.0001,  0.0545, -0.0684,  0.0366, -0.0624, -0.0704, -0.0425,  0.0484,\n         0.0365,  0.0590, -0.0008,  0.0488, -0.0507,  0.0166,  0.0675,  0.0011,\n         0.0711, -0.0631, -0.0038, -0.0030,  0.0538,  0.0505, -0.0369, -0.0687,\n        -0.0181,  0.0252, -0.0454,  0.0181, -0.0329, -0.0284, -0.0554,  0.0014,\n         0.0038, -0.0043,  0.0701,  0.0607, -0.0601,  0.0131, -0.0684,  0.0410,\n         0.0112,  0.0069, -0.0706, -0.0462,  0.0061,  0.0329, -0.0112, -0.0113,\n        -0.0569, -0.0439, -0.0292, -0.0356, -0.0297,  0.0093, -0.0353, -0.0539,\n         0.0716, -0.0107, -0.0183,  0.0714,  0.0307,  0.0446,  0.0141,  0.0528,\n        -0.0072,  0.0062,  0.0648, -0.0140, -0.0184,  0.0473,  0.0664,  0.0252,\n        -0.0687, -0.0624,  0.0440, -0.0550,  0.0332, -0.0137,  0.0353, -0.0488,\n         0.0276,  0.0444, -0.0036,  0.0082, -0.0440, -0.0303,  0.0036,  0.0413,\n         0.0427, -0.0349, -0.0417, -0.0070, -0.0574,  0.0619, -0.0107,  0.0698,\n        -0.0059,  0.0296,  0.0514,  0.0018,  0.0558, -0.0306, -0.0633,  0.0718,\n        -0.0347,  0.0031, -0.0213,  0.0281, -0.0679, -0.0135,  0.0045, -0.0047,\n         0.0184, -0.0171,  0.0593,  0.0594,  0.0494, -0.0290,  0.0019, -0.0497,\n        -0.0280, -0.0657,  0.0033,  0.0252,  0.0702, -0.0074,  0.0018, -0.0609,\n        -0.0170, -0.0074,  0.0198,  0.0122,  0.0311, -0.0476,  0.0087, -0.0550,\n        -0.0041,  0.0577, -0.0103, -0.0592, -0.0596, -0.0690, -0.0190,  0.0337],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0416, -0.0409,  0.0708,  ..., -0.0043,  0.0064, -0.0616],\n        [ 0.0191, -0.0248,  0.0137,  ...,  0.0198,  0.0233, -0.0233],\n        [-0.0390, -0.0631, -0.0500,  ...,  0.0574, -0.0083, -0.0582],\n        ...,\n        [ 0.0253, -0.0011, -0.0410,  ..., -0.0025,  0.0525,  0.0532],\n        [ 0.0689,  0.0190,  0.0710,  ...,  0.0196,  0.0402, -0.0558],\n        [ 0.0587,  0.0177,  0.0543,  ..., -0.0561, -0.0140,  0.0566]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-3.7134e-02, -2.3873e-03, -3.4862e-02,  6.0466e-02,  2.4781e-02,\n         2.7891e-02,  3.6245e-02, -4.3633e-02,  3.2964e-02, -3.2122e-03,\n        -1.1941e-02, -4.5388e-02, -2.8939e-02,  2.7845e-03, -6.7199e-02,\n         7.2168e-02, -6.7622e-02, -1.1607e-02, -2.1805e-03,  6.4918e-02,\n         4.0931e-02, -4.2471e-02,  3.3614e-02,  7.6671e-03,  3.4125e-02,\n         1.4024e-02,  5.7207e-02, -6.3361e-02, -2.2223e-04, -6.3417e-02,\n         5.3044e-02, -4.7608e-02,  6.5460e-02,  1.4089e-02,  2.0374e-03,\n        -3.7520e-02, -4.0610e-03, -5.3281e-02, -4.7033e-02,  2.1499e-02,\n        -2.4327e-02, -1.4078e-02, -3.9269e-02, -8.2893e-04, -5.8075e-02,\n         5.7187e-03,  1.0489e-02, -2.1171e-02,  2.4592e-03,  9.2979e-03,\n         1.8871e-02,  3.0576e-02,  1.4250e-02, -4.6587e-02, -4.9265e-02,\n        -4.9557e-02, -1.4507e-02, -2.8218e-02, -3.5807e-02,  6.0842e-02,\n        -2.9112e-02,  6.0826e-02, -5.1499e-02, -4.0956e-02, -6.0405e-02,\n        -4.7846e-02, -6.4587e-02,  5.7783e-02,  5.8526e-02, -5.2063e-02,\n        -1.6756e-02,  3.1387e-02,  3.4190e-02, -7.0662e-02,  1.2296e-02,\n         4.2599e-02,  5.0132e-02,  4.2546e-02, -6.7532e-02, -4.9459e-02,\n        -1.8398e-02,  6.0610e-02, -6.0643e-02,  4.4108e-02, -4.1821e-02,\n         2.2493e-02, -2.6113e-02,  5.6377e-02, -4.7807e-03,  5.6783e-02,\n        -3.0193e-03,  4.1799e-02, -5.3316e-02,  1.5256e-02, -5.7629e-02,\n        -1.2735e-02, -3.5562e-02, -1.7105e-02, -1.1528e-02, -2.5047e-02,\n         1.6261e-02,  2.7242e-02, -2.1500e-02,  3.8972e-02, -4.3566e-02,\n        -4.4349e-02,  6.9217e-02, -4.2339e-02, -6.3719e-02, -6.3548e-02,\n        -2.2874e-02,  4.8931e-02, -6.5197e-02, -3.6388e-03, -5.9550e-02,\n         1.4145e-02, -6.7820e-02,  8.0643e-03,  4.7988e-04, -5.2182e-03,\n         4.9230e-02,  1.8810e-02,  2.2867e-02,  6.2899e-02, -5.8654e-02,\n        -1.8115e-02,  3.1567e-03, -3.5663e-02, -3.2396e-02, -4.7927e-02,\n        -1.5136e-02, -3.9121e-02, -5.8975e-02, -3.2872e-02, -4.0918e-03,\n        -1.2294e-02, -4.9429e-02, -1.1653e-02,  1.4716e-02,  6.7850e-02,\n         1.0170e-03, -1.6405e-02,  3.4340e-02,  2.3622e-03, -4.7330e-02,\n        -4.0428e-02,  1.8003e-02, -6.7484e-02, -6.8583e-02,  1.5042e-02,\n        -6.6698e-02, -5.4417e-02, -5.3714e-02,  4.8698e-02, -4.1948e-02,\n         1.2054e-02, -5.4133e-02,  5.0374e-02, -3.8047e-02,  5.3701e-02,\n         3.0602e-02,  5.9400e-02, -1.4688e-02,  7.1844e-02, -4.0422e-03,\n        -5.8019e-02, -3.2806e-02, -2.6394e-02, -2.7096e-02,  4.6909e-02,\n         5.7387e-02, -1.3677e-03, -5.2774e-02, -6.7048e-02, -5.6553e-02,\n        -3.5585e-02, -1.7234e-02, -1.2023e-02, -3.8642e-03,  9.3347e-03,\n         3.9468e-02, -5.8679e-02, -3.1387e-02, -3.8280e-02, -3.7967e-02,\n         5.5556e-02,  1.7866e-02,  3.8690e-02, -6.6362e-02, -4.7147e-02,\n         5.9343e-02,  5.6553e-02, -6.8394e-02,  3.2999e-02,  2.9241e-02,\n         4.0916e-02,  6.9864e-02, -1.1331e-02, -5.7399e-02, -1.0702e-02,\n        -1.4557e-03, -1.6191e-02,  6.3667e-02,  6.4834e-02,  1.8984e-02,\n         3.9429e-02, -3.2348e-02,  1.2110e-02,  6.4095e-02, -2.7228e-02,\n         3.8733e-02,  8.5370e-03,  3.9200e-02,  7.8427e-03,  1.6974e-02,\n        -2.8339e-03,  5.3975e-02, -1.8987e-02,  2.1734e-02, -2.0133e-02,\n         2.5462e-02, -2.5154e-02, -3.9411e-02,  5.4616e-02,  2.0763e-02,\n         1.5087e-02, -6.2729e-03,  2.4460e-02,  6.7568e-02, -1.8553e-02,\n         5.6837e-02, -4.5583e-02,  5.3600e-02,  3.1309e-02, -6.7997e-02,\n         7.0199e-02, -6.1992e-03, -6.8729e-02,  8.9993e-03, -6.1705e-02,\n         7.0840e-02, -5.0279e-02,  2.8114e-02,  2.5595e-02,  5.7926e-02,\n        -2.9796e-02, -6.5180e-02,  6.7032e-03,  1.9321e-02, -1.2956e-05,\n         5.4912e-02,  3.2289e-02, -1.2415e-02, -1.3727e-02, -6.3522e-02,\n        -4.9437e-02, -9.2069e-03, -3.4942e-02,  6.5189e-02,  2.6927e-02,\n         2.9763e-03,  3.4346e-02,  5.4890e-02, -6.3548e-03,  4.3065e-02,\n         3.6476e-02,  1.1447e-02, -2.5248e-02, -2.4519e-02, -5.8836e-02,\n         1.0866e-02,  2.2152e-02, -5.7454e-04,  3.3823e-02,  3.5599e-02,\n         6.2790e-02,  4.2155e-02,  3.9263e-02, -7.9459e-03,  6.3161e-02,\n        -3.0672e-02, -7.1739e-02, -1.0541e-02, -4.0018e-02,  6.4597e-02,\n        -2.8253e-03, -6.1661e-04, -5.5255e-03, -6.1260e-02,  2.4451e-02,\n        -6.9271e-02,  6.1981e-02, -1.3354e-02,  5.2941e-02,  4.6412e-02,\n         4.2859e-03,  7.4155e-03,  6.9977e-02,  7.1511e-02, -3.8729e-02,\n        -6.8443e-02, -6.6945e-02,  4.4015e-02, -5.9153e-02, -2.0860e-02,\n        -3.0047e-03,  2.5472e-02,  4.7971e-02, -5.6354e-02,  7.5240e-03,\n        -3.1204e-03, -5.5208e-02, -6.4132e-02,  6.2323e-02,  4.2147e-02,\n        -5.0387e-02, -5.5656e-03, -1.3863e-02, -4.4912e-02,  2.3088e-02,\n        -7.1683e-03, -3.6792e-02, -3.8680e-02, -2.4463e-02,  1.2004e-02,\n        -4.8120e-02, -7.1881e-02,  1.8192e-02, -8.0675e-03, -1.2855e-02,\n         7.5806e-03, -1.5610e-02, -6.2842e-03,  4.0924e-02,  1.3564e-02,\n        -1.9863e-02,  4.7100e-02,  7.2772e-03,  2.5760e-02,  6.5436e-02,\n        -3.6590e-02, -4.2043e-02, -4.7377e-02,  2.0617e-03, -5.7430e-02,\n        -1.5792e-02, -4.9522e-02, -5.5969e-02, -3.9535e-02, -3.8286e-02,\n        -3.3067e-02, -4.9701e-02, -2.3679e-02, -5.0004e-02,  1.4024e-02,\n         1.7654e-02,  5.7547e-02,  3.1976e-02, -5.6966e-02,  4.3819e-02,\n         5.4023e-02, -1.5418e-02,  6.0432e-02,  3.5015e-02, -5.1951e-02,\n         6.9801e-02,  5.7114e-02,  6.8288e-02, -6.0361e-02,  1.5239e-02,\n        -1.5566e-02,  5.6712e-03,  5.9542e-02, -3.9085e-02, -2.3007e-02,\n         4.5076e-02, -4.9383e-02, -3.5962e-02,  2.0773e-02, -6.8211e-02,\n        -3.4245e-02, -1.7106e-02,  2.1024e-02,  4.1832e-02,  7.3773e-03,\n        -1.5993e-02, -4.7209e-02, -4.7874e-02, -4.3675e-02,  7.3778e-03,\n         5.7082e-02, -5.9704e-03, -1.9373e-02,  2.6489e-02, -2.5635e-02,\n         3.4747e-02, -2.2751e-02, -6.5803e-02,  4.6877e-02,  6.1613e-02,\n        -4.1363e-02,  1.0442e-02,  5.3612e-03,  6.5975e-02,  5.2814e-02,\n        -5.7705e-02, -3.2271e-02, -3.5152e-02,  4.7634e-03,  6.6148e-02,\n        -3.6155e-02, -1.3073e-02,  3.3079e-02,  4.6816e-02,  5.1182e-02,\n         4.4731e-02, -3.0216e-02, -1.2463e-02,  3.2105e-02,  2.5737e-02,\n         5.9146e-02,  7.2907e-03,  2.6768e-03,  1.2634e-03, -6.6827e-02,\n         5.5067e-02, -3.2294e-02,  5.7130e-02, -7.5777e-03, -3.9240e-02,\n        -5.0674e-02,  2.5321e-02,  4.1353e-02, -7.1807e-02, -3.5862e-03,\n         3.1627e-02, -1.5115e-02, -3.6908e-02, -2.7511e-02,  8.6004e-03,\n         5.5805e-02,  6.1454e-02,  6.8166e-02,  1.0973e-02, -4.4280e-03,\n        -9.6677e-04, -6.5421e-02,  3.0936e-02, -4.0082e-02, -2.0507e-02,\n        -6.7770e-02,  1.5083e-03,  2.6848e-02, -3.9553e-02,  3.8829e-02,\n         5.6089e-02,  6.1677e-02,  6.0527e-02, -1.5990e-02,  6.2172e-02,\n         1.2449e-02, -5.9151e-02, -2.1863e-02,  3.1395e-02, -2.6859e-02,\n         5.4321e-03,  6.7510e-02, -3.0181e-02,  1.1713e-02, -6.8115e-02,\n         2.7658e-02, -2.7963e-02,  7.7028e-03,  5.2464e-02,  3.6213e-02,\n        -6.8485e-02, -5.4110e-03,  6.8182e-03, -7.0404e-02, -2.9727e-02,\n        -1.3161e-02, -3.7931e-02, -4.6596e-02, -3.1699e-02,  2.2498e-02,\n         7.0819e-02, -4.0572e-02, -4.0618e-02, -1.2930e-02, -6.7777e-02,\n         4.3306e-02, -3.3921e-02, -1.7802e-02, -1.1804e-02, -4.4823e-02,\n        -2.6778e-02,  3.2928e-02, -4.1808e-02,  6.9562e-02, -4.3938e-02,\n        -1.7800e-02,  2.9017e-02,  9.9307e-03, -6.2441e-02,  5.7572e-02,\n         6.2297e-02, -5.1319e-02, -2.8158e-02, -6.5213e-02, -2.0299e-03,\n         1.6476e-03,  5.8438e-02,  3.3156e-03,  6.1247e-02,  6.5704e-02,\n        -5.6795e-02,  1.6975e-02,  1.5770e-02,  1.9362e-03, -6.5543e-02,\n         4.2553e-02,  6.5418e-03, -6.8210e-02, -7.0579e-02, -6.3339e-04,\n        -1.2377e-02, -8.5508e-03,  2.1156e-02, -4.0332e-02, -5.9291e-02,\n        -5.7460e-02, -2.8856e-02,  1.1735e-02, -3.6232e-02, -4.8597e-02,\n         6.5908e-02,  4.6904e-02,  6.2042e-02,  3.9383e-02,  7.0368e-02,\n         2.0370e-02, -6.4485e-02, -3.7560e-02,  7.1459e-02,  4.1177e-03,\n        -1.3670e-02,  4.9999e-02,  2.6719e-02, -6.7981e-02,  6.6212e-03,\n         4.4441e-02, -7.0637e-02, -2.4766e-02, -4.9915e-03,  5.4175e-02,\n        -6.0131e-02,  5.9111e-02,  1.7407e-02, -7.1412e-02,  3.6682e-03,\n         4.4225e-02,  3.2964e-02, -2.8968e-02, -8.4079e-03,  7.4430e-03,\n         4.9858e-02,  2.0514e-02,  5.6842e-02,  4.3312e-02,  6.1451e-02,\n        -8.1394e-03,  2.8336e-02,  1.8318e-02, -3.2082e-02,  3.8708e-03,\n        -1.2229e-02,  1.7852e-02,  8.4492e-03,  6.8340e-02,  5.0272e-02,\n         1.9533e-02,  6.5861e-02, -1.5744e-02, -2.8269e-02,  2.5902e-02,\n         2.6753e-02, -5.5478e-02, -2.6385e-02,  3.5767e-03, -3.4754e-02,\n        -1.8783e-02, -6.7859e-02,  2.5213e-02,  3.7981e-02, -3.7936e-02,\n         6.3917e-02,  6.8026e-02,  5.7448e-02, -2.0104e-02,  4.5386e-02,\n         3.3688e-02,  7.1345e-02, -2.5136e-02, -4.0802e-02, -5.3309e-02,\n         5.6536e-02,  4.8900e-02,  6.9154e-02,  7.0634e-02,  5.4553e-02,\n        -2.0532e-02, -3.9656e-02, -1.3264e-02,  2.0831e-02,  1.4014e-02,\n         5.1395e-02,  1.7166e-02,  5.4381e-02, -4.7667e-02,  1.9502e-02,\n        -5.4555e-02, -6.8830e-02, -2.7475e-03,  6.3427e-02, -1.2179e-02,\n         6.7604e-02,  5.6690e-02, -1.0464e-03,  4.9995e-02, -5.2302e-03,\n         4.9978e-02,  5.8469e-02,  3.3222e-02, -1.6738e-02, -3.8155e-02,\n         1.4085e-02,  7.3133e-03, -4.3855e-02, -4.5683e-02,  2.7860e-02,\n         2.1022e-02, -3.7295e-02,  6.4316e-02,  2.7976e-02,  7.2124e-02,\n        -3.1379e-02,  6.5392e-02,  3.9427e-02,  6.7251e-02,  5.2888e-02,\n        -5.7950e-02,  5.8631e-02, -2.7398e-02,  4.5602e-02,  2.6338e-02,\n         5.8879e-02,  2.9936e-02, -2.3598e-02,  2.8667e-02,  4.8195e-03,\n         8.5568e-03, -3.5885e-02, -3.6347e-02, -3.8981e-02,  6.9264e-02,\n        -6.7122e-02, -1.5195e-02, -2.9635e-02,  6.2646e-02,  2.8796e-02,\n         1.3146e-02, -3.8984e-02,  6.5374e-02, -4.2529e-03, -5.7868e-02,\n        -2.3789e-02,  5.9689e-02, -5.5718e-02,  2.1403e-02, -3.3733e-02,\n        -2.5732e-03, -3.7709e-02, -2.6299e-02,  3.6277e-03, -6.0064e-02,\n         2.6968e-02,  5.4011e-02,  3.4912e-02, -3.9403e-02, -1.2856e-02,\n        -1.1777e-02, -3.5365e-02, -6.9538e-02,  2.9881e-02,  5.4358e-02,\n         1.2422e-02,  2.5674e-02,  2.3180e-02, -1.5261e-02, -1.2979e-02,\n        -2.4268e-02, -4.3912e-02, -1.7275e-02,  1.0322e-02,  6.4262e-02,\n         5.1844e-02, -5.7167e-02,  6.8229e-02, -6.6078e-02, -4.7999e-02,\n         3.5076e-02, -6.9111e-02,  6.0575e-02,  2.9154e-02,  1.0301e-02,\n        -6.5775e-02, -1.3997e-02,  2.3764e-02, -2.9432e-02, -2.7662e-03,\n         3.3528e-02, -2.8177e-02,  1.3096e-02, -4.9009e-02,  1.6164e-02,\n        -9.5409e-03,  5.3267e-04,  5.1109e-02, -4.6207e-02,  6.7561e-02,\n        -3.7156e-02,  3.1145e-03,  1.3818e-02, -6.6527e-03, -6.6154e-02,\n        -4.3831e-02, -4.4618e-02, -5.2543e-02, -6.4515e-02,  2.3748e-02,\n         4.4609e-02,  4.2347e-02, -3.8451e-02,  2.5436e-02, -5.1093e-02,\n         5.3825e-02,  1.2379e-02, -5.1992e-02, -3.6628e-02,  3.8845e-02,\n        -5.3618e-02,  5.2614e-02, -2.4070e-02, -1.1291e-02, -5.2103e-02,\n         6.1145e-02, -4.0126e-02,  2.0252e-02, -4.7723e-03,  3.4709e-02,\n         1.3534e-03,  4.9876e-02,  5.7686e-02,  4.2896e-02,  2.8241e-02,\n        -8.0937e-03, -7.1684e-02,  6.2438e-02], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([[-0.0105,  0.0356,  0.0225,  ..., -0.0162,  0.0351,  0.0107],\n        [ 0.0300,  0.0126, -0.0243,  ..., -0.0146,  0.0056, -0.0319],\n        [-0.0264,  0.0080,  0.0174,  ...,  0.0168, -0.0168,  0.0286],\n        ...,\n        [-0.0356,  0.0303,  0.0308,  ...,  0.0007,  0.0010,  0.0310],\n        [ 0.0159, -0.0096, -0.0222,  ..., -0.0344, -0.0326,  0.0302],\n        [ 0.0290, -0.0171,  0.0164,  ..., -0.0087,  0.0197, -0.0146]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 0.0313, -0.0350, -0.0156, -0.0064, -0.0211,  0.0112, -0.0314,  0.0089,\n         0.0143,  0.0246,  0.0021,  0.0338, -0.0231, -0.0315,  0.0168,  0.0302,\n        -0.0279, -0.0281,  0.0261, -0.0317, -0.0095,  0.0333, -0.0317,  0.0092,\n        -0.0032, -0.0213, -0.0152, -0.0116,  0.0045,  0.0156, -0.0172, -0.0360,\n         0.0260, -0.0289,  0.0197, -0.0261,  0.0035, -0.0264, -0.0329,  0.0224,\n         0.0133, -0.0314, -0.0313, -0.0320,  0.0246,  0.0314,  0.0154,  0.0197,\n        -0.0296, -0.0350,  0.0218, -0.0011, -0.0328,  0.0227,  0.0325,  0.0017,\n        -0.0237,  0.0105, -0.0287,  0.0131, -0.0346,  0.0242,  0.0116,  0.0332,\n        -0.0098, -0.0211,  0.0049,  0.0150,  0.0169,  0.0168,  0.0148, -0.0307,\n         0.0208,  0.0318, -0.0172,  0.0142,  0.0060,  0.0329,  0.0104, -0.0279,\n        -0.0291, -0.0242,  0.0102, -0.0248, -0.0200, -0.0283, -0.0347, -0.0310,\n         0.0117,  0.0071, -0.0112, -0.0356,  0.0065, -0.0043, -0.0131,  0.0240,\n         0.0053, -0.0283,  0.0240,  0.0077, -0.0310, -0.0261,  0.0281,  0.0331,\n         0.0274,  0.0119, -0.0095,  0.0245,  0.0047, -0.0299,  0.0025,  0.0268,\n        -0.0083, -0.0139, -0.0306,  0.0350,  0.0155,  0.0300,  0.0162, -0.0077,\n        -0.0031, -0.0184,  0.0280,  0.0087,  0.0103, -0.0099, -0.0192,  0.0008,\n        -0.0044,  0.0102, -0.0147, -0.0048, -0.0014, -0.0126,  0.0231,  0.0238,\n         0.0005, -0.0343, -0.0045, -0.0075, -0.0059,  0.0113,  0.0042,  0.0281,\n         0.0339,  0.0054,  0.0021,  0.0132, -0.0333, -0.0068,  0.0329, -0.0073,\n         0.0128,  0.0301, -0.0044, -0.0113,  0.0053,  0.0232, -0.0172, -0.0035,\n        -0.0136, -0.0156, -0.0118, -0.0330,  0.0312,  0.0270, -0.0317,  0.0073,\n        -0.0195, -0.0226, -0.0033,  0.0351, -0.0317, -0.0201,  0.0186, -0.0283,\n         0.0148,  0.0291,  0.0117, -0.0009, -0.0254, -0.0091, -0.0166,  0.0131,\n         0.0241,  0.0212, -0.0049,  0.0039, -0.0133, -0.0191,  0.0351, -0.0255],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0684,  0.0314, -0.0631,  ..., -0.0083, -0.0291, -0.0319],\n        [-0.0248,  0.0051,  0.0332,  ...,  0.0335, -0.0230, -0.0346],\n        [-0.0024,  0.0231,  0.0580,  ..., -0.0649,  0.0421,  0.0068],\n        ...,\n        [ 0.0477,  0.0488,  0.0187,  ...,  0.0592,  0.0314, -0.0201],\n        [ 0.0173,  0.0524, -0.0525,  ...,  0.0379,  0.0114, -0.0204],\n        [-0.0583,  0.0302,  0.0215,  ...,  0.0429,  0.0599, -0.0035]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0598, -0.0662,  0.0717,  ...,  0.0291,  0.0094,  0.0118],\n        [-0.0070, -0.0089, -0.0352,  ...,  0.0494,  0.0054, -0.0032],\n        [-0.0382, -0.0023,  0.0134,  ...,  0.0477, -0.0439,  0.0015],\n        ...,\n        [-0.0229, -0.0206,  0.0384,  ...,  0.0648, -0.0069, -0.0284],\n        [-0.0103, -0.0019,  0.0067,  ..., -0.0500, -0.0311,  0.0533],\n        [ 0.0089, -0.0721, -0.0679,  ..., -0.0307,  0.0219, -0.0636]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0247,  0.0473,  0.0602,  ...,  0.0347,  0.0181,  0.0231],\n        [ 0.0097,  0.0353, -0.0469,  ..., -0.0141, -0.0408,  0.0577],\n        [ 0.0230,  0.0391, -0.0507,  ..., -0.0010,  0.0666,  0.0319],\n        ...,\n        [ 0.0407, -0.0091,  0.0675,  ...,  0.0478, -0.0267, -0.0603],\n        [-0.0077,  0.0674, -0.0244,  ..., -0.0525, -0.0184,  0.0278],\n        [-0.0649,  0.0480, -0.0241,  ...,  0.0126,  0.0541,  0.0009]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[-0.0375, -0.0657,  0.0685,  ...,  0.0561, -0.0564, -0.0507],\n        [-0.0120,  0.0251,  0.0232,  ..., -0.0004, -0.0006,  0.0566],\n        [ 0.0614,  0.0283, -0.0299,  ..., -0.0498,  0.0424, -0.0298],\n        ...,\n        [ 0.0182,  0.0369, -0.0138,  ...,  0.0547,  0.0336,  0.0656],\n        [-0.0171,  0.0130,  0.0218,  ..., -0.0104,  0.0339,  0.0275],\n        [-0.0365, -0.0355,  0.0123,  ..., -0.0482,  0.0167,  0.0531]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 0.0109, -0.0243, -0.0101,  0.0109, -0.0534,  0.0490, -0.0227, -0.0464,\n         0.0106, -0.0065, -0.0606,  0.0103, -0.0256,  0.0204, -0.0457,  0.0223,\n         0.0600,  0.0587, -0.0496,  0.0313,  0.0173,  0.0225, -0.0151, -0.0645,\n         0.0247, -0.0464,  0.0152, -0.0283, -0.0261, -0.0246,  0.0412, -0.0142,\n        -0.0330,  0.0654,  0.0690,  0.0422,  0.0018,  0.0138, -0.0628, -0.0035,\n         0.0549, -0.0040, -0.0010,  0.0155, -0.0625,  0.0594, -0.0146, -0.0508,\n         0.0152, -0.0466,  0.0362,  0.0453,  0.0425,  0.0175,  0.0351, -0.0329,\n        -0.0355, -0.0580, -0.0259, -0.0489, -0.0422, -0.0245,  0.0390,  0.0493,\n        -0.0661,  0.0075, -0.0657,  0.0347, -0.0121, -0.0236, -0.0063, -0.0686,\n         0.0712,  0.0417,  0.0718, -0.0319,  0.0221, -0.0267, -0.0040,  0.0045,\n        -0.0624,  0.0138,  0.0074, -0.0289, -0.0104,  0.0260, -0.0504, -0.0632,\n         0.0008, -0.0485,  0.0277, -0.0424,  0.0681,  0.0186,  0.0423,  0.0605,\n         0.0318,  0.0690, -0.0577,  0.0031,  0.0533,  0.0624,  0.0322,  0.0500,\n         0.0054,  0.0646,  0.0498,  0.0015, -0.0379, -0.0665, -0.0667, -0.0622,\n        -0.0091, -0.0171,  0.0470,  0.0626, -0.0164,  0.0383, -0.0450,  0.0134,\n        -0.0500, -0.0246, -0.0081, -0.0331,  0.0269,  0.0156, -0.0493,  0.0284,\n         0.0322,  0.0522, -0.0409, -0.0540,  0.0516, -0.0126, -0.0643,  0.0529,\n         0.0625,  0.0148,  0.0060,  0.0309,  0.0560, -0.0078,  0.0196, -0.0074,\n         0.0346,  0.0531, -0.0504,  0.0552, -0.0582,  0.0643, -0.0188, -0.0089,\n         0.0710,  0.0174, -0.0539,  0.0572, -0.0394,  0.0162, -0.0065,  0.0010,\n        -0.0370,  0.0492,  0.0484, -0.0346, -0.0160, -0.0704,  0.0514,  0.0370,\n         0.0651, -0.0083,  0.0009, -0.0012, -0.0707,  0.0326,  0.0425,  0.0255,\n         0.0617, -0.0354,  0.0216, -0.0451,  0.0458,  0.0308, -0.0643,  0.0206,\n         0.0688, -0.0198,  0.0314,  0.0345, -0.0028, -0.0612,  0.0386,  0.0624],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.0055,  0.0239, -0.0056,  ...,  0.0546,  0.0511,  0.0626],\n        [-0.0066, -0.0164, -0.0163,  ...,  0.0480,  0.0024,  0.0477],\n        [ 0.0509,  0.0370,  0.0261,  ..., -0.0708, -0.0136, -0.0575],\n        ...,\n        [-0.0696,  0.0261,  0.0367,  ..., -0.0545,  0.0185,  0.0297],\n        [ 0.0480,  0.0080, -0.0454,  ...,  0.0394, -0.0295,  0.0280],\n        [ 0.0567, -0.0544,  0.0008,  ..., -0.0598,  0.0013, -0.0444]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([ 1.2767e-02, -6.3104e-02, -5.5320e-02, -1.1053e-02, -3.1923e-02,\n        -1.4351e-03,  8.6172e-03,  1.4313e-02,  3.0064e-02, -4.7307e-02,\n         7.0717e-02,  4.9939e-02,  6.9105e-02, -5.2711e-02,  2.4880e-02,\n        -6.5196e-02, -5.9467e-02, -1.3222e-02, -5.0131e-03,  5.9088e-02,\n         6.1269e-02, -3.5354e-03, -6.1900e-02, -4.4094e-02,  5.9660e-02,\n         6.7316e-02,  5.9488e-02, -2.0731e-02, -5.7151e-02, -3.3597e-02,\n        -1.2549e-02, -1.6503e-02, -1.8078e-02,  6.7583e-02,  6.9798e-02,\n         5.9607e-02, -1.5342e-03,  4.7344e-02,  2.1259e-02, -5.8343e-03,\n        -3.4601e-02,  6.5671e-02,  6.7218e-02,  5.7032e-02,  5.7368e-02,\n        -3.0771e-02, -1.4577e-02, -2.0144e-02,  6.5797e-02, -3.5691e-02,\n         1.3153e-02, -2.4919e-02, -4.3352e-02, -9.6747e-03,  6.2175e-02,\n        -5.9262e-02,  3.9048e-02, -4.4264e-02,  6.6392e-02,  6.8323e-02,\n         5.9837e-02,  1.7482e-02,  4.3965e-02,  6.1701e-02, -4.3033e-02,\n         4.6776e-02, -4.9852e-02, -1.0481e-02, -2.2629e-02,  6.3295e-02,\n         4.0276e-02, -2.6133e-02, -7.7436e-03,  4.6008e-02,  2.5842e-02,\n         4.1732e-02, -4.1685e-02,  3.8553e-02, -5.2033e-02,  4.3224e-02,\n        -3.2293e-02,  3.4871e-02,  3.1235e-02,  8.4099e-03, -6.4635e-02,\n         2.7421e-02, -6.0093e-02, -1.0317e-02, -6.8317e-02, -1.5809e-02,\n        -1.5161e-02, -6.6414e-03,  2.5529e-02,  2.0975e-02, -2.3865e-02,\n         3.9297e-02, -4.1368e-02,  2.8234e-02, -4.2288e-02,  1.7029e-02,\n        -5.7172e-02, -3.2974e-03, -3.0329e-02, -7.1476e-02, -2.3234e-02,\n        -6.4763e-02, -4.3765e-02,  7.1503e-02, -5.1915e-02,  2.5514e-02,\n         3.0113e-02, -6.6941e-02, -3.7408e-02, -5.2072e-02, -4.2484e-02,\n        -6.9369e-02,  4.5385e-02,  2.0951e-02, -2.5057e-02,  4.5797e-02,\n        -6.7342e-02, -4.0506e-03,  1.1078e-02,  7.2681e-03,  6.9325e-02,\n         6.0960e-02, -1.5915e-02,  2.2568e-02, -1.4713e-02, -3.0369e-03,\n         2.0027e-02, -5.4138e-03, -5.6040e-02,  3.9951e-03, -1.8853e-02,\n         6.0292e-02,  6.6445e-02,  6.8818e-02, -9.3859e-04, -2.7537e-02,\n        -3.4091e-02,  4.0566e-02, -7.1501e-02,  1.1447e-02, -2.9301e-02,\n        -1.4325e-02, -4.0360e-02,  4.0816e-02,  1.6614e-02,  4.0926e-02,\n        -3.1214e-02,  2.7335e-02, -4.0568e-02,  1.4617e-02,  3.8866e-02,\n        -5.2187e-02,  6.5722e-02, -6.0600e-02, -2.5208e-02,  6.3299e-02,\n        -5.3508e-02, -3.1199e-02, -2.6473e-02,  5.6853e-02,  5.5717e-02,\n         5.6957e-02, -4.4339e-02, -4.1530e-02, -6.6811e-02,  4.5644e-02,\n        -9.7286e-03,  2.9462e-02,  7.9833e-03,  5.3635e-02, -7.0682e-02,\n         2.4730e-02, -2.7620e-02,  4.8800e-02, -6.5094e-02,  6.6869e-02,\n         3.0469e-02,  1.1451e-02, -4.7979e-03, -4.3908e-02,  7.8624e-04,\n        -6.2937e-02, -1.2237e-02, -1.5566e-02,  4.4278e-02,  3.3875e-02,\n        -5.8501e-02,  3.5734e-02, -1.7743e-02, -5.2218e-02, -4.4180e-02,\n         6.5640e-02,  2.5810e-02,  6.4292e-02,  5.2358e-02,  2.5101e-02,\n         3.0772e-02, -4.7176e-02,  6.3168e-02,  5.2765e-02, -1.7072e-02,\n         1.8577e-02,  5.2968e-02, -5.4959e-02,  7.0160e-03, -4.5347e-02,\n        -1.0250e-03, -5.6978e-02,  5.9524e-02,  1.2361e-03, -1.2068e-02,\n        -5.1362e-02,  4.3456e-02, -5.2572e-02,  2.0970e-02, -3.5908e-02,\n         5.6833e-03,  5.8343e-02,  4.7844e-02, -1.6409e-02,  4.1092e-04,\n         1.9320e-02, -4.2399e-02, -1.3658e-02, -6.8167e-02, -1.8515e-03,\n        -6.5184e-02, -3.2873e-02,  5.6477e-02, -4.3271e-02,  3.1578e-02,\n         5.5657e-02,  3.7212e-02, -6.0804e-02, -3.6596e-02, -4.5934e-02,\n        -3.6049e-02,  5.6101e-02,  1.0770e-02,  1.1706e-02,  5.5483e-02,\n        -4.3985e-02, -1.8244e-02,  4.1915e-02, -3.5657e-02,  1.9077e-02,\n         2.0412e-02, -1.0822e-02, -2.5691e-02,  6.0917e-02, -2.6257e-05,\n        -3.0173e-02, -4.9769e-03, -5.4032e-02, -1.1180e-02,  4.4948e-02,\n        -4.1625e-02,  1.0620e-02, -3.3683e-03, -6.0963e-04, -4.5682e-02,\n         2.6049e-02,  7.0973e-02,  1.2897e-02,  4.6489e-02,  5.6319e-02,\n        -1.5550e-02,  6.7971e-03, -3.3353e-02,  4.7814e-02, -4.7271e-02,\n         3.4183e-03, -3.7851e-02, -6.8798e-02, -4.2605e-02, -2.5108e-02,\n         5.2707e-02, -4.8910e-03,  6.6447e-03,  3.6289e-02, -4.6633e-02,\n        -2.4913e-02,  5.5130e-02,  5.2655e-02,  4.6590e-02,  2.4724e-02,\n        -4.0100e-02, -6.8205e-02, -4.4270e-02, -4.9593e-02, -5.8625e-02,\n         1.6020e-03, -3.5640e-02, -8.2255e-05,  6.8356e-02, -3.8466e-02,\n        -3.6998e-03,  7.8063e-03,  5.3010e-03, -1.7159e-02, -2.0614e-02,\n         1.3895e-02, -3.9943e-02,  1.6456e-02,  2.6703e-02,  1.9807e-03,\n        -6.2640e-02, -4.6612e-02,  6.2066e-02,  3.1961e-02,  6.2988e-02,\n        -1.2310e-02, -1.7358e-02, -6.7333e-02,  4.4589e-02, -7.1195e-02,\n         1.1719e-02, -3.6945e-02,  5.1862e-02,  6.6495e-02,  1.8380e-02,\n        -5.3943e-02,  1.6029e-03, -6.8565e-02,  4.9292e-02,  6.7255e-02,\n         5.5738e-03,  2.3828e-02, -1.9540e-04,  2.3505e-03,  6.8183e-03,\n        -2.6810e-02, -2.2178e-02,  3.4771e-02,  6.2136e-02,  5.0354e-02,\n         1.4289e-02, -5.0505e-02,  2.2097e-02,  1.7421e-02, -6.7172e-02,\n         1.8943e-02,  1.8812e-02, -1.1597e-02, -4.2198e-02, -1.0674e-02,\n         3.2279e-02, -6.2433e-02,  1.0550e-02, -6.7049e-02, -4.0785e-02,\n         6.8250e-02, -3.6396e-02,  2.4263e-02,  3.2128e-02,  6.3319e-02,\n        -4.9513e-02,  2.6019e-03,  9.4250e-03, -1.3786e-02,  2.2057e-02,\n         4.6219e-02, -4.0174e-02, -2.2010e-02,  4.3191e-03, -4.7978e-03,\n        -6.9451e-02, -6.4105e-03,  7.1169e-02,  2.5251e-02, -8.6100e-03,\n         6.0844e-02,  6.4051e-02,  1.2045e-03,  1.4631e-02, -1.3789e-02,\n         2.6208e-02, -5.2679e-02, -6.9422e-02, -3.8423e-03,  1.8824e-02,\n         3.2170e-02, -3.5913e-02, -4.8813e-03, -5.6628e-02, -3.2353e-02,\n         6.7721e-02, -2.1558e-02,  3.7543e-02,  1.3017e-03, -1.7606e-02,\n         6.6437e-02,  2.8553e-02,  5.7818e-02, -1.3361e-02, -9.8852e-03,\n        -2.6187e-03, -6.7545e-02, -2.1188e-02, -4.8145e-02, -4.6274e-02,\n         4.1780e-02,  2.4237e-02,  4.7533e-02, -6.9246e-03,  3.9239e-02,\n         1.8380e-02,  2.6239e-02, -2.0963e-02, -2.6519e-02, -4.0423e-02,\n         4.8137e-02,  3.5368e-02,  5.2485e-02, -1.1022e-02, -2.9745e-03,\n        -9.3446e-03,  6.4166e-02, -5.9950e-02, -3.0088e-02,  1.6072e-02,\n        -5.9111e-02,  3.1550e-02, -5.6389e-02, -6.9279e-02, -2.9082e-02,\n        -5.5428e-02, -4.6838e-02,  2.0701e-02, -6.4694e-02,  7.5873e-03,\n        -4.3475e-02,  1.1254e-02, -5.3007e-02,  3.5239e-02, -2.6981e-02,\n        -3.0534e-02,  5.5054e-02, -5.1613e-02, -3.2759e-02, -6.4354e-02,\n        -3.1533e-02,  4.8619e-02,  5.2936e-02,  6.9933e-02, -5.0484e-02,\n         2.0790e-02,  4.5812e-02,  3.7566e-03,  5.4031e-02, -3.2500e-02,\n         3.7199e-02, -3.8302e-02,  5.7830e-02,  7.1859e-02,  6.5581e-02,\n        -6.3397e-02, -5.9729e-02, -4.0851e-03, -6.7339e-02, -5.2494e-02,\n         2.3526e-02,  4.9160e-02,  6.5334e-02, -1.1188e-02, -3.5846e-02,\n        -1.1554e-02, -1.5846e-02, -3.1276e-02,  6.4890e-02, -2.2803e-02,\n        -4.7957e-02,  3.9645e-02, -1.5305e-02,  6.1666e-02, -5.6981e-02,\n        -3.9483e-02,  6.6057e-02, -4.3280e-02, -2.1090e-02,  3.9627e-02,\n        -6.4342e-02,  1.6465e-02,  1.6731e-02,  4.9725e-03, -6.6539e-02,\n         2.2151e-02, -3.8902e-02,  2.6785e-02,  3.8924e-02, -2.1744e-02,\n         5.9477e-02,  5.3397e-02,  7.0221e-02,  7.0421e-02,  2.4653e-03,\n         2.7798e-02, -4.6171e-02,  1.7401e-02,  3.1369e-02,  4.1604e-03,\n         5.4837e-02,  5.2220e-02,  1.3578e-02, -6.9329e-02, -1.8330e-02,\n        -6.8698e-02, -4.4479e-02, -3.8935e-02, -5.2953e-02, -5.9571e-02,\n        -4.0505e-02, -3.8469e-02, -4.3515e-03, -4.8577e-02,  7.4868e-03,\n        -5.6027e-02,  6.9529e-02,  6.4235e-02,  5.3675e-02,  4.6267e-03,\n        -2.0684e-02,  1.6951e-02,  1.6283e-02,  2.5531e-03,  1.4623e-02,\n        -6.0234e-02,  2.2686e-02, -4.4755e-02, -5.2902e-03, -2.6878e-03,\n        -2.3834e-03, -1.0609e-04, -6.0012e-02,  1.3902e-02,  3.4529e-02,\n         7.0218e-03, -1.9188e-02,  6.6333e-03,  1.2360e-03, -5.4806e-02,\n        -6.9680e-02,  4.0603e-02, -3.4844e-02,  5.5207e-02, -6.3023e-02,\n        -2.9591e-02,  6.5919e-02,  7.3324e-03, -3.5007e-02, -3.0235e-02,\n        -2.7417e-02, -4.4717e-02,  5.7972e-02, -6.6828e-02, -5.0226e-02,\n         6.0395e-02, -6.1689e-02, -1.5720e-02, -1.9941e-02,  3.2488e-02,\n        -5.2449e-02, -1.6554e-02, -2.9682e-03,  5.5780e-02,  5.3625e-02,\n         9.4470e-03, -5.7633e-02, -1.3164e-02,  5.3318e-02, -6.1956e-02,\n         3.6646e-02,  4.3100e-02,  1.0066e-02, -6.6171e-02, -4.7325e-02,\n         2.6862e-02, -3.2994e-03,  4.6034e-03,  6.8733e-02, -5.2702e-02,\n        -5.3263e-02,  5.6083e-02, -1.5013e-02,  6.7339e-03,  2.6982e-02,\n         1.7071e-02,  6.6256e-02, -2.0612e-02,  4.0450e-02, -1.2049e-02,\n         1.0772e-02, -3.5064e-02, -5.6134e-02, -6.8087e-02, -6.6452e-02,\n        -3.1799e-02,  6.1782e-02, -1.0863e-02,  4.8851e-02, -6.8059e-02,\n        -2.8449e-02, -5.3961e-02, -6.5213e-02,  5.1974e-02, -6.0868e-02,\n        -1.4885e-02, -4.7067e-03, -2.5931e-02, -9.9718e-03, -5.5353e-02,\n         3.1047e-02, -5.4522e-02, -6.3164e-02,  3.6632e-02, -3.5766e-02,\n         4.1181e-02,  4.8655e-02, -3.3991e-02, -4.6561e-02, -5.8825e-03,\n        -6.1040e-02, -2.1730e-03,  2.4858e-02, -1.6958e-02, -1.0007e-02,\n         3.9244e-03, -6.4855e-02,  6.4822e-02,  6.7613e-02,  2.5567e-02,\n         8.6365e-03,  2.6470e-02, -2.0031e-02, -4.9495e-02,  6.5028e-02,\n         2.4015e-02, -1.9707e-02, -2.9546e-02,  6.3112e-02, -6.3957e-02,\n         5.6245e-02, -2.1537e-02,  4.2512e-02,  5.4303e-03,  1.2735e-02,\n        -4.6561e-02,  1.2538e-02, -3.5161e-02,  7.1256e-02, -6.0844e-02,\n         1.4696e-02, -8.9246e-03, -8.7076e-03, -1.3378e-02, -2.2245e-02,\n         3.3137e-02, -5.6613e-02, -7.0408e-02,  1.9920e-03, -3.6781e-02,\n        -3.1971e-02,  1.9289e-02, -4.9985e-02,  2.9692e-02, -5.5655e-02,\n        -5.5212e-02,  5.3034e-02,  5.6828e-02, -3.2013e-02,  1.2779e-02,\n        -3.9111e-02, -4.0701e-02, -7.0060e-03, -1.2338e-02, -4.4129e-02,\n        -5.3952e-02,  2.8272e-02,  5.9263e-02, -2.9624e-02, -3.7091e-02,\n         3.8325e-02, -2.3694e-02, -4.3787e-02, -4.1038e-02,  5.6761e-02,\n        -1.1895e-02, -1.5018e-02,  6.7953e-02, -1.5572e-02,  1.1451e-02,\n         4.2557e-02, -3.8806e-02,  6.5658e-03,  4.6794e-04, -3.8548e-02,\n         6.8476e-02, -3.6908e-02, -1.0609e-02,  1.5724e-02, -6.4153e-02,\n         4.4392e-02, -4.0673e-02,  3.9536e-02,  2.4893e-02,  5.2689e-02,\n         2.0717e-02, -1.8644e-02,  5.6864e-02, -3.3427e-02,  5.8114e-02,\n        -7.5511e-03, -5.7317e-02, -4.3337e-02,  7.0103e-02,  4.6018e-02,\n         6.6866e-02, -4.9424e-02, -1.4863e-02, -4.6930e-03,  6.1020e-02,\n         5.5348e-03, -7.1648e-02,  3.0309e-05, -3.1269e-02,  1.9746e-02,\n         5.1166e-02, -4.3815e-02,  5.3681e-02, -1.5094e-02,  4.8790e-03,\n        -5.7906e-02, -5.3919e-02, -2.3207e-02,  3.7379e-02, -5.4725e-02,\n        -5.1655e-02, -4.7277e-02, -4.6828e-02,  6.2027e-02, -1.0851e-03,\n         3.8220e-02, -5.1204e-02, -2.8950e-02, -6.7969e-02, -1.2027e-03,\n        -4.9210e-02,  1.4120e-02,  3.7636e-02, -4.9614e-02, -2.9278e-02,\n         4.9104e-02, -5.5113e-02, -2.3162e-03, -4.6361e-02,  5.6810e-02,\n         1.9744e-02, -3.3789e-02, -4.7995e-02,  4.9561e-02, -3.9313e-02,\n         1.1140e-02,  4.9874e-02, -4.1696e-02], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([[ 0.0235,  0.0192,  0.0006,  ...,  0.0009,  0.0007, -0.0036],\n        [ 0.0300, -0.0190,  0.0336,  ..., -0.0186,  0.0205,  0.0211],\n        [ 0.0182, -0.0287,  0.0086,  ..., -0.0257,  0.0220, -0.0072],\n        ...,\n        [-0.0295,  0.0269, -0.0306,  ...,  0.0214,  0.0287, -0.0152],\n        [ 0.0286, -0.0253,  0.0321,  ...,  0.0034,  0.0321,  0.0059],\n        [ 0.0078, -0.0118, -0.0106,  ...,  0.0139,  0.0110, -0.0317]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([-0.0292,  0.0187,  0.0115, -0.0200,  0.0056, -0.0140, -0.0314,  0.0337,\n        -0.0308, -0.0273,  0.0152,  0.0322, -0.0038,  0.0259,  0.0220,  0.0132,\n        -0.0207, -0.0071, -0.0281,  0.0121, -0.0186,  0.0319,  0.0145,  0.0101,\n         0.0050,  0.0007,  0.0300, -0.0339,  0.0042, -0.0125,  0.0002, -0.0260,\n        -0.0117, -0.0108, -0.0177,  0.0354, -0.0355,  0.0090,  0.0281,  0.0188,\n         0.0183,  0.0354, -0.0102,  0.0170, -0.0051,  0.0336, -0.0162, -0.0276,\n        -0.0219, -0.0017, -0.0201,  0.0080,  0.0314, -0.0020, -0.0214, -0.0034,\n        -0.0316, -0.0058, -0.0278,  0.0223, -0.0094,  0.0206, -0.0291, -0.0297,\n        -0.0051, -0.0136, -0.0268, -0.0059,  0.0091,  0.0238, -0.0108, -0.0183,\n         0.0076,  0.0073, -0.0081,  0.0073,  0.0167, -0.0321,  0.0266, -0.0246,\n        -0.0128,  0.0122, -0.0261,  0.0322,  0.0031,  0.0204, -0.0339, -0.0190,\n         0.0287,  0.0173, -0.0082,  0.0222, -0.0205,  0.0335,  0.0336,  0.0121,\n         0.0100,  0.0327,  0.0142, -0.0187,  0.0100, -0.0024,  0.0074,  0.0280,\n         0.0048, -0.0146,  0.0067,  0.0178, -0.0095, -0.0183, -0.0145,  0.0315,\n         0.0041,  0.0104,  0.0095, -0.0212,  0.0021, -0.0312, -0.0109,  0.0155,\n         0.0331, -0.0267,  0.0013, -0.0311, -0.0294,  0.0039,  0.0003,  0.0017,\n         0.0333, -0.0308,  0.0100, -0.0325,  0.0278, -0.0054, -0.0297, -0.0048,\n         0.0263, -0.0266, -0.0133, -0.0053, -0.0101,  0.0176, -0.0279,  0.0053,\n         0.0088, -0.0289,  0.0349, -0.0118, -0.0155, -0.0031,  0.0278, -0.0081,\n         0.0081, -0.0101,  0.0290, -0.0269,  0.0249, -0.0087,  0.0055, -0.0216,\n         0.0210, -0.0282,  0.0310,  0.0266,  0.0142,  0.0026, -0.0114, -0.0327,\n         0.0283, -0.0093,  0.0104, -0.0332, -0.0141,  0.0297,  0.0308, -0.0192,\n        -0.0072, -0.0212, -0.0056, -0.0253,  0.0322,  0.0111,  0.0170, -0.0284,\n        -0.0202,  0.0129, -0.0270, -0.0298, -0.0009,  0.0214, -0.0202, -0.0337],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n       requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([[ 0.1658, -0.1471,  0.1168,  ...,  0.0117,  0.1138,  0.0073],\n        [-0.0301, -0.0816,  0.0735,  ..., -0.0994,  0.1554, -0.0231],\n        [-0.0384,  0.0099, -0.0777,  ...,  0.1158, -0.0576,  0.1168],\n        ...,\n        [ 0.0526, -0.1324,  0.0493,  ...,  0.1165, -0.1686, -0.0884],\n        [-0.1528, -0.0813,  0.1270,  ...,  0.1172,  0.1318,  0.0108],\n        [-0.0213,  0.0426,  0.0630,  ..., -0.1559, -0.0060,  0.0740]],\n       device='cuda:0', requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n       requires_grad=True)]\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_parameters(model: nn.Module) -> int:\n    \"\"\"\n    Count the number of trainable parameters in a PyTorch model.\n\n    Args:\n        model (nn.Module): The PyTorch model.\n\n    Returns:\n        int: The total number of trainable parameters.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000\n\ntorch.cuda.empty_cache()\nprint(\"Número de parámetros: {:.2f} millones\".format(count_parameters(model)))","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:07:16.892562Z","iopub.execute_input":"2024-10-29T23:07:16.893098Z","iopub.status.idle":"2024-10-29T23:07:16.900340Z","shell.execute_reply.started":"2024-10-29T23:07:16.893061Z","shell.execute_reply":"2024-10-29T23:07:16.899418Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Número de parámetros: 3.13 millones\n","output_type":"stream"}]},{"cell_type":"code","source":"# Número de épocas\nnum_epochs = 10\n\n# Listas para tracking\ntrain_losses = []\ntrain_accs = []\ntest_losses = []\ntest_accs = []\n\nfor epoch in range(num_epochs):\n    print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n    \n    # Entrenamiento\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    \n    # Evaluación\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    # Actualizar learning rate\n    scheduler.step()\n    \n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:07:33.889709Z","iopub.execute_input":"2024-10-29T23:07:33.890318Z","iopub.status.idle":"2024-10-29T23:18:48.143020Z","shell.execute_reply.started":"2024-10-29T23:07:33.890276Z","shell.execute_reply":"2024-10-29T23:18:48.141969Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\nEpoch: 1/10\n","output_type":"stream"},{"name":"stderr","text":"W1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] WON'T CONVERT forward /tmp/ipykernel_30/2056583351.py line 66 \nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] due to: \nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:44.626000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] WON'T CONVERT forward /tmp/ipykernel_30/1901395189.py line 53 \nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] due to: \nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:44.775000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] WON'T CONVERT forward /tmp/ipykernel_30/4086796280.py line 158 \nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] due to: \nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:53.852000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] WON'T CONVERT forward /tmp/ipykernel_30/4086796280.py line 141 \nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] due to: \nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:55.378000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] WON'T CONVERT forward /tmp/ipykernel_30/4086796280.py line 97 \nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] due to: \nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return _compile(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     tracer.run()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     super().run()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     while self.step():\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._return(inst)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.output.compile_subgraph(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1098, in compile_subgraph\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = self.call_user_compiler(gm)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_gm = compiler_fn(gm, example_inputs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return aot_autograd(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, _ = create_aot_dispatcher_function(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn, fw_metadata = compiler_fn(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 461, in aot_dispatch_autograd\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1410, in fw_compiler_base\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return inner_compile(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 84, in debug_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/debug.py\", line 304, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 527, in compile_fx_inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_graph = fx_codegen_and_compile(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 831, in fx_codegen_and_compile\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     compiled_fn = graph.compile_to_fn()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1749, in compile_to_fn\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return self.compile_to_module().call\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1678, in compile_to_module\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/graph.py\", line 1634, in codegen\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.scheduler = Scheduler(self.buffers)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in __init__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1364, in <listcomp>\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 1462, in create_scheduler_node\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     return SchedulerNode(self, node)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 731, in __init__\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self._compute_attrs()\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 742, in _compute_attrs\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2663, in get_backend\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     self.backends[device] = self.create_backend(device)\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 2655, in create_backend\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009]     raise RuntimeError(\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] RuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] \nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1029 23:07:56.167000 138405542786880 torch/_dynamo/convert_frame.py:1009] \n","output_type":"stream"},{"name":"stdout","text":"Batch: 0, Loss: 3.0283, Acc: 10.74%\nTrain Loss: 2.1661, Train Acc: 23.47%\nTest Loss: 1.7969, Test Acc: 33.54%\n\nEpoch: 2/10\nBatch: 0, Loss: 1.9416, Acc: 30.47%\nTrain Loss: 1.7730, Train Acc: 34.12%\nTest Loss: 1.5772, Test Acc: 42.72%\n\nEpoch: 3/10\nBatch: 0, Loss: 1.7147, Acc: 37.70%\nTrain Loss: 1.6003, Train Acc: 41.11%\nTest Loss: 1.5090, Test Acc: 44.68%\n\nEpoch: 4/10\nBatch: 0, Loss: 1.5524, Acc: 45.70%\nTrain Loss: 1.5597, Train Acc: 42.80%\nTest Loss: 1.4870, Test Acc: 45.43%\n\nEpoch: 5/10\nBatch: 0, Loss: 1.6022, Acc: 41.02%\nTrain Loss: 1.5368, Train Acc: 43.80%\nTest Loss: 1.4673, Test Acc: 46.48%\n\nEpoch: 6/10\nBatch: 0, Loss: 1.5934, Acc: 43.95%\nTrain Loss: 1.5319, Train Acc: 43.97%\nTest Loss: 1.4626, Test Acc: 46.32%\n\nEpoch: 7/10\nBatch: 0, Loss: 1.5428, Acc: 41.99%\nTrain Loss: 1.5308, Train Acc: 43.93%\nTest Loss: 1.4621, Test Acc: 46.87%\n\nEpoch: 8/10\nBatch: 0, Loss: 1.4942, Acc: 43.95%\nTrain Loss: 1.5306, Train Acc: 44.01%\nTest Loss: 1.4689, Test Acc: 46.79%\n\nEpoch: 9/10\nBatch: 0, Loss: 1.5126, Acc: 43.55%\nTrain Loss: 1.5291, Train Acc: 44.12%\nTest Loss: 1.4676, Test Acc: 46.36%\n\nEpoch: 10/10\nBatch: 0, Loss: 1.6077, Acc: 42.97%\nTrain Loss: 1.5320, Train Acc: 44.36%\nTest Loss: 1.4598, Test Acc: 46.74%\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate_model(model, data_loader):\n    y_true = []\n    y_pred = []\n    model.eval()\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader):\n            images = images.to(device).float()\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    \n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    \n    return accuracy, precision, recall, f1\n\n# Evaluar el modelo\naccuracy, precision, recall, f1 = evaluate_model(model, test_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:20:41.381999Z","iopub.execute_input":"2024-10-29T23:20:41.383212Z","iopub.status.idle":"2024-10-29T23:20:48.637991Z","shell.execute_reply.started":"2024-10-29T23:20:41.383157Z","shell.execute_reply":"2024-10-29T23:20:48.637220Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 20/20 [00:07<00:00,  2.78it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 score: {f1:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:21:00.789309Z","iopub.execute_input":"2024-10-29T23:21:00.789965Z","iopub.status.idle":"2024-10-29T23:21:00.795077Z","shell.execute_reply.started":"2024-10-29T23:21:00.789924Z","shell.execute_reply":"2024-10-29T23:21:00.794063Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.47\nPrecision: 0.46\nRecall: 0.47\nF1 score: 0.46\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset.data[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:21:13.221379Z","iopub.execute_input":"2024-10-29T23:21:13.222254Z","iopub.status.idle":"2024-10-29T23:21:13.228287Z","shell.execute_reply.started":"2024-10-29T23:21:13.222213Z","shell.execute_reply":"2024-10-29T23:21:13.227330Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(32, 32, 3)"},"metadata":{}}]},{"cell_type":"markdown","source":"3.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms.functional as TF\nimport random\nimport numpy as np\n\nclass CustomGaussianBlur:\n    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n        self.kernel_size = kernel_size\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n\n    def __call__(self, img):\n        sigma = random.uniform(self.sigma_min, self.sigma_max)\n        return TF.gaussian_blur(img, self.kernel_size, [sigma, sigma])\n\nclass RandAugment:\n    \"\"\"Implementación simplificada de RandAugment\"\"\"\n    def __init__(self, n=2, m=9):\n        self.n = n  # número de transformaciones a aplicar\n        self.m = m  # magnitud de las transformaciones\n        \n        # Lista de posibles transformaciones\n        self.transforms = [\n            transforms.RandomRotation(degrees=(-20, 20)),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            CustomGaussianBlur(kernel_size=3),\n            transforms.RandomAdjustSharpness(sharpness_factor=2),\n            transforms.RandomAutocontrast(),\n            transforms.RandomEqualize(),\n        ]\n\n    def __call__(self, img):\n        transforms_to_apply = random.choices(self.transforms, k=self.n)\n        for transform in transforms_to_apply:\n            img = transform(img)\n        return img\n\n# Definir las transformaciones de entrenamiento con aumentación de datos\ntrain_transform = transforms.Compose([\n    # Transformaciones espaciales\n    transforms.RandomResizedCrop(\n        size=(32, 32),\n        scale=(0.8, 1.0),    # rango de escala\n        ratio=(0.9, 1.1)     # rango de ratio\n    ),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.2),\n    transforms.RandomRotation(degrees=(-15, 15)),\n    \n    # Transformaciones de color y ruido\n    transforms.ColorJitter(\n        brightness=0.2,\n        contrast=0.2,\n        saturation=0.2,\n        hue=0.1\n    ),\n    CustomGaussianBlur(\n        kernel_size=3,\n        sigma_min=0.1,\n        sigma_max=2.0\n    ),\n    \n    # RandAugment para transformaciones adicionales\n    RandAugment(n=2, m=9),\n    \n    # Normalización final\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.49139968, 0.48215841, 0.44653091],\n        std=[0.24703223, 0.24348513, 0.26158784]\n    ),\n])\n\n# Transformaciones para validación/prueba (sin aumentación)\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.49139968, 0.48215841, 0.44653091],\n        std=[0.24703223, 0.24348513, 0.26158784]\n    ),\n])\n\n# Cargar los datasets con las nuevas transformaciones\ndef load_datasets(batch_size=512):\n    train_dataset = datasets.CIFAR10(\n        root='./data',\n        train=True,\n        download=True,\n        transform=train_transform\n    )\n    \n    test_dataset = datasets.CIFAR10(\n        root='./data',\n        train=False,\n        download=True,\n        transform=test_transform\n    )\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    return train_loader, test_loader\n\n# Función para visualizar las transformaciones\ndef visualize_augmentations(dataset, num_examples=5):\n    import matplotlib.pyplot as plt\n    \n    plt.figure(figsize=(15, 3))\n    for i in range(num_examples):\n        img, _ = dataset[i]\n        plt.subplot(1, num_examples, i + 1)\n        plt.imshow(img.permute(1, 2, 0).clip(0, 1))\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Función de entrenamiento modificada para usar mixup\ndef mixup_data(x, y, alpha=0.2):\n    \"\"\"Realiza mixup en los datos de entrada\"\"\"\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n\n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    \"\"\"Calcula la pérdida para datos con mixup\"\"\"\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n# Función de entrenamiento modificada\ndef train_epoch_with_augmentation(model, train_loader, criterion, optimizer, device, use_mixup=True):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        if use_mixup:\n            data, target_a, target_b, lam = mixup_data(data, target)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        \n        if use_mixup:\n            loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n        else:\n            loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Calcular precisión (sin mixup para simplicidad)\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Batch: {batch_idx}, Loss: {loss.item():.4f}, '\n                  f'Acc: {100.*correct/total:.2f}%')\n    \n    return total_loss / len(train_loader), 100. * correct / total\n\n# Ejemplo de uso\nif __name__ == \"__main__\":\n    # Cargar datos con las nuevas transformaciones\n    train_loader, test_loader = load_datasets(batch_size=512)\n    \n    # Visualizar ejemplos de aumentación\n    train_dataset = datasets.CIFAR10(\n        root='./data',\n        train=True,\n        download=True,\n        transform=train_transform\n    )\n    visualize_augmentations(train_dataset)\n    \n    # Configurar el modelo y entrenamiento\n    model, criterion, optimizer, scheduler, device = setup_training()\n    \n    # Entrenar con las nuevas técnicas de aumentación\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n        \n        train_loss, train_acc = train_epoch_with_augmentation(\n            model, train_loader, criterion, optimizer, device, use_mixup=True\n        )\n        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n        \n        scheduler.step()\n        \n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:21:27.582347Z","iopub.execute_input":"2024-10-29T23:21:27.582734Z","iopub.status.idle":"2024-10-29T23:31:29.706313Z","shell.execute_reply.started":"2024-10-29T23:21:27.582697Z","shell.execute_reply":"2024-10-29T23:31:29.704900Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x300 with 5 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\nEpoch: 1/10\nBatch: 0, Loss: 3.4044, Acc: 9.18%\nTrain Loss: 2.4098, Train Acc: 13.43%\nTest Loss: 1.9440, Test Acc: 28.56%\n\nEpoch: 2/10\nBatch: 0, Loss: 2.1164, Acc: 9.96%\nTrain Loss: 2.0728, Train Acc: 16.26%\nTest Loss: 1.7993, Test Acc: 33.44%\n\nEpoch: 3/10\nBatch: 0, Loss: 1.9857, Acc: 29.10%\nTrain Loss: 1.9582, Train Acc: 19.38%\nTest Loss: 1.7171, Test Acc: 37.73%\n\nEpoch: 4/10\nBatch: 0, Loss: 1.9987, Acc: 30.86%\nTrain Loss: 1.8684, Train Acc: 21.17%\nTest Loss: 1.5756, Test Acc: 43.44%\n\nEpoch: 5/10\nBatch: 0, Loss: 2.1619, Acc: 15.62%\nTrain Loss: 1.8177, Train Acc: 24.57%\nTest Loss: 1.4811, Test Acc: 47.35%\n\nEpoch: 6/10\nBatch: 0, Loss: 1.9325, Acc: 39.06%\nTrain Loss: 1.7659, Train Acc: 26.44%\nTest Loss: 1.4776, Test Acc: 46.84%\n\nEpoch: 7/10\nBatch: 0, Loss: 1.5496, Acc: 43.16%\nTrain Loss: 1.7146, Train Acc: 28.35%\nTest Loss: 1.4244, Test Acc: 49.78%\n\nEpoch: 8/10\nBatch: 0, Loss: 2.0122, Acc: 17.97%\nTrain Loss: 1.7606, Train Acc: 24.77%\nTest Loss: 1.3841, Test Acc: 49.73%\n\nEpoch: 9/10\nBatch: 0, Loss: 1.5601, Acc: 8.40%\nTrain Loss: 1.7104, Train Acc: 25.63%\nTest Loss: 1.3198, Test Acc: 52.41%\n\nEpoch: 10/10\nBatch: 0, Loss: 1.4815, Acc: 7.23%\nTrain Loss: 1.6722, Train Acc: 26.33%\nTest Loss: 1.3136, Test Acc: 53.20%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}